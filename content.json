{"meta":{"title":"好记忆不如烂笔头","subtitle":"问题记录，学习笔记","description":"学习笔记，记录过往","author":"Dawn","url":"https://github.com/fafucoder","root":"/"},"pages":[{"title":"categories","date":"2020-01-09T15:59:50.000Z","updated":"2021-12-17T07:42:48.353Z","comments":true,"path":"categories/index.html","permalink":"https://github.com/fafucoder/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2020-01-09T15:59:39.000Z","updated":"2021-12-17T07:42:48.353Z","comments":true,"path":"tags/index.html","permalink":"https://github.com/fafucoder/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"golang中的goroutine死循环","slug":"golang-dead-goroutine","date":"2022-02-08T02:52:47.000Z","updated":"2022-02-08T02:59:01.444Z","comments":true,"path":"2022/02/08/golang-dead-goroutine/","link":"","permalink":"https://github.com/fafucoder/2022/02/08/golang-dead-goroutine/","excerpt":"","text":"参考文档 https://juejin.cn/post/6969802234179616798","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"DNS的相关知识","slug":"linux-dns","date":"2022-01-06T15:03:56.000Z","updated":"2022-01-06T15:10:25.373Z","comments":true,"path":"2022/01/06/linux-dns/","link":"","permalink":"https://github.com/fafucoder/2022/01/06/linux-dns/","excerpt":"","text":"什么DNS解析系统相关名词概念什么是EDNS搭建DNS服务器相关文档","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"内容分发网络(CDN)的技术和原理","slug":"linux-cdn","date":"2021-12-26T08:07:12.000Z","updated":"2022-01-06T15:03:18.098Z","comments":true,"path":"2021/12/26/linux-cdn/","link":"","permalink":"https://github.com/fafucoder/2021/12/26/linux-cdn/","excerpt":"","text":"前言 本文大部分内容都来源于 内容分发网络原理与实践 这本书，这本书全面的阐述了CDN的方方面面，感兴趣的可以仔细阅读原文～ 参考文档","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubernetes dns解析续","slug":"kubernetes-dns1","date":"2021-12-26T03:27:15.000Z","updated":"2022-01-17T11:37:33.935Z","comments":true,"path":"2021/12/26/kubernetes-dns1/","link":"","permalink":"https://github.com/fafucoder/2021/12/26/kubernetes-dns1/","excerpt":"","text":"概述在kubernetes dns解析篇章中讲述了DNS的基本概念，这个篇章让我们深入的了解下DNS协议，以及kubernetes的DNS解析插件coreDNS的相关知识 参考文档 https://www.cnblogs.com/lovezbs/p/13701185.html","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"linux lvs原理及实践","slug":"linux-lvs","date":"2021-12-19T09:01:20.000Z","updated":"2021-12-30T11:26:08.524Z","comments":true,"path":"2021/12/19/linux-lvs/","link":"","permalink":"https://github.com/fafucoder/2021/12/19/linux-lvs/","excerpt":"","text":"前言负载均衡(Load Balance)的职责是将网络请求，或者其他形式的负载“均摊”到不同的机器上，让每台服务器获取到适合自己处理能力的负载。在为高负载服务器分流的同时，还可以避免资源浪费。负载均衡的原理就是当用户的请求到达前端负载均衡器(Director Server)时，通过设置好的调度算法，智能均衡的将请求分发到后端真正服务器上(Real Server)。根据请求类型的不同可以将负载均衡分为四层负载均衡(L4)和七层负载均衡(L7)， 常见的负载均衡器包括LVS，Nginx, HAProxy等。 LVS概述LVS是Linux Virtual Server的简称， 也就是 Linux 虚拟服务器，工作在 OSI 模型的传输层，即四层负载均衡。LVS主要由两部分组成，包括ipvs和ipvsadm。 ipvs(ip virtual server)：工作在内核空间，是真正生效实现调度的代码。 ipvsadm：工作在用户空间，叫 ipvsadm，负责为 ipvs 内核框架编写规则，定义谁是集群服务，而谁是后端真实的服务器(Real Server) LVS 基本工作原理LVS的底层是利用NETIFILTER的钩子能力： 当用户向负载均衡调度器（Director Server）发起请求，调度器将请求发往至内核空间 PREROUTING 链首先会接收到用户请求，判断目标 IP 确定是本机 IP，将数据包发往 INPUT 链 IPVS 是工作在 INPUT 链上的，当用户请求到达 INPUT 时，IPVS 会将用户请求和自己已定义好的集群服务进行比对，如果用户请求的就是定义的集群服务，那么此时 IPVS 会强行修改数据包里的目标 IP 地址及端口，并将新的数据包发往 POSTROUTING 链 POSTROUTING 链接收数据包后发现目标 IP 地址刚好是自己的后端服务器，那么此时通过选路，将数据包最终发送给后端的服务器 LVS 调度算法前言说到负载均衡器原理是根据负载均衡算法把请求转发到后端真实服务器上。LVS作为四层负载均衡器，针对不同的网络服务需求和服务器配置，LVS调度器实现了多种负载均衡调度算法，主要包括如下： 轮询调度 rr（Round Robin）：这种算法是最简单的，就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是简单。轮询算法假设所有的服务器处理请求的能力都是一样的，调度器会将所有的请求平均分配给每个真实服务器，不管后端 RS 配置和处理能力，非常均衡地分发下去。 加权轮叫 wrr（Weighted Round Robin）：这种算法比 rr 的算法多了一个权重的概念，可以给 RS 设置权重，权重越高，那么分发的请求数越多，权重的取值范围 0 – 100。主要是对 rr 算法的一种优化和补充，LVS 会考虑每台服务器的性能，并给每台服务器添加要给权值，如果服务器 A 的权值为 1，服务器 B 的权值为 2，则调度到服务器 B 的请求会是服务器 A 的 2 倍。权值越高的服务器，处理的请求越多。 最少链接 lc（Least Connections）：这个算法会根据后端 RS 的连接数来决定把请求分发给谁，比如 RS1 连接数比 RS2 连接数少，那么请求就优先发给 RS1。 加权最少链接 wlc（Weighted Least Connections）：这个算法比最少链接 lc算法多了一个权重的概念。 基于局部性的最少链接 lblc（Locality-Based Least Connections）：这个算法是针对目标 IP 地址的负载均衡，目前主要用于 Cache 集群系统。该算法根据请求的目标 IP 地址找出该目标 IP 地址最近使用的服务器，若该服务器 是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则用 “最少链接” 的原则选出一个可用的服务 器，将请求发送到该服务器。 复杂的基于局部性最少的连接算法 lblcr（Locality-Based Least Connections with Replication）*：这个算法也是针对目标 IP 地址的负载均衡，目前主要用于 Cache 集群系统。它与 LBLC 算法的不同之处是它要维护从一个 目标 IP 地址到一组服务器的映射，而 LBLC 算法维护从一个目标 IP 地址到一台服务器的映射。该算法根据请求的目标 IP 地址找出该目标 IP 地址对应的服务 器组，按 “最小连接” 原则从服务器组中选出一台服务器，若服务器没有超载，将请求发送到该服务器，若服务器超载；则按 “最小连接” 原则从这个集群中选出一 台服务器，将该服务器加入到服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的 程度。 目标地址散列 dh（Destination Hashing）：这个算法根据请求的目标 IP 地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。 源地址散列 sh（Source Hashing）：这个调度算法根据请求的源 IP 地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且未超载，将请求发送到该服务器，否则返回空。 LVS 工作模式根据负载均衡器对数据包的处理方式分类，LVS支持三种工作模式，分别为NAT模式，DR模式以及TUN模式。原生的LVS工作模式不支持FULLNAT，FULLNAT模式需要自己重新编译LVS。在介绍LVS工作模式之前有必要先解释下相关名词： DS：Director Server，指的是前端负载均衡器节点。 RS：Real Server，后端真实的工作服务器。 VIP：向外部直接面向用户请求，作为用户请求的目标的 IP 地址。 DIP：Director Server IP，主要用于和内部主机通讯的 IP 地址。 RIP：Real Server IP，后端服务器的 IP 地址。 CIP：Client IP，访问客户端的 IP 地址。 LVS NAT模式LVS NAT的工作原理 当用户请求到达 Director Server，此时请求的数据报文会先到内核空间的 PREROUTING 链。 此时报文的源 IP 为 CIP，目标 IP 为 VIP PREROUTING 检查发现数据包的目标 IP 是本机，将数据包送至 INPUT 链 IPVS 比对数据包请求的服务是否为集群服务，若是，修改数据包的目标 IP 地址为后端服务器 IP，然后将数据包发至 POSTROUTING 链。 此时报文的源 IP 为 CIP，目标 IP 为 RIP POSTROUTING 链通过选路，将数据包发送给 Real Server Real Server 比对发现目标为自己的 IP，开始构建响应报文发回给 Director Server。 此时报文的源 IP 为 RIP，目标 IP 为 CIP Director Server 在响应客户端前，此时会将源 IP 地址修改为自己的 VIP 地址，然后响应给客户端。 此时报文的源 IP 为 VIP，目标 IP 为 CIP LVS NAT模式特点 RS 应该使用私有地址，RS 的网关必须指向 DIP。 DIP 和 RIP 必须在同一个网段内。 支持端口映射。 RS 可以使用任意操作系统。 请求和响应报文都需要经过 Director Server，高负载场景中，Director Server 易成为性能瓶颈。 LVS NAT模式实践一、使用Docker模拟(适用于没虚拟机的情况) 12345678910111213141516171819202122232425# 1. 起两个nginx容器，分别是 [root@localhost ~]# docker run -d -p 8000:8000 --name first -t jwilder/whoami[root@localhost ~]# docker run -d -p 8001:8000 --name second -t jwilder/whoami# 2. 获取容器的IP地址[root@localhost ~]# docker inspect -f '&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' first[root@localhost ~]# docker inspect -f '&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;' second# 执行curl获取结果(ip为步骤二返回的结果)[root@localhost ~]# curl 172.17.0.2:8000[root@localhost ~]# curl 172.17.0.3:8000# 创建ipvs规则[root@localhost ~]# ipvsadm -A -t 192.168.56.102:80 -s rr[root@localhost ~]# ipvsadm -a -t 192.168.56.102:80 -r 172.17.0.2:8000 -m[root@localhost ~]# ipvsadm -a -t 192.168.56.102:80 -r 172.17.0.3:8000 -m[root@localhost ~]# ipvsadm -LnTCP 192.168.56.102:80 rr -&gt; 172.17.0.2:8000 Masq 1 0 0 -&gt; 172.17.0.3:8000 Masq 1 0 0 # 验证是否生效(返回结果一次是A，一次是B)[root@localhost ~]# curl 192.168.56.102I'm fdb291a03b87[root@localhost ~]# curl 192.168.56.102I'm 9799eb62225c 二、使用虚拟机模拟 123456789101112131415161718# 前置条件：# 1. 虚拟机网段一致# 2. 服务器2，3上部署了http服务器,且内容不一致# 在虚拟机1上执行， (本机IP为192.168.56.101)ipvsadm -A -t 192.168.57.4:80 -s rripvsadm -a -t 192.168.57.4:80 -r 192.168.56.102:80 -mipvsadm -a -t 192.168.57.4:80 -r 192.168.56.104:80 -m## 开启ip forwardecho 1 &gt;/proc/sys/net/ipv4/ip_forward# 在虚拟机2，3上执行，(本机IP为192.168.56.102， 192.168.56.104), 如果默认路由不设置为虚拟机1，发现是访问不通的ip route del defaultip route add default via 192.168.56.101# 检查是否能够访问, 在虚拟机1上执行curlcurl 192.168.57.4 LVS DR模式DR（Direct Routing 直接路由模式）模式时 LVS 调度器只接收客户发来的请求并将请求转发给后端服务器，后端服务器处理请求后直接把内容直接响应给客户，而不用再次经过 LVS 调度器。LVS 只需要将网络帧的 MAC 地址修改为某一台后端服务器 RS 的 MAC，该包就会被转发到相应的 RS 处理，注意此时的源 IP 和目标 IP 都没变。RS 收到 LVS 转发来的包时，链路层发现 MAC 是自己的，到上面的网络层，发现 IP 也是自己的，于是这个包被合法地接受，RS 感知不到前面有 LVS 的存在。而当 RS 返回响应时，只要直接向源 IP（即用户的 IP）返回即可，不再经过 LVS。 LVS DR模式工作原理 当用户请求到达 Director Server，此时请求的数据报文会先到内核空间的 PREROUTING 链。 此时报文的源 IP 为 CIP，目标 IP 为 VIP。 PREROUTING 检查发现数据包的目标 IP 是本机，将数据包送至 INPUT 链。 IPVS 比对数据包请求的服务是否为集群服务，若是，将请求报文中的源 MAC 地址修改为 DIP 的 MAC 地址，将目标 MAC 地址修改 RIP 的 MAC 地址，然后将数据包发至 POSTROUTING 链。 此时的源 IP 和目的 IP 均未修改，仅修改了源 MAC 地址为 DIP 的 MAC 地址，目标 MAC 地址为 RIP 的 MAC 地址。 由于 DS 和 RS 在同一个网络中，所以是通过二层来传输。POSTROUTING 链检查目标 MAC 地址为 RIP 的 MAC 地址，那么此时数据包将会发至 Real Server。 RS 发现请求报文的 MAC 地址是自己的 MAC 地址，就接收此报文。处理完成之后，将响应报文通过 lo 接口传送给 eth0 网卡然后向外发出。 此时的源 IP 地址为 VIP，目标 IP 为 CIP。 响应报文最终送达至客户端。 LVS DR模式特点 保证前端路由将目标地址为 VIP 报文统统发给 Director Server，而不是 RS。 RS 可以使用私有地址；也可以是公网地址，如果使用公网地址，此时可以通过互联网对 RIP 进行直接访问。 VIP与DIP，RIP可以不在同一网段中. 所有的请求报文经由 Director Server，但响应报文必须不能进过 Director Server。 不支持地址转换，也不支持端口映射。 RS 可以是大多数常见的操作系统。 RS 的网关绝不允许指向 DIP（因为我们不允许他经过 Director Server）。 RS 上的 lo 接口配置 VIP 的 IP 地址。 RS 和 DS 必须在同一个物理网络中(同一机房中)。 LVS DR模式的相关问题为了解决保证前端路由将目标地址为 VIP 报文统统发给 Director Server，而不是 RS。的问题一般有如下解决方案： 在前端路由器做静态地址路由绑定，将对于 VIP 的地址仅路由到 Director Server。但用户未必有路由操作权限，因为有可能是运营商提供的，所以这个方法未必实用。 arptables：在 arp 的层次上实现在 ARP 解析时做防火墙规则，过滤 RS 响应 ARP 请求。这是由 iptables 提供的。 修改 RS 上内核参数（arp_ignore 和 arp_announce）将 RS 上的 VIP 配置在 lo 接口的别名上，并限制其不能响应对 VIP 地址解析请求。 关于arp_ignore和arp_announcearp_ignore：定义接收 ARP 请求时的响应级别 0：响应任意网卡上接收到的对本机 IP 地址的 ARP 请求（包括环回网卡），不论目的 IP 地址是否在接收网卡上 1：只响应目的 IP 地址为接收网卡地址的 ARP 请求 2：只响应目的 IP 地址为接收网卡地址的 ARP 请求，且 ARP 请求的源 IP 地址必须和接收网卡的地址在同网段 1234567891011121314arp_ignore - INTEGER Define different modes for sending replies in response to received ARP requests that resolve local target IP addresses: 0 - (default): reply for any local target IP address, configured on any interface 1 - reply only if the target IP address is local address configured on the incoming interface 2 - reply only if the target IP address is local address configured on the incoming interface and both with the sender&#39;s IP address are part from same subnet on this interface 3 - do not reply for local addresses configured with scope host, only resolutions for global and link addresses are replied 4-7 - reserved 8 - do not reply for all local addresses The max value from conf&#x2F;&#123;all,interface&#125;&#x2F;arp_ignore is used when ARP request is received on the &#123;interface&#125; arp_announce：定义将自己地址向外通告时的通告级别 0：允许任意网卡上的任意地址向外通告 1：尽量仅向目标网络通告与其网络匹配的地址 2：仅向与本地接口上地址匹配的网络进行通告 12345678910111213141516171819202122arp_announce - INTEGER Define different restriction levels for announcing the local source IP address from IP packets in ARP requests sent on interface: 0 - (default) Use any local address, configured on any interface 1 - Try to avoid local addresses that are not in the target&#39;s subnet for this interface. This mode is useful when target hosts reachable via this interface require the source IP address in ARP requests to be part of their logical network configured on the receiving interface. When we generate the request we will check all our subnets that include the target IP and will preserve the source address if it is from such subnet. If there is no such subnet we select source address according to the rules for level 2. 2 - Always use the best local address for this target. In this mode we ignore the source address in the IP packet and try to select local address that we prefer for talks with the target host. Such local address is selected by looking for primary IP addresses on all our subnets on the outgoing interface that include the target IP address. If no suitable local address is found we select the first local address we have on the outgoing interface or on all other interfaces, with the hope we will receive reply for our request and even sometimes no matter the source IP address we announce. The max value from conf&#x2F;&#123;all,interface&#125;&#x2F;arp_announce is used. Increasing the restriction level gives more chance for receiving answer from the resolved target while decreasing the level announces more valid sender&#39;s information. LVS DR模式实践1234567891011121314151617181920212223# 前置条件：# 1. 虚拟机网段一致# 2. 服务器2，3上部署了http服务器,且内容不一致# 在虚拟机1上执行， (本机IP为192.168.56.101)ip addr add 192.168.56.200/32 dev enp0s8ipvsadm -A -t 192.168.56.200:80 -s wrripvsadm -a -t 192.168.56.200:80 -r 192.168.56.102:80 -g -w 1ipvsadm -a -t 192.168.56.200:80 -r 192.168.56.104:80 -g -w 3## 开启ip forwardecho 1 &gt;/proc/sys/net/ipv4/ip_forward# 在虚拟机2，3上执行，(本机IP为192.168.56.102， 192.168.56.104)ip addr add 192.168.56.200/32 dev loip route add 192.168.56.200 dev loecho \"1\" &gt;/proc/sys/net/ipv4/conf/lo/arp_ignoreecho \"2\" &gt;/proc/sys/net/ipv4/conf/lo/arp_announceecho \"1\" &gt;/proc/sys/net/ipv4/conf/all/arp_ignoreecho \"2\" &gt;/proc/sys/net/ipv4/conf/all/arp_announce# 检查是否能够访问, 在外部主机上执行curlcurl 192.168.56.200 LVS TUN 模式 TUN工作在三层，TAP工作在二层 LVS TUN模式工作原理 当用户请求到达 Director Server，此时请求的数据报文会先到内核空间的 PREROUTING 链。 此时报文的源 IP 为 CIP，目标 IP 为 VIP。 PREROUTING 检查发现数据包的目标 IP 是本机，将数据包送至 INPUT 链。 IPVS 比对数据包请求的服务是否为集群服务，若是，在请求报文的首部再次封装一层 IP 报文，封装源 IP 为 DIP，目标 IP 为 RIP。然后发至 POSTROUTING 链。 此时源 IP 为 DIP，目标 IP 为 RIP。 POSTROUTING 链根据最新封装的 IP 报文，将数据包发至 RS（因为在外层封装多了一层 IP 首部，所以可以理解为此时通过隧道传输）。 此时源 IP 为 DIP，目标 IP 为 RIP。 RS 接收到报文后发现是自己的 IP 地址，就将报文接收下来，拆除掉最外层的 IP 后，会发现里面还有一层 IP 首部，而且目标是自己的 lo 接口 VIP，那么此时 RS 开始处理此请求，处理完成之后，通过 lo 接口送给 eth0 网卡，然后向外传递。 此时的源 IP 地址为 VIP，目标 IP 为 CIP。 响应报文最终送达至客户端。 LVS TUN模式特点 不改变请求数据包，而是在请求数据包上新增一层 IP 首部信息。因此负载均衡器不能对端口进行转发，但可以和真实服务器不在同一局域网内，且真实服务器需要支持能够解析两层 IP 首部信息，即需要支持“IP Tunneling”或“IP Encapsulation”协议 真实服务器中的 VIP，只能被自己 “看见”，其他设备不可知。因此 VIP 必须绑定在真实服务器的 lo 网卡上，并且不允许将此网卡信息经过 ARP 协议对外通告 请求的数据包经过负载均衡器后，直接由真实服务器返回给客户端，响应数据包不需要再经过负载均衡器 RS 的网关不会也不可能指向 DIP。 LVS TUN模式实践12345678910111213141516171819202122232425# 前置条件：# 2. 服务器2，3上部署了http服务器,且内容不一致# 在虚拟机1上执行， (本机IP为192.168.56.101)ip addr add 192.168.56.210/32 dev enp0s8ipvsadm -A -t 192.168.56.210:80 -s rripvsadm -a -t 192.168.56.210:80 -r 192.168.56.102:80 -iipvsadm -a -t 192.168.56.210:80 -r 192.168.56.104:80 -i# 在虚拟机2，3上执行，(本机IP为192.168.56.102， 192.168.56.104)modprobe ipip // 启用ipiplsmod | grep ipipip addr add 192.168.56.210/32 dev tunl0ip link set tunl0 upip route add 192.168.56.210 dev tunl0echo 1 &gt;/proc/sys/net/ipv4/conf/lo/arp_ignoreecho 2 &gt;/proc/sys/net/ipv4/conf/lo/arp_announceecho 1 &gt;/proc/sys/net/ipv4/conf/all/arp_ignoreecho 2 &gt;/proc/sys/net/ipv4/conf/all/arp_announceecho 0 &gt; /proc/sys/net/ipv4/conf/tunl0/rp_filterecho 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter# 检查是否能够访问, 在外部主机上执行curlcurl 192.168.56.210 LVS FULLNAT模式LVS NAT, DR模式中，RS跟DS必须在同一个VLAN中，当集群规模较小时，使用 NAT、DR 模式都是没有问题的，当集群内有几十台以上时，那么这些服务器通常都不在同一个 VLAN/网段 内了。这时，必须再研发出一种能支持跨 VLAN/网段 通信的模式，FULLNAT 模式就是为了解决这个问题而生的。 LVS FullNAT 模式几乎和 LVS NAT 模式相同，不同之处即是：引入 Local Address（内网 IP 地址）。CIP-&gt;VIP 转换换为 LIP-&gt;RIP，而 LIP 和 RIP 均为 IDC 内网 IP，因此可以跨 VLAN 通讯。LVS原生模式不支持FULLNAT，因此需要自己手动编译内核～ LVS FULLNAT内核编译 注意：内核编译的内核包需要跟自己的操作系统内核版本对应上(大版本对应上~)，暂时未找到centos7对应的内核包在哪儿。 本机的内核版本为：2.6.32-220 安装依赖 123456yum -y install elfutils-devel.x86_64 audit-libs-devel.x86_64yum -y install rpmdevtools yum-utilsyum -y install redhat-rpm-configyum -y install gcc xmlto patchutils asciidoc zlib-devel binutils-devel newt-devel python-devel hmaccalc perl-ExtUtils-Embed.x86_64yum -y install rng-tools 随机数生成器yum -y install bison 下载内核包，ipvs补丁包 1234# 内核包wget ftp://ftp.redhat.com/pub/redhat/linux/enterprise/6Server/en/os/SRPMS/kernel-2.6.32-220.23.1.el6.src.rpm# lvs fullnat包wget http://kb.linuxvirtualserver.org/images/a/a5/Lvs-fullnat-synproxy.tar.gz 生成内核代码 123456789rpm -ivh kernel-2.6.32-220.23.1.el6.src.rpm# 生成随机数(一定要生成随机数，否则rpmbuild的时候会卡住)rngd -r /dev/urandom # rpmbuild -bp kernel.speccd ~/rpmbuild/SPECSrpmbuild -bp --target=$(uname -m) kernel.spec 4. 打 LVS补丁12345tar -zxvf Lvs-fullnat-synproxy.tar.gzcp ~/lvs-fullnat-synproxy/lvs-2.6.32-220.23.1.el6.patch ~/rpmbuild/BUILD/kernel-2.6.32-220.23.1.el6/linux-2.6.32-220.23.1.el6.x86_64 &amp;&amp; cd $_patch -p1 &lt; lvs-2.6.32-220.23.1.el6.patch 编译内核 12345# 最好不要重新修改内核版本(遇到过改了内核版本后，编译没问题，重启后触发Kernel panic – not syncing: Attempted to kill init的问题)# 使用-j 加快内核编译速度make -j16make modules_installmake install 重启操作系统 123reboot# 重启后发现系统版本已经改变了， Linux localhost.localdomain 2.6.32 #2 SMP Thu Dec 30 03:38:53 EST 2021 x86_64 x86_64 x86_64 GNU/Linux 编译ipvsadm, keepalived等工具 12345678910111213cd ~/lvs-fullnat-synproxy &amp;&amp; tar -zxvf lvs-tools.tar.gz# 编译 keepalivedcd ~/lvs-fullnat-synproxy/tools/keepalived./configure --with-kernel-dir=\"/lib/modules/`uname -r`/build\" &amp;&amp; make &amp;&amp; make install # 编译 ipvsadmcd ~/lvs-fullnat-synproxy/tools/ipvsadmmake &amp;&amp; make install# 编译 quaagecd ~/lvs-fullnat-synproxy/tools/quagga./configure --disable-ripd --disable-ripngd --disable-bgpd --disable-watchquagga --disable-doc --enable-user=root --enable-vty-group=root --enable-group=root --enable-zebra --localstatedir=/var/run/quagga &amp;&amp; make &amp;&amp; make install 验证是否编译成功 123456# 输入 ipvsadm -h 可以看到有fullnat 模式 --gatewaying -g gatewaying (direct routing) (default) --ipip -i ipip encapsulation (tunneling) --fullnat -b fullnat mode --masquerading -m masquerading (NAT) LVS FULLNAT模式工作原理 当用户请求到达 Director Server，此时请求的数据报文会先到内核空间的 PREROUTING 链。 此时报文的源 IP 为 CIP，目标 IP 为 VIP PREROUTING 检查发现数据包的目标 IP 是本机，将数据包送至 INPUT 链 IPVS 比对数据包请求的服务是否为集群服务，若是，修改数据包的源IP地址分发服务器IP，目标 IP 地址为后端服务器 IP，然后将数据包发至 POSTROUTING 链。 此时报文的源 IP 为 DIP，目标 IP 为 RIP POSTROUTING 链通过选路，将数据包发送给 Real Server Real Server 比对发现目标为自己的 IP，开始构建响应报文发回给 Director Server。 此时报文的源 IP 为 RIP，目标 IP 为 DIP Director Server 在响应客户端前，此时会将源 IP 地址修改为自己的 VIP 地址，然后响应给客户端。 此时报文的源 IP 为 VIP，目标 IP 为 CIP LVS FULLNAT模式实践12345678910# 虚拟机4的IP地址为192.168.58.100， 虚拟机2，3的IP地址为192.168.56.102， 192.168.56.104, 在虚拟机上执行：ip addr add 192.168.58.200/32 dev eth1ipvsadm -A -t 192.168.58.200:80 -s wrripvsadm -a -t 192.168.58.200:80 -r 192.168.56.102:80 -b -w 2ipvsadm -a -t 192.168.58.200:80 -r 192.168.56.104:80 -b -w 1ipvsadm -P -t 192.168.58.200:80 -z 192.168.58.100# 集群外执行curl, 验证是否成功curl 192.168.56.200 参考文档 LVS 负载均衡原理及安装配置详解 // 推荐阅读 LVS 原理介绍和配置实践 // 推荐阅读 深入浅出 LVS 负载均衡系列（一）：NAT、FULLNAT 模型原理 //这系列文章不错 深入浅出 LVS 负载均衡系列（二）：DR、TUN 模型原理 //这系列文章不错 深入浅出 LVS 负载均衡（三）实操 NAT、DR 模型 //这系列文章不错 LVS FULLNAT 实战 // lvs fullnat 编译，样例 IPVS FULLNAT and SYNPROXY // 编译 lvs fullnat LVS/fullnat模式（内核编译）","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"Hello World","slug":"hello-world","date":"2021-12-17T07:42:48.328Z","updated":"2021-12-17T07:42:48.328Z","comments":true,"path":"2021/12/17/hello-world/","link":"","permalink":"https://github.com/fafucoder/2021/12/17/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"golang中锁的实现原理","slug":"golang-metux","date":"2021-11-22T13:35:17.000Z","updated":"2021-12-17T07:42:48.327Z","comments":true,"path":"2021/11/22/golang-metux/","link":"","permalink":"https://github.com/fafucoder/2021/11/22/golang-metux/","excerpt":"","text":"参考文档 https://nxw.name/2021/golang-mutexde-shi-xian-yuan-li-1ef30cc7 // 这边文章值得好好看 https://cloud.tencent.com/developer/article/1493418 // 死锁的产生条件 https://draveness.me/golang/docs/part3-runtime/ch06-concurrency/golang-sync-primitives/#cond // 面向信仰编程，这文章也挺不错的~ https://segmentfault.com/a/1190000039712353 // 这文章也不错","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"什么是云原生","slug":"kubernetes-cloud-native","date":"2021-11-22T09:23:10.000Z","updated":"2021-12-17T07:42:48.333Z","comments":true,"path":"2021/11/22/kubernetes-cloud-native/","link":"","permalink":"https://github.com/fafucoder/2021/11/22/kubernetes-cloud-native/","excerpt":"","text":"什么是云原生参考文档 https://juejin.cn/post/6844904197859590151 https://jimmysong.io/kubernetes-handbook/cloud-native/cloud-native-definition.html https://tech.meituan.com/2020/03/12/cloud-native-security.html //美团容器安全实践，说的不错","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"golang 垃圾回收算法","slug":"golang-gc","date":"2021-11-14T02:09:38.000Z","updated":"2021-12-17T07:42:48.326Z","comments":true,"path":"2021/11/14/golang-gc/","link":"","permalink":"https://github.com/fafucoder/2021/11/14/golang-gc/","excerpt":"","text":"参考文档 golang垃圾回收gc // 推荐阅读此大佬的所有go文章，写得针不戳 Golang GC 垃圾回收机制详解 // 对常见的gc做了说明 万字长文深入浅出 Golang Runtime //之前go夜读有看到分享，说的贼好","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"golang中的内存分配","slug":"golang-memory","date":"2021-11-11T09:21:46.000Z","updated":"2021-12-17T07:42:48.327Z","comments":true,"path":"2021/11/11/golang-memory/","link":"","permalink":"https://github.com/fafucoder/2021/11/11/golang-memory/","excerpt":"","text":"概述Go语言内置运行时(Runtime), 抛弃了传统的内存分配方式，改为自主管理。这样可以自主地实现更好的内存使用模式，比如内存池、预分配等等。这样，不会每次内存分配都需要进行系统调用。 Golang运行时的内存分配算法主要源自 Google 为 C 语言开发的TCMalloc算法，全称Thread-Caching Malloc。核心思想就是把内存分为多级管理，从而降低锁的粒度。它将可用的堆内存采用二级分配的方式进行管理：每个线程都会自行维护一个独立的内存池，进行内存分配时优先从该内存池中分配，当内存池不足时才会向全局内存池申请，以避免不同线程对全局内存池的频繁竞争。 TC Malloc原理go 内存分配make与new的区别参考文档 万字长文深入浅出 Golang Runtime // go夜读的分享，推荐去看下go夜读 TC Malloc 内存分配原理简析 // 简洁tc malloc的原理 图解Go语言内存分配 // 绕全成大佬写的，通俗易懂 golang内存分配原理及make和new的区别 // 强烈推荐这篇文档，这位大佬的go系列文章都很牛批","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"goroutine并发数量控制","slug":"golang-goroutine-count","date":"2021-11-09T02:02:02.000Z","updated":"2021-12-17T07:42:48.326Z","comments":true,"path":"2021/11/09/golang-goroutine-count/","link":"","permalink":"https://github.com/fafucoder/2021/11/09/golang-goroutine-count/","excerpt":"","text":"概述在golang语言中创建协程（Goroutine）的成本非常低，因此稍不注意就可能创建出大量的协程，一方面会造成资源的浪费，例如有一万个任务需要处理，如果启用一万个goroutine同时处理，意味了CPU内存资源大量的飙升，所以一般会控制goroutine的数量，例如最多只有一百个goroutine在运行，本章将看下如何控制goroutine的并发数量 goroutine并发控制并发未控制情形在说明goroutine并发控制前，先看下并发不控制的代码逻辑 12345678910111213141516171819202122package mainimport ( \"fmt\" \"sync\")func main() &#123; wg := sync.WaitGroup&#123;&#125; workerCount := 100 for i := 0; i &lt; workerCount; i++ &#123; wg.Add(1) go func(i int) &#123; defer wg.Done() fmt.Printf(\"i am goroutine %d \\n\", i) &#125;(i) &#125; wg.Wait() fmt.Println(\"worker have done\")&#125; 上面的输出如下, 在多核的场景下，goroutine不一定是顺序输出的: 1234567891011......i am goroutine 88 i am goroutine 63 i am goroutine 89 i am goroutine 98 i am goroutine 82 i am goroutine 93 i am goroutine 87 i am goroutine 95 i am goroutine 97 worker have done 下图展示了为每个 job 创建一个 goroutine 的情况（换句话说，goroutine 的数量是不受控制的）。此种情况虽然生成了很多的 goroutine，但是每个 CPU 核上同一时间只能执行一个 goroutine；当 job 很多且生成了相应数目的 goroutine 后，会出现很多等待执行的 goroutine，从而造成资源上的浪费。 并发控制概述给每个 job 生成一个 goroutine 的方式显得粗暴了很多，那么可以通过什么样的方式控制 goroutine 的数目呢？其实上面的代码通过一个 for-range 循环完成了两件事情：①为每个 job 创建 goroutine；②把任务相关的标识传给相应的 goroutine 执行。为了控制 goroutine 的数目，完全可以把上面的两个过程拆分开：a）先通过一个 for-range 循环创建指定数目的 goroutine，b）然后通过 channel/buffered channel 给每个 goroutine 传递任务相关的信息（这里的channel是否缓冲无所谓，主要用到的是 channel 的线程安全特性）。如下图所示。 goroutine并发控制方案一针对上面的代码，如果想达到goroutine并发执行的控制，我们可以加个buffer channel来限制最多只有多少个goroutine在执行，代码如下： 1234567891011121314151617181920212223242526package mainimport ( \"fmt\" \"sync\")func main() &#123; taskCount := 10 worker := make(chan int, 3) wg := sync.WaitGroup&#123;&#125; for i := 0; i &lt; taskCount; i++ &#123; wg.Add(1) go func(i int) &#123; defer wg.Done() worker &lt;- i fmt.Println(\"i am worker \", i) &lt;-worker &#125;(i) &#125; wg.Wait()&#125; 通过channel限制最多有三个goroutine在执行，其余的被挂起等待中，但是此方案也有个缺陷，那就是goroutine还是都被创建了，只不过这些goroutine被挂起了而已。 goroutine并发控制方案二12345678910111213141516171819202122232425262728package mainimport ( \"fmt\" \"sync\")func main() &#123; taskCount, workerCount := 10, 3 worker := make(chan int, workerCount) wg := sync.WaitGroup&#123;&#125; for i := 0; i &lt; workerCount; i++ &#123; go func() &#123; for w := range worker &#123; fmt.Println(\"i am worker \", w) wg.Done() &#125; &#125;() &#125; for i := 0; i &lt; taskCount; i++ &#123; wg.Add(1) worker &lt;- i &#125; wg.Wait()&#125; 在方案二中，我们起了三个goroutine一直去消费woker以达到限制goroutine最大并发数的目的。 参考文档 【图示】控制 Goroutine 的并发数量的方式 // jing 维大佬 来，控制一下 goroutine 的并发数量 // 煎鱼老师","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"golang goroutine浅析","slug":"golang-goroutine","date":"2021-11-08T14:32:55.000Z","updated":"2021-12-17T07:42:48.326Z","comments":true,"path":"2021/11/08/golang-goroutine/","link":"","permalink":"https://github.com/fafucoder/2021/11/08/golang-goroutine/","excerpt":"","text":"概述什么是goroutine? Goroutine 可以看作对 thread 加的一层抽象，它更轻量级，可以单独执行。 goroutine(协程) 跟 线程的区别goroutine跟线程的区别可以从内存消耗、创建与销毀、切换三个维度说明 内存创建： 创建一个 goroutine 的栈内存消耗为 2 KB，实际运行过程中，如果栈空间不够用，会自动进行扩容。创建一个 thread 则需要消耗 1 MB 栈内存，而且还需要一个被称为 “a guard page” 的区域用于和其他 thread 的栈空间进行隔离。 创建和销毀： 线程创建和销毀都会有巨大的消耗，因为要和操作系统打交道，是内核级的，通常解决的办法就是线程池。而 goroutine 因为是由 Go runtime 负责管理的，创建和销毁的消耗非常小，是用户级。 切换： 当 threads 切换时，需要保存各种寄存器，以便将来恢复， 而 goroutines 切换只需保存三个寄存器：Program Counter, Stack Pointer and BP。因此goroutines 切换成本比 threads 要小得多。 G-P-M模型概述在 Go 语言中，每一个 goroutine 是一个独立的执行单元，相较于每个 OS 线程固定分配 2M 内存的模式，goroutine 的栈采取了动态扩容方式， 初始时仅为2KB，随着任务执行按需增长，最大可达 1GB（64 位机器最大是 1G，32 位机器最大是 256M），且完全由 golang 自己的调度器 Go Scheduler 来调度。此外，GC 还会周期性地将不再使用的内存回收，收缩栈空间。 因此，Go 程序可以同时并发成千上万个 goroutine 是得益于它强劲的调度器和高效的内存模型。 将 goroutines 调度到线程上执行，仅仅是 runtime 层面的一个概念，在操作系统之上的层面，在golang中有三个基础的结构体来实现 goroutines 的调度。g，m，p， 俗称GPM模型： G: 表示 Goroutine，每个 Goroutine 对应一个 G 结构体，G 存储 Goroutine 的运行堆栈、状态以及任务函数，可重用。G 并非执行体，每个 G 需要绑定到 P 才能被调度执行。 P: Processor，表示逻辑处理器， 对 G 来说，P 相当于 CPU 核，G 只有绑定到 P(在 P 的 local runq 中)才能被调度。对 M 来说，P 提供了相关的执行环境(Context)，如内存分配状态(mcache)，任务队列(G)等，P 的数量决定了系统内最大可并行的 G 的数量（前提：物理 CPU 核数 &gt;= P 的数量），P 的数量由用户设置的 GOMAXPROCS 决定，但是不论 GOMAXPROCS 设置为多大，P 的数量最大为 256。 M: Machine，OS 线程抽象，代表着真正执行计算的资源，在绑定有效的 P 后，进入 schedule 循环；而 schedule 循环的机制大致是从 Global 队列、P 的 Local 队列以及 wait 队列中获取 G，切换到 G 的执行栈上并执行 G 的函数，调用 goexit 做清理工作并回到 M，如此反复。M 并不保留 G 状态，这是 G 可以跨 M 调度的基础，M 的数量是不定的，由 Go Runtime 调整，为了防止创建过多 OS 线程导致系统调度不过来，目前默认最大限制为 10000 个。 每个 P 维护一个 G 的本地队列； 当一个 G 被创建出来，或者变为可执行状态时，就把他放到 P 的本地可执行队列中，如果满了则放入Global； 当一个 G 在 M 里执行结束后，P 会从队列中把该 G 取出；如果此时 P 的队列为空，即没有其他 G 可以执行， M 就随机选择另外一个 P，从其可执行的 G 队列中取走一半。 除了GPM外，还有两个比较重要的组件： 全局可运行队列（GRQ）和本地可运行队列（LRQ）。 LRQ 存储本地（也就是具体的 P）的可运行 goroutine，GRQ 存储全局的可运行 goroutine，这些 goroutine 还没有分配到具体的 P。 调度过程 当通过 go 关键字创建一个新的 goroutine 的时候，它会优先被放入 P 的本地队列。 为了运行 goroutine，M 需要持有（绑定）一个 P，接着 M 会启动一个 OS 线程，循环从 P 的本地队列里取出一个 goroutine 并执行。 执行调度算法：当 M 执行完了当前 P 的 Local 队列里的所有 G 后，P 也不会就这么在那划水啥都不干，它会先尝试从 Global 队列寻找 G 来执行，如果 Global 队列为空，它会随机挑选另外一个 P，从它的队列里中拿走一半的 G 到自己的队列中执行。 调度时机在四种情形下，goroutine 可能会发生调度，但也并不一定会发生，只是说 Go scheduler 有机会进行调度。 使用关键字 go： go 创建一个新的 goroutine，Go scheduler 会考虑调度 GC: 由于进行 GC 的 goroutine 也需要在 M 上运行，因此肯定会发生调度。当然，Go scheduler 还会做很多其他的调度，例如调度不涉及堆访问的 goroutine 来运行。GC 不管栈上的内存，只会回收堆上的内存 系统调用: 当 goroutine 进行系统调用时，会阻塞 M，所以它会被调度走，同时一个新的 goroutine 会被调度上来 内存同步访问: atomic，mutex，channel 操作等会使 goroutine 阻塞，因此会被调度走。等条件满足后（例如其他 goroutine 解锁了）还会被调度上来继续运行 同步/异步系统调用当 G 需要进行系统调用时，根据调用的类型，它所依附的 M 有两种情况：同步和异步。 对于同步的情况，M 会被阻塞，进而从 P 上调度下来，P 可不养闲人，G 仍然依附于 M。之后，一个新的 M 会被调用到 P 上，接着执行 P 的 LRQ 里嗷嗷待哺的 G 们。一旦系统调用完成，G 还会加入到 P 的 LRQ 里，M 则会被“雪藏”，待到需要时再“放”出来。 对于异步的情况，M 不会被阻塞，G 的异步请求会被“代理人” network poller 接手，G 也会被绑定到 network poller，等到系统调用结束，G 才会重新回到 P 上。M 由于没被阻塞，它因此可以继续执行 LRQ 里的其他 G。 goroutine状态流转 GPM结构G 结构g是goroutine的缩写，是goroutine的控制结构，是对goroutine的抽象。看下它内部主要的一些结构： 1234567891011121314151617181920212223242526272829303132333435363738394041type g struct &#123; //堆栈参数。 //堆栈描述了实际的堆栈内存：[stack.lo，stack.hi）。 // stackguard0是在Go堆栈增长序言中比较的堆栈指针。 //通常是stack.lo + StackGuard，但是可以通过StackPreempt触发抢占。 // stackguard1是在C堆栈增长序言中比较的堆栈指针。 //它是g0和gsignal堆栈上的stack.lo + StackGuard。 //在其他goroutine堆栈上为〜0，以触发对morestackc的调用（并崩溃）。 //当前g使用的栈空间，stack结构包括 [lo, hi]两个成员 stack stack // offset known to runtime/cgo // 用于检测是否需要进行栈扩张，go代码使用 stackguard0 uintptr // offset known to liblink // 用于检测是否需要进行栈扩展，原生代码使用的 stackguard1 uintptr // offset known to liblink // 当前g所绑定的m m *m // current m; offset known to arm liblink // 当前g的调度数据，当goroutine切换时，保存当前g的上下文，用于恢复 sched gobuf // goroutine运行的函数 fnstart *FuncVal // g当前的状态 atomicstatus uint32 // 当前g的id goid int64 // 状态Gidle,Grunnable,Grunning,Gsyscall,Gwaiting,Gdead status int16 // 下一个g的地址，通过guintptr结构体的ptr set函数可以设置和获取下一个g，通过这个字段和sched.gfreeStack sched.gfreeNoStack 可以把 free g串成一个链表 schedlink guintptr // 判断g是否允许被抢占 preempt bool // preemption signal, duplicates stackguard0 = stackpreempt // g是否要求要回到这个M执行, 有的时候g中断了恢复会要求使用原来的M执行 lockedm muintptr // 用于传递参数，睡眠时其它goroutine设置param，唤醒时此goroutine可以获取 param *void // 创建这个goroutine的go表达式的pc uintptr gopc&#125; 其中包含了栈信息stackbase和stackguard，有运行的函数信息fnstart。这些就足够成为一个可执行的单元了，只要得到CPU就可以运行。goroutine切换时，上下文信息保存在结构体的sched域中。goroutine切换时，上下文信息保存在结构体的sched域中。goroutine是轻量级的线程或者称为协程，切换时并不必陷入到操作系统内核中，很轻量级。 12345678struct Gobuf&#123; //这些字段的偏移是libmach已知的（硬编码的）。 sp uintper; pc *byte; g *G; ...&#125;; P 结构P是Processor的缩写。结构体P的加入是为了提高Go程序的并发度，实现更好的调度。M代表OS线程。P代表Go代码执行时需要的资源。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849type p struct &#123; lock mutex id int32 // p的状态 status uint32 // one of pidle/prunning/... // 下一个p的地址，可参考 g.schedlink link puintptr // 每次调用 schedule 时会加一 schedtick uint32 // 每次系统调用时加一 syscalltick uint32 // 用于 sysmon 线程记录被监控 p 的系统调用时间和运行时间 sysmontick sysmontick // last tick observed by sysmon10 // p所关联的m 指向绑定的 m，如果 p 是 idle 的话，那这个指针是 nil m muintptr // back-link to associated m (nil if idle) // 内存分配的时候用的，p所属的m的mcache用的也是这个 mcache *mcache // Cache of goroutine ids, amortizes accesses to runtime·sched.goidgen. // 从sched中获取并缓存的id，避免每次分配goid都从sched分配 goidcache uint64 goidcacheend uint64 // Queue of runnable goroutines. Accessed without lock. // p 本地的runnbale的goroutine形成的队列 runqhead uint32 runqtail uint32 runq [256]guintptr // runnext，如果不是nil，则是已准备好运行的G //当前的G，并且应该在下一个而不是其中运行 // runq，如果运行G的时间还剩时间 //切片。它将继承当前时间剩余的时间 //切片。如果一组goroutine锁定在 //交流等待模式，该计划将其设置为 //单位并消除（可能很大）调度 //否则会由于添加就绪商品而引起的延迟 // goroutines到运行队列的末尾。 // 下一个执行的g，如果是nil，则从队列中获取下一个执行的g runnext guintptr // Available G's (status == Gdead) // 状态为 Gdead的g的列表，可以进行复用 gfree *g gfreecnt int32&#125; 跟G不同的是，P不存在waiting状态。MCache被移到了P中，但是在结构体M中也还保留着。在P中有一个Grunnable的goroutine队列，这是一个P的局部队列。当P执行Go代码时，它会优先从自己的这个局部队列中取，这时可以不用加锁，提高了并发度。如果发现这个队列空了，则去其它P的队列中拿一半过来，这样实现工作流窃取的调度。这种情况下是需要给调用器加锁的。 M 结构1234567891011121314151617181920212223242526272829303132type m struct &#123; // g0是用于调度和执行系统调用的特殊g g0 *g // goroutine with scheduling stack // m当前运行的g curg *g // current running goroutine // 当前拥有的p p puintptr // attached p for executing go code (nil if not executing go code) // 线程的 local storage tls [6]uintptr // thread-local storage // 唤醒m时，m会拥有这个p nextp puintptr id int64 // 如果 !=\"\", 继续运行curg preemptoff string // if != \"\", keep curg running on this m // 自旋状态，用于判断m是否工作已结束，并寻找g进行工作 spinning bool // m is out of work and is actively looking for work // 用于判断m是否进行休眠状态 blocked bool // m is blocked on a note // m休眠和唤醒通过这个，note里面有一个成员key，对这个key所指向的地址进行值的修改，进而达到唤醒和休眠的目的 park note // 所有m组成的一个链表 alllink *m // on allm // 下一个m，通过这个字段和sched.midle 可以串成一个m的空闲链表 schedlink muintptr // mcache，m拥有p的时候，会把自己的mcache给p mcache *mcache // lockedm的对应值 lockedg guintptr // 待释放的m的list，通过sched.freem 串成一个链表 freelink *m // on sched.freem&#125; 和G类似，M中也有alllink域将所有的M放在allm链表中。lockedg是某些情况下，G锁定在这个M中运行而不会切换到其它M中去。M中还有一个MCache，是当前M的内存的缓存。M也和G一样有一个常驻寄存器变量，代表当前的M。同时存在多个M，表示同时存在多个物理线程。 参考文档 深度解密go语言之scheduler //绕全成大佬的解密完全看不懂 GoLang GPM模型 //写的不错 万字长文深入浅出 Golang Runtime //之前go夜读有看到分享，说的贼好 go中的协程-goroutine的底层实现 // pdd 牛皮","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"web应用的未来是webAssembly?","slug":"golang-webassembly","date":"2021-10-13T13:30:10.000Z","updated":"2021-12-17T07:42:48.328Z","comments":true,"path":"2021/10/13/golang-webassembly/","link":"","permalink":"https://github.com/fafucoder/2021/10/13/golang-webassembly/","excerpt":"","text":"什么是WebAssembly在说明什么是Webassembly之前，我们有必要了解一下asm.js。2012年，Mozilla 的工程师 Alon Zakai 在研究 LLVM 编译器时突发奇想：许多 3D 游戏都是用 C / C++ 语言写的，如果能将 C / C++ 语言编译成 JavaScript 代码，它们不就能在浏览器里运行了吗？众所周知，JavaScript 的基本语法与 C 语言高度相似。于是，他开始研究怎么才能实现这个目标，为此专门做了一个编译器项目 Emscripten。这个编译器可以将 C / C++ 代码编译成 JS 代码，但不是普通的 JS，而是一种叫做 asm.js 的 JavaScript 变体，性能差不多是原生代码的50%。 之后Google开发了Portable Native Client，也是一种能让浏览器运行C/C++代码的技术。 后来可能是因为彼此之间有共同的更高追求，Google, Microsoft, Mozilla, Apple等几家大公司一起合作开发了一个面向Web的通用二进制和文本格式的项目，那就是WebAssembly。asm.js 与 WebAssembly 功能基本一致，就是转出来的代码不一样：asm.js 是文本，WebAssembly 是二进制字节码，因此运行速度更快、体积更小。 WebAssembly(又称 wasm) 是一种新的字节码格式，主流浏览器都已经支持 WebAssembly。 和 JS 需要解释执行不同的是，WebAssembly 字节码和底层机器码很相似可快速装载运行，因此性能相对于 JS 解释执行大大提升。 也就是说 WebAssembly 并不是一门编程语言，而是一份字节码标准，需要用高级编程语言编译出字节码放到 WebAssembly 虚拟机中才能运行， 浏览器厂商需要做的就是根据 WebAssembly 规范实现虚拟机。 这里引用MDN上官方对其的解释：WebAssembly是一种新的编码方式，可以在现代的网络浏览器中运行 － 它是一种低级的类汇编语言，具有紧凑的二进制格式，可以接近原生的性能运行，并为诸如C / C ++ / Rust等语言提供一个编译目标，以便它们可以在Web上运行。它也被设计为可以与JavaScript共存，允许两者一起工作(总结一句话：Webassembly就像是JVM虚拟机，可以执行二进制码)。 有人这么评价Webassembly： Webassembly是Rust押宝的唯一使用场景(Rust语言的使用场景太少了，不像golang在云原生跟中间键领域有很多应用)。 为什么需要WebAssembly自从 JavaScript 诞生起到现在已经变成最流行的编程语言，这背后正是 Web 的发展所推动的。Web 应用变得更多更复杂，但这也渐渐暴露出了 JavaScript 的问题： 语法太灵活导致开发大型 Web 项目困难； 性能不能满足一些场景的需要。 针对以上两点缺陷，近年来出现了一些 JS 的代替语言，例如： 微软的 TypeScript 通过为 JS 加入静态类型检查来改进 JS 松散的语法，提升代码健壮性； 谷歌的 Dart 则是为浏览器引入新的虚拟机去直接运行 Dart 程序以提升性能； 火狐的 asm.js 则是取 JS 的子集，JS 引擎针对 asm.js 做性能优化。 以上尝试各有优缺点，其中： TypeScript 只是解决了 JS 语法松散的问题，最后还是需要编译成 JS 去运行，对性能没有提升； Dart 只能在 Chrome 预览版中运行，无主流浏览器支持，用 Dart 开发的人不多； asm.js 语法太简单、有很大限制，开发效率低。 三大浏览器巨头分别提出了自己的解决方案，互不兼容，这违背了 Web 的宗旨； 是技术的规范统一让 Web 走到了今天，因此形成一套新的规范去解决 JS 所面临的问题迫在眉睫。于是 WebAssembly 诞生了，WebAssembly 是一种新的字节码格式，主流浏览器都已经支持 WebAssembly。 和 JS 需要解释执行不同的是，WebAssembly 字节码和底层机器码很相似可快速装载运行，因此性能相对于 JS 解释执行大大提升。 也就是说 WebAssembly 并不是一门编程语言，而是一份字节码标准，需要用高级编程语言编译出字节码放到 WebAssembly 虚拟机中才能运行， 浏览器厂商需要做的就是根据 WebAssembly 规范实现虚拟机。 总结一句话就是：主流厂商极力主导 + webassembly速度快 Webassembly原理要搞懂 WebAssembly 的原理，需要先搞懂计算机的运行原理。 电子计算机都是由电子元件组成，为了方便处理电子元件只存在开闭两种状态，对应着 0 和 1，也就是说计算机只认识 0 和 1，数据和逻辑都需要由 0 和 1 表示，也就是可以直接装载到计算机中运行的机器码。 机器码可读性极差，因此人们通过高级语言 C、C++、Rust、Go 等编写再编译成机器码。 由于不同的计算机 CPU 架构不同，机器码标准也有所差别，常见的 CPU 架构包括 x86、AMD64、ARM， 因此在由高级编程语言编译成可自行代码时需要指定目标架构。WebAssembly 字节码是一种抹平了不同 CPU 架构的机器码，WebAssembly 字节码不能直接在任何一种 CPU 架构上运行， 但由于非常接近机器码，可以非常快的被翻译为对应架构的机器码，因此 WebAssembly 运行速度和机器码接近，这听上去非常像 Java 字节码。 在javascript中，JavaScript引擎是执行 JavaScript 代码的程序或解释器。JavaScript引擎可以实现为标准解释器，或者以某种形式将JavaScript编译为字节码的即时编译器。不用的浏览器使用不同的引擎如下所示： V8 — 开源，由 Google 开发，用 C ++ 编写 Rhino — 由 Mozilla 基金会管理，开源，完全用 Java 开发 SpiderMonkey — 是第一个支持 Netscape Navigator 的 JavaScript 引擎，目前正供 Firefox 使用 JavaScriptCore — 开源，以Nitro形式销售，由苹果为Safari开发 KJS — KDE 的引擎，最初由 Harri Porten 为 KDE 项目中的 Konqueror 网页浏览器开发 Chakra (JScript9) — Internet Explorer Chakra (JavaScript) — Microsoft Edge Nashorn, 作为 OpenJDK 的一部分，由 Oracle Java 语言和工具组编写 JerryScript — 物联网的轻量级引擎 因此我们看下javascript在V8引擎下的工作机制: javacript运行中，引擎首先解析javascript生成抽象语法树(AST), 然后生成机器码。 接着通过V8 的优化编译器 (TurboFan)的优化，把优化后的机器码推向后端(所谓的后端就是即时编译器JIT) 但是webassembly并不需要以上的全部步骤－如下所示是它被插入到执行过程示意图: 可以看到webassembly并不需要编译阶段，而是直接把编译后的字节码发送给后端，因此速度更快。 目前能编译成 WebAssembly 字节码的高级语言有： AssemblyScript:语法和 TypeScript 一致，对前端来说学习成本低，为前端编写 WebAssembly 最佳选择； C\\C++:官方推荐的方式，详细使用见文档; Rust:语法复杂、学习成本高，详细使用见文档; Kotlin:语法和 Java、JS 相似，语言学习成本低，详细使用见文档; Golang:语法简单学习成本低。详细使用见文档。 其他语言: 详细见文档。 Webassembly使用场景：总体而言，WASM不会替代JavaScript，但是却可以辅助解决很多JavaScript无法解决的问题。下面是Webassembly的一些场景： 浏览器场景： 更好的让一些语言和工具可以编译到 Web 平台运行。 图片/视频编辑。 游戏： 需要快速打开的小游戏 AAA 级，资源量很大的游戏。 游戏门户（代理/原创游戏平台） P2P 应用（游戏，实时合作编辑） 音乐播放器（流媒体，缓存） 图像识别 视频直播 VR 和虚拟现实 CAD 软件 科学可视化和仿真 互动教育软件和新闻文章。 模拟/仿真平台(ARC, DOSBox, QEMU, MAME, …)。 语言编译器/虚拟机。 非浏览器场景： 游戏分发服务（便携、安全）。 服务端执行不可信任的代码。 服务端应用。 移动混合原生应用。 多节点对称计算 下一步基于golang实现webassembly的demo","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"nodejs中package.json的常见字段","slug":"node-package-json","date":"2021-10-12T15:42:34.000Z","updated":"2023-01-14T07:39:00.854Z","comments":true,"path":"2021/10/12/node-package-json/","link":"","permalink":"https://github.com/fafucoder/2021/10/12/node-package-json/","excerpt":"","text":"参考文档 https://www.cnblogs.com/luowen075/p/10361211.html https://zhuanlan.zhihu.com/p/212832506 https://docs.npmjs.com/cli/v7/configuring-npm/package-json#browser","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://github.com/fafucoder/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://github.com/fafucoder/tags/nodejs/"}]},{"title":"网络加速方案dpdk","slug":"linux-dpdk","date":"2021-10-12T13:38:34.000Z","updated":"2021-12-17T07:42:48.341Z","comments":true,"path":"2021/10/12/linux-dpdk/","link":"","permalink":"https://github.com/fafucoder/2021/10/12/linux-dpdk/","excerpt":"","text":"参考文档 https://mp.weixin.qq.com/s/GaCbRbZJnCR9ZeRUsyZtAw https://blog.csdn.net/cloudvtech/article/details/80359834 https://www.zhihu.com/question/56820346","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"nodejs中require跟import的区别","slug":"node-require","date":"2021-10-12T13:25:22.000Z","updated":"2021-12-17T07:42:48.353Z","comments":true,"path":"2021/10/12/node-require/","link":"","permalink":"https://github.com/fafucoder/2021/10/12/node-require/","excerpt":"","text":"概述 require/exports 出生在野生规范当中，什么叫做野生规范？即这些规范是 JavaScript 社区中的开发者自己草拟的规则，得到了大家的承认或者广泛的应用。比如 CommonJS、AMD、CMD 等等。import/export 则是名门正派。TC39 制定的新的 ECMAScript 版本，即 ES6（ES2015）中包含进来。 关于 import 和 require 的不同，其实可以理解成 CommonJs 和 ES Module 的区别。这两者都是前端模块化的规范。 CommonJsNodejs 是 CommonJS 规范的主要实践者，在 CommonJs 里每个文件就是一个模块，有自己的作用域。在一个文件里面定义的变量、函数、类，都是私有的，对其他文件不可见。 12345678910class MyClass &#123; constructor(name) &#123; this.name = name; &#125;&#125;module.exports.MyClass = Myclassconst MyClass = require('./myclass.js')const obj = new MyClass('hello') ES ModuleES6 模块的设计思想是尽量的静态化，使得编译时就能确定模块的依赖关系，以及输入和输出的变量。所以ES6 模块不是对象，而是通过 export 命令显式指定输出的代码，再通过 import 命令输入。这种加载称为“编译时加载”或者静态加载，即 ES6 可以在编译时就完成模块加载，效率要比 CommonJS 模块的加载方式高。 12345678export default class MyClass &#123; constructor(name) &#123; this.name = name; &#125;&#125;import MyClass from './myclass'const obj = new MyClass('hello') 区别 命令 规范 调用 本质 特点 require CommonJS规范 运行时调用 赋值过程 非语言层面的标准。 社区方案，提供了服务器/浏览器的模块加载方案。只能在运行时确定模块的依赖关系及输入/输出的变量，无法进行静态优化。 import es6+的语法标准 编译时调用 解构过程 语言规格层面支持模块功能。支持编译时静态分析，便于JS引入宏和类型检验。动态绑定 关于调用 require的引用可以在代码的任何地方。 import语法规范上是放在文件开头。 参考文档 https://juejin.cn/post/6844903765489745927 // 值得一看 https://www.zhihu.com/question/56820346 // 值得一看 https://segmentfault.com/a/1190000023082896 // 值得一看","categories":[{"name":"nodejs","slug":"nodejs","permalink":"https://github.com/fafucoder/categories/nodejs/"}],"tags":[{"name":"nodejs","slug":"nodejs","permalink":"https://github.com/fafucoder/tags/nodejs/"}]},{"title":"linux top命令","slug":"linux-top","date":"2021-09-18T07:56:48.000Z","updated":"2021-12-17T07:42:48.352Z","comments":true,"path":"2021/09/18/linux-top/","link":"","permalink":"https://github.com/fafucoder/2021/09/18/linux-top/","excerpt":"","text":"top命令参数信息在命令top中可以方便的查看系统的cpu和内存信息(如下图所示), 然后你可能不清楚每一个字段代表的含义，现在就一块揭秘top命令中每个字段的含义 us：user time，表示 CPU 执行用户进程的时间，包括 nice 时间。通常都是希望用户空间CPU越高越好。 sy：system time，表示 CPU 在内核运行的时间，包括 IRQ 和 softirq。系统 CPU 占用越高，表明系统某部分存在瓶颈。通常这个值越低越好。 ni：nice time，具有优先级的用户进程执行时占用的 CPU 利用率百分比。 id：idle time，表示系统处于空闲期，等待进程运行。 wa：waiting time，表示 CPU 在等待 IO 操作完成所花费的时间。系统不应该花费大量的时间来等待 IO 操作，否则就说明 IO 存在瓶颈。 hi：hard IRQ time，表示系统处理硬中断所花费的时间。 si：soft IRQ time，表示系统处理软中断所花费的时间。 st：steal time，被强制等待（involuntary wait）虚拟 CPU 的时间，此时 Hypervisor 在为另一个虚拟处理器服务 cpu 平均负载Load Average​ 在上面的top命令中跟cpu相关的信息还有一个叫load average, load average这一列中的三个数值分别表示过去一分钟，五分钟，十五分钟这个节点上的load average。那么到底啥是load average呢? ​ 这里的load average表示对CPU资源需求的度量(说的很好，但是等于放屁~)，举个例子你可能就懂了，对于一个单个 CPU 的系统，如果在 1 分钟的时间里，处理器上始终有一个进程在运行，同时操作系统的进程可运行队列中始终都有 9 个进程在等待获取 CPU 资源。那么对于这 1 分钟的时间来说，系统的”load average”就是 1+9=10(这个定义对绝大部分的Unix 系统都适用)。 总结一句话就是: load average 等于单位时间内正在运行的进程+可运行队列的进程(注意，这里说的是unix系统)，因此： 第一，不论计算机 CPU 是空闲还是满负载，Load Average 都是 Linux 进程调度器中可运行队列（Running Queue）里的一段时间的平均进程数目。 第二，计算机上的 CPU 还有空闲的情况下，CPU Usage 可以直接反映到”load average”上，什么是 CPU 还有空闲呢？具体来说就是可运行队列中的进程数目小于 CPU个数，这种情况下，单位时间进程 CPU Usage 相加的平均值应该就是”load average”的值。 第三，计算机上的 CPU 满负载的情况下，计算机上的 CPU 已经是满负载了，同时还有更多的进程在排队需要 CPU 资源。这时”load average”就不能和 CPU Usage 等同了。比如对于单个 CPU 的系统，CPU Usage 最大只是有 100%，也就 1 个 CPU；而”loadaverage”的值可以远远大于 1，因为”load average”看的是操作系统中可运行队列中进程的个数。 对于linux系统来说： load average = 可运行队列的进程 + 处于TASK_UNINTERRUPTIBLE状态的进程(D 状态进程) TASK_UNINTERRUPTIBLE 是 Linux 进程状态的一种，是进程为等待某个系统资源而进入了睡眠的状态，并且这种睡眠的状态是不能被信号打断的。俗称D状态进程 因此，如果发现系统的load average很高，而CPU还是处于空闲状态，说明有很多进程处于阻塞状态，这时候得检查下代码写的是否有问题啦~(通过 ps 命令可以查阅D状态进程的信息) top命令原理分析了top命令中cpu字段的含义，你肯定还有疑问top命令中的数值是怎么计算出来的， 容器中如何查看cpu信息参考文档 https://github.com/silenceshell/topic // 容器中正确展示top信息","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubernetes pause容器解析","slug":"kubernetes-pause","date":"2021-09-13T06:25:36.000Z","updated":"2021-12-17T07:42:48.337Z","comments":true,"path":"2021/09/13/kubernetes-pause/","link":"","permalink":"https://github.com/fafucoder/2021/09/13/kubernetes-pause/","excerpt":"","text":"参考文档 https://zhuanlan.zhihu.com/p/66252461 //知乎专栏 https://cloud.tencent.com/developer/article/1375877 //腾讯云 https://github.com/kubernetes/kubernetes/tree/master/build/pause // pause 源码解析 https://o-my-chenjian.com/2017/10/17/The-Pause-Container-Of-Kubernetes/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"linux中的page cache","slug":"linux-pagecache","date":"2021-08-24T12:31:11.000Z","updated":"2021-12-17T07:42:48.350Z","comments":true,"path":"2021/08/24/linux-pagecache/","link":"","permalink":"https://github.com/fafucoder/2021/08/24/linux-pagecache/","excerpt":"","text":"图片 参考文档 https://spongecaptain.cool/SimpleClearFileIO/1.%20page%20cache.html","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux cgroup资源限制","slug":"linux-cgroup","date":"2021-08-24T07:05:13.000Z","updated":"2021-12-17T07:42:48.341Z","comments":true,"path":"2021/08/24/linux-cgroup/","link":"","permalink":"https://github.com/fafucoder/2021/08/24/linux-cgroup/","excerpt":"","text":"Cgroup简述​ 在Docker中，容器使用Linux namespace技术进行资源隔离，使得容器中的进程看不到别的容器的资源，但是容器内的进程仍然可以任意地使用主机的 CPU 内存等资源，如果某一个容器使用的主机资源过多，可能导致主机的资源竞争，进而影响业务。那如果我们想限制一个容器资源的使用（如 CPU、内存等）应该如何做呢？ ​ 这里就需要用到 Linux 内核的另一个核心技术cgroups。那么究竟什么是cgroups？cgroups（全称：Control Groups）是 Linux 内核的一个功能，它可以实现限制进程或者进程组的资源（如 CPU、内存、磁盘 IO 等）。 在 2006 年，Google 的工程师（ Rohit Seth 和 Paul Menage 为主要发起人） 发起了这个项目，起初项目名称并不是cgroups，而被称为进程容器（process containers）。在 2007 年cgroups代码计划合入Linux 内核，但是当时在 Linux 内核中，容器（container）这个词被广泛使用，并且拥有不同的含义。为了避免命名混乱和歧义，进程容器被重名为cgroups，并在 2008 年成功合入 Linux 2.6.24 版本中。cgroups目前已经成为 systemd、Docker、Linux Containers（LXC） 等技术的基础。 Cgroup 功能组件cgroups功能的实现依赖于三个核心概念：子系统、控制组、层级树。 控制组（cgroup）： 控制组是对进程分组管理的一种机制,一个Cgroup包含一组进程,并可以在上面添加添加Linux Subsystem的各种参数配置,将一组进程和一组Subsystem的系统参数关联起来。 子系统（subsystem）：是一个内核的组件，一个子系统代表一类资源调度控制器。例如内存子系统可以限制内存的使用量，CPU 子系统可以限制 CPU 的使用时间。不同版本的Kernel支持的子系统有所偏差,可以通过cat /proc/cgroups查看。 层级树（hierarchy）：是由一系列的控制组(Cgroup) 按照树状结构排列组成的。这种排列方式可以使得控制组(Cgroup) 拥有父子关系，子控制组默认拥有父控制组的属性，也就是子控制组会继承于父控制组。比如，系统中定义了一个控制组 c1，限制了 CPU 可以使用 1 核，然后另外一个控制组 c2 想实现既限制 CPU 使用 1 核，同时限制内存使用 2G，那么 c2 就可以直接继承 c1，无须重复定义 CPU 限制。每个 hierarchy在初始化时会有默认的CGroup(Root CGroup)。 任务（task）: 进程(process)在cgroups中称为task，taskid就是pid。 libcgroups：一个开源软件，提供了一组支持cgroups的应用程序和库，方便用户配置和使用cgroups。目前许多发行版都附带这个软件。 linux 支持的子系统不同版本的Kernel支持的子系统有所偏差,可以通过cat /proc/cgroups查看。 blkio 对块设备(比如硬盘)的IO进行访问限制 cpu 设置进程的CPU调度的策略, 比如CPU时间片的分配 cpuacct 统计/生成cgroup中的任务占用CPU资源报告 cpuset 在多核机器上分配给任务(task)独立的CPU和内存节点(内存仅使用于NUMA架构) devices 控制cgroup中对设备的访问 freezer 挂起(suspend) / 恢复 (resume)cgroup 中的进程 memory 用于控制cgroup中进程的占用以及生成内存占用报告 net_cls 使用等级识别符（classid）标记网络数据包，这让 Linux 流量控制器 tc (traffic controller) 可以识别来自特定 cgroup 的包并做限流或监控 net_prio 设置cgroup中进程产生的网络流量的优先级 hugetlb 限制使用的内存页数量 pids 限制任务的数量 ns 可以使不同cgroups下面的进程使用不同的namespace. 每个subsystem会关联到定义的cgroup上,并对这个cgoup中的进程做相应的限制和控制. 挂载cgroup 文件系统1234567891011# 创建目录mkdir mycgroup# 挂载cgroupmount -t cgroup -o none,name=mycgroup mycgroup `pwd`/mycgroup# 查看是否挂载成功mount -t cgroup | grep mycgroup# 创建子目录mkdir -p mycgroup/cgroup-1 &amp;&amp; mkdir -p mycgroup/cgroup-2 执行完上面的命令后查看mycgroup目录发现多了几个文件: 123456789101112131415161718192021[root@node1 mycgroup]# lscgroup-1 cgroup-2 cgroup.clone_children cgroup.procs cgroup.sane_behavior notify_on_release release_agent tasks[root@node1 mycgroup]# tree ..├── cgroup-1│ ├── cgroup.clone_children│ ├── cgroup.procs│ ├── notify_on_release│ └── tasks├── cgroup-2│ ├── cgroup.clone_children│ ├── cgroup.procs│ ├── notify_on_release│ └── tasks├── cgroup.clone_children├── cgroup.procs├── cgroup.sane_behavior├── notify_on_release├── release_agent└── tasks 上面这些文件就是hierarchy中cgroup根节点的配置项,这些文件的含义是: group.clone_children cpuset的subsystem会读取这个配置文件,如果这个值(默认值是0)是 1 子cgroup才会继承父cgroup的cpuset的配置 cgroup.procs是树中当前节点 cgroup 中的进程组ID,现在的位置是根节点,这个文件中会有现在系统中所有进程组的ID (查看目前全部进程PID ps -ef | awk &#39;{print $2}&#39;) notify_on_release 标志当这个cgroup最后一个进程退出的时候是否执行了release_agent (notify_on_release和release_agent 会一起使用) release_agent 则是一个路径,通常用作进程退出后自动清理不再使用的cgroup task 标识该cgroup下面进程ID,如果把一个进程ID写到task文件中,便会把相应的进程加入到这个cgroup中 在刚刚创建好的hierarchy上cgroup根节点中扩展出两个子cgroup, 它们会继承父cgroup的属性 通过subsystem限制cgroup中进程的资源在上面创建的 hierarchy并没有关联到任何的subsystem， 需要我们手动创建subsystem挂载上去。 1mkdir -p mycgroup/memory &amp;&amp; mount -t cgroup -o memory mycgroup-memory `pwd`/mycgroup/memory 执行完上面命令后在memory会生成很多文件(这些文件其实是继承至/sys/fs/cgroup/memory), 其实不需要我们手动挂载cgroup，下面将演示如何使用cgroup进行资源限制~ 如何使用cgroup进行资源限制cgroups的创建很简单，只需要在相应的子系统下创建目录即可(默认是在/sys/fs/cgroup目录下), 接下来将演示如何限制cpu跟内存使用数量 123[root@node1 cgroup]# lsblkio cpuacct cpuset freezer memory net_cls,net_prio perf_event rdmacpu cpu,cpuacct devices hugetlb net_cls net_prio pids systemd CPU 子系统CPU子系统有两个目录, cpuset和cpu,cpuacct, 其中cpu,cpuacct用于设置cpu配额，cpuset用于设置cpu绑定(设置进程只能在指定的核上), 下面对cpu 子系统的一些字段进行说明： cpu.cfs_period_us: 表示一个cpu带宽，单位为微秒(默认是10000000)。系统总CPU带宽： cpu核心数 * cfs_period_us。 cpu.cfs_quota_us 文件: 代表在某一个阶段限制的 CPU 时间总量，单位为微秒。cfs_quota_us为-1，表示使用的CPU不受cgroup限制。cfs_quota_us的最小值为1ms(1000)，最大值为1s。 结合cfs_period_us可以限制进程使用的cpu。例如配置cfs_period_us=10000，而cfs_quota_us=20000。那么该进程就可以可以用2个cpu core。 cpuacct.stat: 记录cgroup的所有任务（包括其子孙层级中的所有任务）使用的用户和系统CPU时间. cpuacct.usage: 记录这​​​个​​​cgroup中​​​所​​​有​​​任​​​务​​​（包括其子孙层级中的所有任务）消​​​耗​​​的​​​总​​​CPU时​​​间​​​（纳​​​秒​​​）。 cpuset.cpus: 指​​​定​​​允​​​许​​​这​​​个​​​ cgroup 中​​​任​​​务​​​(进程)访​​​问​​​的​​​ CPU。​​​这​​​是​​​一​​​个​​​用​​​逗​​​号​​​分​​​开​​​的​​​列​​​表​​​，格​​​式​​​为​​​ ASCII，使​​​用​​​小​​​横​​​线​​​（”-“）代​​​表​​​范​​​围​​​。​​​如下，代​​​表​​​ CPU 0、​​​1、​​​2 和​​​ 16。​​​ cpuset.mems: 指​​​定​​​允​​​许​​​这​​​个​​​ cgroup 中​​​任​​​务​​​可​​​访​​​问​​​的​​​内​​​存​​​节​​​点​​​。​​​这​​​是​​​一​​​个​​​用​​​逗​​​号​​​分​​​开​​​的​​​列​​​表​​​，格​​​式​​​为​​​ ASCII，使​​​用​​​小​​​横​​​线​​​（”-“）代​​​表​​​范​​​围​​​。​​​如下代​​​表​​​内​​​存​​​节​​​点​​​ 0、​​​1、​​​2 和​​​ 16。 创建CPU Cgroup 创建cpu子系统很简单，只需要在cpu子系统下创建一个目录即可。 1mkdir /sys/fs/cgroup/cpu/mydocker 执行完上述命令后，我们查看一下我们新创建的目录下发生了什么？ 1234567891011121314151617181920[root@node1 mydocker]# ls -l &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;mydockertotal 0-rw-r--r--. 1 root root 0 Aug 24 10:20 cgroup.clone_children-rw-r--r--. 1 root root 0 Aug 24 10:20 cgroup.procs-r--r--r--. 1 root root 0 Aug 24 10:20 cpuacct.stat-rw-r--r--. 1 root root 0 Aug 24 10:20 cpuacct.usage-r--r--r--. 1 root root 0 Aug 24 10:20 cpuacct.usage_all-r--r--r--. 1 root root 0 Aug 24 10:20 cpuacct.usage_percpu-r--r--r--. 1 root root 0 Aug 24 10:20 cpuacct.usage_percpu_sys-r--r--r--. 1 root root 0 Aug 24 10:20 cpuacct.usage_percpu_user-r--r--r--. 1 root root 0 Aug 24 10:20 cpuacct.usage_sys-r--r--r--. 1 root root 0 Aug 24 10:20 cpuacct.usage_user-rw-r--r--. 1 root root 0 Aug 24 10:20 cpu.cfs_period_us-rw-r--r--. 1 root root 0 Aug 24 08:52 cpu.cfs_quota_us-rw-r--r--. 1 root root 0 Aug 24 10:20 cpu.rt_period_us-rw-r--r--. 1 root root 0 Aug 24 10:20 cpu.rt_runtime_us-rw-r--r--. 1 root root 0 Aug 24 10:20 cpu.shares-r--r--r--. 1 root root 0 Aug 24 10:20 cpu.stat-rw-r--r--. 1 root root 0 Aug 24 10:20 notify_on_release-rw-r--r--. 1 root root 0 Aug 24 08:54 tasks 由上可以看到我们新建的目录下被自动创建了很多文件(（这里利用到了继承，子进程会继承父进程的 cgroup）) 创建进程，加入 cgroup 这里为了方便演示，我先把当前运行的 shell 进程加入 cgroup，然后在当前 shell 运行 cpu 耗时任务（这里利用到了继承，子进程会继承父进程的 cgroup）。 12345cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;cpu&#x2F;mydockerecho $$ &gt; tasksecho $$ 设置cpu quote配额 123# 设置当前shell 可以使用1core cpuecho 1000000 &gt; cpu.cpu.cfs_period_usecho 1000000 &gt; cpu.cfs_quota_us 观察cgroup是否生效 123456# 当前shellwhile true; do echo; done;# 另起一个shelltop -p pid(刚才的$$的数字) 通过top命令可以看到刚才的shell 进程cpu已经达到了100%，说明cgroup起作用了. 让我们更近一步，设置bash进程可以使用的cpu为0.5core, 通过echo 500000 &gt; cpu.cfs_quota_us 然后继续观察，发现cpu到达50%后就上不去了。验证完 cgroup 限制 cpu，我们使用相似的方法来验证 cgroup 对内存的限制。 Memory 子系统创建memory子系统的方式跟cpu子系统的方式差不多，只需要在memory子系统下创建一个目录即可。 在 memory 子系统下创建 cgroup 1mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;mydocker 执行完上述命令后，我们查看一下我们新创建的目录下发生了什么？ 123456789101112131415161718192021222324252627282930313233[root@node1 mydocker]# ls -l &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;mydockertotal 0-rw-r--r--. 1 root root 0 Aug 24 23:34 cgroup.clone_children--w--w--w-. 1 root root 0 Aug 24 23:34 cgroup.event_control-rw-r--r--. 1 root root 0 Aug 24 23:34 cgroup.procs-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.failcnt--w-------. 1 root root 0 Aug 24 23:34 memory.force_empty-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.failcnt-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.limit_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.max_usage_in_bytes-r--r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.slabinfo-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.tcp.failcnt-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.tcp.limit_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.tcp.max_usage_in_bytes-r--r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.tcp.usage_in_bytes-r--r--r--. 1 root root 0 Aug 24 23:34 memory.kmem.usage_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.limit_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.max_usage_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.memsw.failcnt-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.memsw.limit_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.memsw.max_usage_in_bytes-r--r--r--. 1 root root 0 Aug 24 23:34 memory.memsw.usage_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.move_charge_at_immigrate-r--r--r--. 1 root root 0 Aug 24 23:34 memory.numa_stat-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.oom_control----------. 1 root root 0 Aug 24 23:34 memory.pressure_level-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.soft_limit_in_bytes-r--r--r--. 1 root root 0 Aug 24 23:34 memory.stat-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.swappiness-r--r--r--. 1 root root 0 Aug 24 23:34 memory.usage_in_bytes-rw-r--r--. 1 root root 0 Aug 24 23:34 memory.use_hierarchy-rw-r--r--. 1 root root 0 Aug 24 23:34 notify_on_release-rw-r--r--. 1 root root 0 Aug 24 23:34 tasks 由上可以看到我们新建的目录下被自动创建了很多文件(这里利用到了继承，子进程会继承父进程的 cgroup), 其中 memory.limit_in_bytes 文件代表内存使用总量，单位为 byte。例如，这里我希望对内存使用限制为 1G(1G = 102410241024)，则向 memory.limit_in_bytes 文件写入 1073741824即可 创建进程，加入 cgroup 12345cd &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;memory&#x2F;mydockerecho $$ &gt; tasksecho $$ 设置memory 配额 12# 设置当前shell 可以使用1G内存echo 1073741824 &gt; memory.limit_in_bytes 观察cgroup是否生效 12# 使用stress-ng进行压力测试stress-ng --vm 5 --vm-bytes 250M 通过运行stress-ng压力测试后，能发现终端会直接被卡死(其实无法说明Cgroup是否生效， 使用pidstat也不好查看内存占用情况) pids子系统参考文档 使用cgroups控制进程cpu配额 //图不错 一文彻底搞懂Linux Cgroup如何限制容器资源 //可以看看 资源限制：如何通过 Cgroups 机制实现资源限制？ //推荐动手实验 Cgroup中的CPU资源控制 //cgroup各个字段的含义","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux共享内存和内存零拷贝技术","slug":"linux-sharememory","date":"2021-08-23T13:56:16.000Z","updated":"2021-12-17T07:42:48.351Z","comments":true,"path":"2021/08/23/linux-sharememory/","link":"","permalink":"https://github.com/fafucoder/2021/08/23/linux-sharememory/","excerpt":"","text":"进程间通信(IPC)每个进程各自有不同的用户地址空间，任何一个进程的全局变量在另一个进程中都看不到，所以进程之间要交换数据必须通过内核，在内核中开辟一块缓冲区，进程1把数据从用户空间拷到内核缓冲区，进程2再从内核缓冲区把数据读走，内核提供的这种机制称为进程间通信（IPC，InterProcess Communication） 常见的进程间通信的方式有如下： 管道/匿名管道(pipe)：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。例如 ls -l | grep string就是一个管道 信号量(signal)： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 套接字(socket): 套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同主机的进程通信。 消息队列(MessageQueue): 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 共享内存(shareMemory)：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式。 共享内存共享内存就是允许两个或多个进程共享一定的存储区。就如同 malloc() 函数向不同进程返回了指向同一个物理内存区域的指针。当一个进程改变了这块地址中的内容的时候，其它进程都会察觉到这个更改。因为数据不需要在客户机和服务器端之间复制，数据直接写到内存，不用若干次数据拷贝，所以这是最快的一种IPC(进程间通信)。 共享内存是IPC通信当中传输速度最快的通信方式没有之一，但是共享内存并未提供同步机制，也就是说，在一个服务进程结束对共享内存的写操作之前，并没有自动机制可以阻止另一个进程（客户进程）开始对它进行读取。 因此常用会通过信号量来实现对共享内存同步访问控制。 常见的共享内存函数 创建共享内存——&gt;shmget() 函数 操作共享内存———&gt;shmctl()函数 挂接操作———&gt;shmat()函数 分离操作———&gt;shmdt()函数 内存零拷贝参考文档 https://mp.weixin.qq.com/s/GMCi0tA-syEliTYzjrv6Mw // 分布式实验室 https://www.cnblogs.com/wuyepeng/p/9748889.html // Linux下进程间通信方式——共享内存","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"使用LVM实现linux扩容","slug":"linux-lvm","date":"2021-08-15T14:06:47.000Z","updated":"2021-12-27T06:23:19.108Z","comments":true,"path":"2021/08/15/linux-lvm/","link":"","permalink":"https://github.com/fafucoder/2021/08/15/linux-lvm/","excerpt":"","text":"概述​ 玩过虚拟机的同学肯定遇到过磁盘容量100%而无法安装新的软件这一问题(因为磁盘分配不合理或者磁盘挂载的盘大小太小了导致的)， 磁盘空间不足的通常方法就是挂在一块新的盘到虚拟机，然后把占用磁盘空间多的目录挂载到这块新的盘上去(我之前就都是这么干的^^), 或者使用LVM来实现磁盘的扩容(使用LVM扩容才是正解 _^ ), 下面就来了解下LVM的知识，已经使用lvm如何实现磁盘扩容吧。 参考文档 使用LVM //骏马金龙老师的文章，推荐阅读 https://www.cnblogs.com/tiantianhappy/p/10143663.html centos扩容 https://aurthurxlc.github.io/Aurthur-2017/Centos-7-extend-lvm-volume.html 扩容","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux 文件挂载mount","slug":"linux-mount","date":"2021-08-15T13:24:00.000Z","updated":"2021-12-17T07:42:48.343Z","comments":true,"path":"2021/08/15/linux-mount/","link":"","permalink":"https://github.com/fafucoder/2021/08/15/linux-mount/","excerpt":"","text":"mount 简介mount用来显示挂载信息或者进行文件系统挂载。mount并非只能挂载文件系统，也可以将目录挂载到另一个目录下，其实它实现的是目录【硬链接】，默认情况下，是无法对目录建立硬链接的，但是通过mount可以完成绑定，绑定后两个目录的inode号是完全相同的，但尽管建立的是目录的【硬链接】，但其实也仅是拿来当软链接用。 mount 常用命令mount123456789101112131415161718192021222324mount [-t vfstype ] [-o options] device directory-a 将/etc/fstab文件里指定的挂载选项重新挂载一遍。-t 支持ext2/ext3/ext4/vfat/fat/iso9660(光盘默认格式)。不用-t时默认会调用blkid来获取文件系统类型。-n 不把挂载记录写在/etc/mtab文件中，一般挂载会在/proc/mounts中记录下挂载信息，然后同步到/etc/mtab，指定-n表示不同步该挂载信息。-o 指定挂载特殊选项。下面是两个比较常用的： loop 挂载镜像文件，如iso文件 ro 只读挂载 rw 读写挂载 auto 相当于mount -a dev 如果挂载的文件系统中有设备访问入口则启用它，使其可以作为设备访问入口 default rw,suid,dev,exec,auto,nouser,async,and relatime async 异步挂载，只写到内存 sync 同步挂载，通过挂载位置写入对方硬盘 atime 修改访问时间，每次访问都修改atime会导致性能降低，所以默认是noatime noatime 不修改访问时间，高并发时使用这个选项可以减少磁盘IO nodiratime 不修改文件夹访问时间，高并发时使用这个选项可以减少磁盘IO exec/noexec 挂载后的文件系统里的可执行程序是否可执行，默认是可以执行exec，优先级高于权限的限定 remount 重新挂载，此时可以不用指定挂载点。 suid/nosuid 对挂载的文件系统启用或禁用suid，对于外来设备最好禁用suid _netdev 需要网络挂载时默认将停留在挂载界面直到加载网络了。使用_netdev可以忽略网络正常挂载。如NFS开机挂载。 user 允许普通用户进行挂载该目录，但只允许挂载者进行卸载该目录 users 允许所有用户挂载和卸载该目录 nouser 禁止普通用户挂载和卸载该目录，这是默认的，默认情况下一个目录不指定user/users时，将只有root能挂载 umount1umount device&#x2F;directory mount bind 命令mount bind可为当前挂载点绑定一个新的挂载点。 执行如下命令，可创建foo目录的一个镜像目录bar，它们已经绑定在一起： 12mkdir foo barmount --bind foo bar mount bind绑定后的两个目录类似于硬链接，无论读写bar还是读写foo，都会反应在另一方，内核在底层所操作的都是同一个物理位置。将bar卸载后，bar目录回归原始空目录状态，期间所执行的修改都保留在foo目录下。 12345[root@centos3 mount]# echo \"hello foo\" &gt; foo/hello[root@centos3 mount]# ls barhello[root@centos3 mount]# cat bar/hellohello foo mount bind除了可以绑定两个普通目录，还可以绑定挂载点。 1mount --bind /mnt/foo /mnt/bar shared subtreesnux的每个挂载点都具有一个决定该挂载点是否共享子挂载点的属性，称为shared subtrees(注：这是挂载点的属性，就像是否只读一样的属性)。该属性用于决定某挂载点之下新增或移除子挂载点时，是否同步影响bind另一端的挂载点。 hared subtrees有四种属性值，它们的设置方式分别为： 123456789101112# 挂载前直接设置shared subtrees属性mount --make-private --bind &lt;olddir&gt; &lt;newdir&gt;mount --make-shared --bind &lt;olddir&gt; &lt;newdir&gt;mount --make-slave --bind &lt;olddir&gt; &lt;newdir&gt;mount --make-unbindable --bind &lt;olddir&gt; &lt;newdir&gt;# 或者挂载后设置挂载点属性mount --bind &lt;olddir&gt; &lt;newdir&gt;mount --make-private &lt;newdir&gt;mount --make-shared &lt;newdir&gt; mount --make-slave &lt;newdir&gt;mount --make-unbindable &lt;newdir&gt; 对于shared subtrees这几种属性值，以mount –bind foo bar为例： private属性：表示在foo或bar下新增、移除子挂载点，不会体现在另一方，即foo &lt;-x-&gt; barshared属性：表示在foo或bar下新增、移除子挂载点，都会体现在另一方，即foo &lt;–&gt; barslave属性：类似shared，但只是单向的，foo下新增或移除子挂载点会体现在bar中，但bar中新增或移除子挂载点不会影响foo，即，即foo –&gt; bar, bar -x-&gt; foounbindable属性：表示挂载点bar目录将无法执行bind操作关联到其它挂载点 /etc/fstab与/etc/mtab和/proc/mounts区别/etc/fstab/etc/fstab是用来存放文件系统的静态信息的文件。当系统启动的时候，系统会自动地从这个文件读取信息，并且会自动将此文件中指定的文件系统挂载到指定的目录。(/etc/fstab 是只读不写的) /etc/mtab/etc/mtab是当前的分区挂载情况，记录的是当前系统已挂载的分区。每次挂载/卸载分区时会更新/etc/mtab文件中的信息(执行mount命令会改变/etc/mtab的信息)。 /proc/mounts/proc/mounts是mount namespace而引入的，功能跟/etc/mtab一样，都是记录当前分区的挂载情况。因为记录 mount 信息的 /etc/mtab 是全局的，也就是说，即使某个进程有自己的 namespace，但只要还和外面共享同一个 /etc/mtab，那么，里面进行umount/mount操作的信息也会被记录到/etc/mtab里，外面也会看到。 参考文档 /etc/mtab与/proc/mounts //mtab mounts fstab这三个文件的区别 man Linux manual page //man mount 命令 mount bind功能详解 // 骏马金龙老师的文章都很不错的","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux 流量控制TC","slug":"linux-tc","date":"2021-08-13T12:14:39.000Z","updated":"2021-12-17T07:42:48.352Z","comments":true,"path":"2021/08/13/linux-tc/","link":"","permalink":"https://github.com/fafucoder/2021/08/13/linux-tc/","excerpt":"","text":"参考文档 https://www.ituring.com.cn/article/274014 https://int64.me/2018/TC%20-%20Linux%20%e6%b5%81%e9%87%8f%e6%8e%a7%e5%88%b6%e5%b7%a5%e5%85%b7.html https://tonydeng.github.io/sdn-handbook/linux/tc.html https://www.cnblogs.com/sunsky303/p/14250452.html https://davidlovezoe.club/wordpress/archives/952#TC%E5%92%8CBPF%E4%BA%B2%E5%AF%86%E5%90%88%E4%BD%9C https://cloud.tencent.com/developer/article/1367795 https://man7.org/linux/man-pages/man8/tc-netem.8.html // tc netum https://cizixs.com/2017/10/23/tc-netem-for-terrible-network/ //这位大佬有很多牛皮文章~","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"docker 优雅退出","slug":"docker-lifecycle","date":"2021-08-11T13:03:13.000Z","updated":"2021-12-17T07:42:48.324Z","comments":true,"path":"2021/08/11/docker-lifecycle/","link":"","permalink":"https://github.com/fafucoder/2021/08/11/docker-lifecycle/","excerpt":"","text":"参考文档 https://www.cnblogs.com/cuishuai/p/14859182.html https://www.cnblogs.com/37yan/p/12936026.html https://v1-18.docs.kubernetes.io/zh/docs/concepts/containers/container-lifecycle-hooks/ https://zhuanlan.zhihu.com/p/59796137?from_voters_page=true","categories":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/tags/docker/"}]},{"title":"docker 镜像是怎样炼成的","slug":"docker-image","date":"2021-07-28T12:10:35.000Z","updated":"2021-12-17T07:42:48.324Z","comments":true,"path":"2021/07/28/docker-image/","link":"","permalink":"https://github.com/fafucoder/2021/07/28/docker-image/","excerpt":"","text":"Docker 简介Docker 是一个构建，发布和运行应用程序的开放平台。Docker 以容器为资源分隔和调度的基本单位，容器封装了整个项目运行时所需要的所有环境，通过 Docker 你可以将应用程序与基础架构分离，像管理应用程序一样管理基础架构，以便快速完成项目的部署与交付(docker 本质还是一个进程，只不过这个进程被关进了小黑屋)。 docker 与传统的虚拟化技术区别传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，进而在该系统上运行所需应用进程；而 Docker 容器内的应用进程则是直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟，因此要比传统虚拟机更为轻便。 关于什么是Hypervisor, 维基百科是这样说的：Hypervisor，又称虚拟机监控器（英语：virtual machine monitor，缩写为 VMM），是用来创建与运行虚拟机的软件、固件或硬件。 Docker 架构与核心概念Docker 使用 client-server 架构， Docker 客户端将命令发送给 Docker 守护进程，后者负责构建，运行和分发 Docker 容器。 Docker 客户端和守护程序使用 REST API，通过 UNIX 套接字或网络接口进行通信。 docker常见命令Docker 提供了大量命令用于管理镜像、容器和服务，命令的统一使用格式为：docker [OPTIONS] COMMAND 1. 基础命令 docker version：用于查看 docker 的版本信息 docker info：用于查看 docker 的配置信息 docker help：用于查看帮助信息 2. 镜像相关命令 docker search 镜像名：从官方镜像仓库 Docker Hub 查找指定名称的镜像。常用参数为 --no-trunc，代表显示完整的镜像信息。 docker images: 列出所有顶层镜像的相关信息。 docker pull 镜像名 [:TAG]：从官方仓库下载镜像，:TAG 为镜像版本，不加则默认下载最新版本。 docker rmi 镜像名或ID [:TAG]: 删除指定版本的镜像，不加 :TAG 则默认删除镜像的最新版本。 docker inspect 镜像名或ID [:TAG]: 查看镜像的详细信息 docker push 镜像名或ID [:TAG]: 推送镜像到镜像仓库 docker image save: 保存镜像(为tar包) docker image load: 加载镜像(从tar包或者标准输入) 3. 容器相关命令 docker run [OPTIONS] IMAGE [COMMAND] [ARG…]: 新建并启动容器 docker ps [OPTIONS]: 列出当前所有正在运行的容器。 docker start|restart|stop|kill 容器名或ID: start 命令用于启动已有的容器，restart 用于重启正在运行的容器，stop 用于停止正在运行的容器，kill 用于强制停止容器 docker rm 容器名或ID: 删除已停止的容器 docker insepct 容器名或ID: 查看容器或者镜像的详细信息 docker exec -it 容器名或ID sh|/bin/bash: 进入正在运行的容器，与容器主进程交互 docker logs 容器名或ID: 查看容器日志 docker commit container imageName:tag: 从容器创建一个新的镜像 下面这张图是docker 容器的状态流转图，方便助记: 镜像是怎样炼成的从OCI规范说起OCI 即Open Container Initiative, linux基金会与2015年6月成立的组织，旨在围绕容器格式和运行时制定一个开放的工业化标准。目前 OCI 主要有三个规范：运行时规范 runtime-spec ，镜像规范 image-spec 以及不常见的镜像仓库规范 distribution-spec 。 Image SpecOCI 规范中的镜像规范(image-spec) 决定了我们的镜像按照什么标准来构建，以及构建完镜像之后如何存放，接着下文提到的 Dockerfile 则决定了镜像的 layer 内容以及镜像的一些元数据信息。一个镜像规范 image-spec 和一个 Dockerfile 就指导着我们构建一个镜像，那么接下来我们就简单了解一下这个镜像规范，看看镜像是长什么样子的，对镜像有个大体的主观认识。官方的image spec主要由以下文件组成: 12345678910111213├── annotations.md # 注解规范├── config.md # image config 文件规范├── considerations.md # 注意事项├── conversion.md # 转换为 OCI 运行时├── descriptor.md # OCI Content Descriptors 内容描述├── image-index.md # manifest list 文件├── image-layout.md # 镜像的布局├── implementations.md # 使用 OCI 规范的项目├── layer.md # 镜像层 layer 规范├── manifest.md # manifest 规范├── media-types.md # 文件类型├── README.md # README 文档├── spec.md # OCI 镜像规范的概览 Layer文件系统：以 layer （镜像层）保存的文件系统，每个 layer 保存了和上层之间变化的部分，layer 应该保存哪些文件，怎么表示增加、修改和删除的文件等。 Image configimage config 文件: 保存了文件系统的层级信息（每个层级的 hash 值，以及历史信息），以及容器运行时需要的一些信息（比如环境变量、工作目录、命令参数、mount 列表），指定了镜像在某个特定平台和系统的配置(比较接近我们使用 docker inspect &lt;image_id&gt; 看到的内容, 通过docker save一个镜像后解压，里面会有config信息)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&#123; \"architecture\": \"amd64\", \"config\": &#123; \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"DOCKER_VERSION=20.10.6\", \"DOCKER_TLS_CERTDIR=/certs\" ], \"Entrypoint\": [ \"docker-entrypoint.sh\" ], \"Cmd\": [ \"sh\" ], \"OnBuild\": null &#125;, \"created\": \"2021-07-22T07:20:09.896531Z\", \"history\": [ &#123; \"created\": \"2021-04-14T19:19:39.267885491Z\", \"created_by\": \"/bin/sh -c #(nop) ADD file:8ec69d882e7f29f0652d537557160e638168550f738d0d49f90a7ef96bf31787 in / \" &#125;, &#123; \"created\": \"2021-04-14T19:19:39.643236135Z\", \"created_by\": \"/bin/sh -c #(nop) CMD [\\\"/bin/sh\\\"]\", \"empty_layer\": true &#125;, &#123; \"created\": \"2021-04-14T20:16:58.64055643Z\", \"created_by\": \"/bin/sh -c apk add --no-cache \\t\\tca-certificates \\t\\topenssh-client\" &#125;, &#123; \"created\": \"2021-04-14T20:16:59.651022704Z\", \"created_by\": \"/bin/sh -c [ ! -e /etc/nsswitch.conf ] &amp;&amp; echo 'hosts: files dns' &gt; /etc/nsswitch.conf\" &#125;, &#123; \"created\": \"2021-04-14T20:16:59.839384238Z\", \"created_by\": \"/bin/sh -c #(nop) ENV DOCKER_VERSION=20.10.6\", \"empty_layer\": true &#125;, &#123; \"created\": \"2021-04-14T20:17:06.162866201Z\", \"created_by\": \"/bin/sh -c set -eux; \\t\\tapkArch=\\\"$(apk --print-arch)\\\"; \\tcase \\\"$apkArch\\\" in \\t\\t'x86_64') \\t\\t\\turl='https://download.docker.com/linux/static/stable/x86_64/docker-20.10.6.tgz'; \\t\\t\\t;; \\t\\t'armhf') \\t\\t\\turl='https://download.docker.com/linux/static/stable/armel/docker-20.10.6.tgz'; \\t\\t\\t;; \\t\\t'armv7') \\t\\t\\turl='https://download.docker.com/linux/static/stable/armhf/docker-20.10.6.tgz'; \\t\\t\\t;; \\t\\t'aarch64') \\t\\t\\turl='https://download.docker.com/linux/static/stable/aarch64/docker-20.10.6.tgz'; \\t\\t\\t;; \\t\\t*) echo &gt;&amp;2 \\\"error: unsupported architecture ($apkArch)\\\"; exit 1 ;; \\tesac; \\t\\twget -O docker.tgz \\\"$url\\\"; \\t\\ttar --extract \\t\\t--file docker.tgz \\t\\t--strip-components 1 \\t\\t--directory /usr/local/bin/ \\t; \\trm docker.tgz; \\t\\tdockerd --version; \\tdocker --version\" &#125;, &#123; \"created\": \"2021-04-14T20:17:06.643076836Z\", \"created_by\": \"/bin/sh -c #(nop) COPY file:abb137d24130e7fa2bdd38694af607361ecb688521e60965681e49460964a204 in /usr/local/bin/modprobe \" &#125;, &#123; \"created\": \"2021-04-14T20:17:06.854234992Z\", \"created_by\": \"/bin/sh -c #(nop) COPY file:5b18768029dab8174c9d5957bb39560bde5ef6cba50fbbca222731a0059b449b in /usr/local/bin/ \" &#125;, &#123; \"created\": \"2021-04-14T20:17:07.047323417Z\", \"created_by\": \"/bin/sh -c #(nop) ENV DOCKER_TLS_CERTDIR=/certs\", \"empty_layer\": true &#125;, &#123; \"created\": \"2021-04-14T20:17:08.025897909Z\", \"created_by\": \"/bin/sh -c mkdir /certs /certs/client &amp;&amp; chmod 1777 /certs /certs/client\" &#125;, &#123; \"created\": \"2021-04-14T20:17:08.214397968Z\", \"created_by\": \"/bin/sh -c #(nop) ENTRYPOINT [\\\"docker-entrypoint.sh\\\"]\", \"empty_layer\": true &#125;, &#123; \"created\": \"2021-04-14T20:17:08.435794313Z\", \"created_by\": \"/bin/sh -c #(nop) CMD [\\\"sh\\\"]\", \"empty_layer\": true &#125;, &#123; \"created\": \"2021-07-22T07:20:09.896531Z\", \"created_by\": \"RUN /bin/sh -c apk add curl # buildkit\", \"comment\": \"buildkit.dockerfile.v0\" &#125; ], \"os\": \"linux\", \"rootfs\": &#123; \"type\": \"layers\", \"diff_ids\": [ \"sha256:b2d5eeeaba3a22b9b8aa97261957974a6bd65274ebd43e1d81d0a7b8b752b116\", \"sha256:91e3b96418079299100f88ca50988325e1f397f49d9c49c30b511e947a49bcd3\", \"sha256:563ea45970c143d16be5c97173d2720cc31f38e2892aa0bd9075e5bede9dc334\", \"sha256:9e9604462a57778a1d4b47a01039ee53fe29bd0e332f3f95a2cd20098fec4504\", \"sha256:241588b2373ed654409b0d0881a92b801f8749cb21e3c5ed10d1d82d0a4434de\", \"sha256:4bc13ba6e7d2ba7a0ff8df8f3ddab9893741d0a51f73659e2d0cf731510fb385\", \"sha256:b82f54db960be856743e9edecf13f04e8e650586b5bf6cc829b3efafdcc6aca5\", \"sha256:dd1b694543daae59c44b849b4ae37e82c722b8008a9b2a740ebc698702be7d0c\" ] &#125;&#125; manifestmanifest 文件 ：镜像的 config 文件索引，有哪些 layer，额外的 annotation 信息等。manifest 文件是存放在 registry 中，当我们拉取镜像的时候，会根据该文件拉取相应的 layer。 manifest 是一个 JSON 文件，其定义包括两个部分，分别是Config 和 Layers。Config 是一个 JSON 对象，Layers 是一个由 JSON 对象组成的数组。Config 与 Layers 中的每一个对象的结构相同，都包括三个字段，分别是 digest、mediaType 和 size。其中 digest 可以理解为是这一对象的 ID。mediaType 表明了这一内容的类型。size 是这一内容的大小。 容器镜像的 Config 有着固定的 mediaType application/vnd.oci.image.config.v1+json。一个 Config 的示例配置如下，它记录了关于容器镜像的配置，可以理解为是镜像的元数据。通常它会被镜像仓库用来在 UI 中展示信息，以及区分不同操作系统的构建等。 而容器镜像的 Layers 是由多层 mediaType 为 application/vnd.oci.image.layer.v1.*（其中最常见的是 application/vnd.oci.image.layer.v1.tar+gzip) 的内容组成的。众所周知，容器镜像是分层构建的，每一层就对应着 Layers 中的一个对象。 容器镜像的 Config，和 Layers 中的每一层，都是以 Blob 的方式存储在镜像仓库中的，它们的 digest 作为 Key 存在。因此，在请求到镜像的 Manifest 后，Docker 会利用 digest 并行下载所有的 Blobs，其中就包括 Config 和所有的 Layers。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&#123; \"schemaVersion\": 2, \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"config\": &#123; \"mediaType\": \"application/vnd.docker.container.image.v1+json\", \"size\": 4203, \"digest\": \"sha256:08bdaf2f88f90320cd3e92a469969efb1f066c6d318631f94e7864828abd7c75\" &#125;, \"layers\": [ &#123; \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 2811969, \"digest\": \"sha256:540db60ca9383eac9e418f78490994d0af424aab7bf6d0e47ac8ed4e2e9bcbba\" &#125;, &#123; \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 2050156, \"digest\": \"sha256:5a38b3726f4b24fa93b80450be63ad67fd3239c2f3b83695118d7b1a88447d84\" &#125;, &#123; \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 154, \"digest\": \"sha256:e5fa5deb334027202841b051d10e7c7137fa3b63e97734309cedf6b48804df5f\" &#125;, &#123; \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 70247077, \"digest\": \"sha256:c521bbbd9945cde2415eae95c3a2a604ea5ca18eb86ab5cadeb589d3b8a13185\" &#125;, &#123; \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 545, \"digest\": \"sha256:59707d521c45c42bbb7dbbef463d5d3800859f20122fb54894f0c79d9b435483\" &#125;, &#123; \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 1015, \"digest\": \"sha256:26abe223b186967eef77715b2c6efd147f3a203a0c7b19e61a8947dab4236397\" &#125;, &#123; \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 150, \"digest\": \"sha256:9d806bc203610fb061281b581a0b2522c9fcbd15e55c55ac0c496c9b1dbe63b0\" &#125; ]&#125; 小结看完 image-spec 里面提到的各种 id 相信你又很多疑惑，在此总结一下这些 id 的作用(看完还是很懵逼)： image-id image config 的 sha256 哈希值，在本地镜像存储中由它唯一标识一个镜像 image digest 在 registry 中的 image manifest 的 sha256 哈希值，在 registry 中由它唯一标识一个镜像 diff_ids 镜像每一层的 id ，是对本地镜像中 layer 的 tar 包的 sha256 哈希值 layer digest 镜像在 registry 存储中的 id ，是对远程 layer的 tar 包的 sha256 哈希值 镜像的 image config 中的 rootfs 字段记录了每一层 layer 的 id，而镜像的 layer id 则是 layer tar 包的 sha256 值，如果镜像的 layer 改变，则这个 layer id 会改变，而记录它的 image config 内容也会改变，image config 内容变了，image config 文件的 sha256 值也就会改变，这样就可以由 image id 和 image digest 唯一标识一个镜像，达到防治篡改的安全目的。 Runtime Spec容器运行时(Runtime Spec) 决定了根据镜像怎么运行一个容器，规范指定了容器的配置、执行环境和生命周期。其中容器的配置包含了创建和运行容器所需的元数据，包括要运行的进程、环境变量、资源限制和要使用的沙箱功能等。官方的Spec主要内容如下： 12345├── bundle.md # 将容器编码为文件系统包的格式，一组以某种方式组织的文件├── config.md # 容器运行所需的元数据。包括要运行的进程、要注入的环境变量、要使用的沙盒功能等├── runtime.md # 容器的生命周期├── README.md # README 文档├── spec.md # OCI 运行时规范的概览 Runtimeruntime文件： 规定了state文件包含的内容，以及容器从创建到停止的事件。总结容器运行时主要做一下几方面工作： 容器镜像管理 容器生命周期管理 容器创建 容器资源管理 市面上常见的容器运行时有runc, rkt, lxc, containerd等。 关于Rootfs与Bootfs一个典型的 Linux 系统要能运行的话，它至少需要两个文件系统, rootfs和bootfs. BootfsBootfs 包含BootLoader(引导加载程序)和kernel(内核)。回忆下操作系统的启动过程，当按下电脑的开机键之后，电脑首先进行加电自检(检查硬件是否有问题), 接着就是GRUB引导，然后是内核加载，内核加载完成后进行init初始化(内核启动第一个用户空间程序)，bootfs的作用就是把引导程序和把/boot文件系统加载进内核的过程，当内核都会被加载进内存后，此时 bootfs 会被卸载掉从而释放出所占用的内存。 Rootfsrootfs就是root文件系统，包含的就是典型的Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。 Docker镜像是由文件系统叠加而成。最低端是bootfs，并使用宿主机的bootfs（docker中操作系统启动几秒钟，原因就是，通过docker镜像启动的操作系统，底层使用的是docker宿主机的bootfs不需要重新加载bootfs), 第二层是root文件系统rootfs 称为base image(基础镜像)。然后可以再往上叠加其它镜像文件，每一层就是一个layer(每一层都是只读的)。 当从一个镜像启动容器时，docker会使用联合文件系统把多个不同位置的目录(layer)联合挂载（union mount）到同一个目录下，然后会在最顶层加载一个读写文件系统作为容器。(联合文件系统下一个章节会说明) Dockerfile众所周知 docker 镜像需要一个 Dockerfile 来构建而成，当我们对 OCI 镜像规范和Rootfs,Bootfs有了个大致的了解之后，我们接下来就拿着 Dockerfile 这个 ”图纸“ 去一步步构建镜像。 下面是 Nodejs 12.22.4 版本的Dockerfile: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798FROM alpine:3.13ENV NODE_VERSION 12.22.4RUN addgroup -g 1000 node \\ &amp;&amp; adduser -u 1000 -G node -s /bin/sh -D node \\ &amp;&amp; apk add --no-cache \\ libstdc++ \\ &amp;&amp; apk add --no-cache --virtual .build-deps \\ curl \\ &amp;&amp; ARCH= &amp;&amp; alpineArch=\"$(apk --print-arch)\" \\ &amp;&amp; case \"$&#123;alpineArch##*-&#125;\" in \\ x86_64) \\ ARCH='x64' \\ CHECKSUM=\"2fa5020bab6501cd3e6fb90dae0cd19a64df80bb8e8bf184af4f562df5a08b4f\" \\ ;; \\ *) ;; \\ esac \\ &amp;&amp; if [ -n \"$&#123;CHECKSUM&#125;\" ]; then \\ set -eu; \\ curl -fsSLO --compressed \"https://unofficial-builds.nodejs.org/download/release/v$NODE_VERSION/node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\"; \\ echo \"$CHECKSUM node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" | sha256sum -c - \\ &amp;&amp; tar -xJf \"node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" -C /usr/local --strip-components=1 --no-same-owner \\ &amp;&amp; ln -s /usr/local/bin/node /usr/local/bin/nodejs; \\ else \\ echo \"Building from source\" \\ # backup build &amp;&amp; apk add --no-cache --virtual .build-deps-full \\ binutils-gold \\ g++ \\ gcc \\ gnupg \\ libgcc \\ linux-headers \\ make \\ python2 \\ # gpg keys listed at https://github.com/nodejs/node#release-keys &amp;&amp; for key in \\ 4ED778F539E3634C779C87C6D7062848A1AB005C \\ 94AE36675C464D64BAFA68DD7434390BDBE9B9C5 \\ 74F12602B6F1C4E913FAA37AD3A89613643B6201 \\ 71DCFD284A79C3B38668286BC97EC7A07EDE3FC1 \\ 8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 \\ C4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8 \\ C82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C \\ DD8F2338BAE7501E3DD5AC78C273792F7D83545D \\ A48C2BEE680E841632CD4E44F07496B3EB3C1762 \\ 108F52B48DB57BB0CC439B2997B01419BD92F80A \\ B9E2F5981AA6E0CD28160D9FF13993A75599653C \\ ; do \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\ gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\ done \\ &amp;&amp; curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/node-v$NODE_VERSION.tar.xz\" \\ &amp;&amp; curl -fsSLO --compressed \"https://nodejs.org/dist/v$NODE_VERSION/SHASUMS256.txt.asc\" \\ &amp;&amp; gpg --batch --decrypt --output SHASUMS256.txt SHASUMS256.txt.asc \\ &amp;&amp; grep \" node-v$NODE_VERSION.tar.xz\\$\" SHASUMS256.txt | sha256sum -c - \\ &amp;&amp; tar -xf \"node-v$NODE_VERSION.tar.xz\" \\ &amp;&amp; cd \"node-v$NODE_VERSION\" \\ &amp;&amp; ./configure \\ &amp;&amp; make -j$(getconf _NPROCESSORS_ONLN) V= \\ &amp;&amp; make install \\ &amp;&amp; apk del .build-deps-full \\ &amp;&amp; cd .. \\ &amp;&amp; rm -Rf \"node-v$NODE_VERSION\" \\ &amp;&amp; rm \"node-v$NODE_VERSION.tar.xz\" SHASUMS256.txt.asc SHASUMS256.txt; \\ fi \\ &amp;&amp; rm -f \"node-v$NODE_VERSION-linux-$ARCH-musl.tar.xz\" \\ &amp;&amp; apk del .build-deps \\ # smoke tests &amp;&amp; node --version \\ &amp;&amp; npm --versionENV YARN_VERSION 1.22.5RUN apk add --no-cache --virtual .build-deps-yarn curl gnupg tar \\ &amp;&amp; for key in \\ 6A010C5166006599AA17F08146C2130DFD2497F5 \\ ; do \\ gpg --batch --keyserver hkps://keys.openpgp.org --recv-keys \"$key\" || \\ gpg --batch --keyserver keyserver.ubuntu.com --recv-keys \"$key\" ; \\ done \\ &amp;&amp; curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz\" \\ &amp;&amp; curl -fsSLO --compressed \"https://yarnpkg.com/downloads/$YARN_VERSION/yarn-v$YARN_VERSION.tar.gz.asc\" \\ &amp;&amp; gpg --batch --verify yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz \\ &amp;&amp; mkdir -p /opt \\ &amp;&amp; tar -xzf yarn-v$YARN_VERSION.tar.gz -C /opt/ \\ &amp;&amp; ln -s /opt/yarn-v$YARN_VERSION/bin/yarn /usr/local/bin/yarn \\ &amp;&amp; ln -s /opt/yarn-v$YARN_VERSION/bin/yarnpkg /usr/local/bin/yarnpkg \\ &amp;&amp; rm yarn-v$YARN_VERSION.tar.gz.asc yarn-v$YARN_VERSION.tar.gz \\ &amp;&amp; apk del .build-deps-yarn \\ # smoke test &amp;&amp; yarn --versionCOPY docker-entrypoint.sh /usr/local/bin/ENTRYPOINT [\"docker-entrypoint.sh\"]CMD [ \"node\" ] Docker build 原理使用docker build 构建镜像的流程大概如下: 执行 docker build -t &lt;imageName:Tag&gt; .可以使用 -f参数来指定 Dockerfile 文件； docker 客户端会将构建命令后面指定的路径(.)下的所有文件(打包成一个 tar 包)，发送给 Docker 服务端; docker 服务端收到客户端发送的 tar 包，然后解压，接下来根据 Dockerfile 里面的指令进行镜像的分层构建； docker 下载 FROM 语句中指定的基础镜像，然后将基础镜像的 layer 联合挂载为一层，并在上面创建一个空目录； 接着启动一个临时的容器并在 chroot 中启动一个 bash，运行 RUN 语句中的命令 (使用docker ps 查看临时的镜像): 一条 RUN 命令结束后，会把上层目录压缩，形成新镜像中的新的一层； 如果 Dockerfile 中包含其它命令，就以之前构建的层次为基础，从第二步开始重复创建新层，直到完成所有语句后退出； 构建完成之后为该镜像打上 tag； Base Image当我们在写 Dockerfile 的时候都需要用 FROM 语句来指定一个基础镜像，这些基础镜像并不是无中生有，也需要一个 Dockerfile 来构建成镜像。例如上面的Nodejs Dockerfile指定了apline:3.13作为基础镜像。下面我们拿 debian:buster 这个基础镜像的 Dockerfile 来看一下基础镜像是如何炼成的(官方镜像例如ubuntu, centos是如何炼成的可以参考 docker library official images)。 123FROM scratchADD rootfs.tar.xz /CMD [\"bash\"] 一个基础镜像的 Dockerfile 一般仅有三行。第一行 FROM scratch 中的scratch 这个镜像并不真实的存在。在 Dockerfile 中 FROM scratch 指令并不进行任何操作，也就是不会创建一个镜像层；接着第二行的 ADD rootfs.tar.xz / 会自动把 rootfs.tar.xz 解压到 / 目录下，由此产生的一层镜像就是最终构建的镜像真实的 layer 内容；第三行 CMD [&quot;bash&quot;] 指定这镜像在启动容器的时候执行的应用程序，一般基础镜像的 CMD 默认为 bash 或者 sh 。 ADD rootfs.tar.xz / 中，这个 rootfs.tar.xz 就是我们经过一系列骚操作（一般是发行版源码编译）搓出来的根文件系统，这个操作比较复杂(我太菜了，说不清楚哈~)，感兴趣可以去看一下构建 debian 基础镜像的 Jenkins 流水线任务 debuerreotype，上面有构建这个 rootfs.tar.xz 完整过程，或者参考 Debian 官方的 docker-debian-artifacts 这个 repo 里的 shell 脚本。 需要额外注意一点，在这里往镜像里添加 rootfs.tar.xz 时使用的是 ADD 而不是 COPY ，因为在 Dockerfile 中的 ADD 指令 src 文件如果是个 tar 包，在构建的时候 docker 会帮我们把 tar 包解开到指定目录，使用 copy 指令则不会解开 tar 包。另外一点区别就是 ADD 指令是添加一个文件，这个文件可以是构建上下文环境中的文件，也可以是个 URL，而 COPY 则只能添加构建上下文中的文件。 当我们把这个 rootfs.tar.xz 解开后里面就是一个 Linux 的根文件系统，不同于我们使用 ISO 安装系统的那个根文件系统，这个根文件系统是经过一系列的裁剪，去掉了一些在容器运行中不必要的文件，使之更加轻量适用于容器运行的场景。 123@todo使用dockerfile 创建一个镜像~ 镜像是怎样存放的本地存储当我们构建完一个镜像之后，镜像就存储在了我们 docker 本地存储目录，默认情况下为 /var/lib/docker ，下面就探寻一下镜像是以什么样的目录结构存放的。 12345678910111213[root@centos3 docker]# tree -L 1.├── buildkit├── containers├── image├── network├── overlay2├── plugins├── runtimes├── swarm├── tmp├── trust└── volumes 容器的元数据存放在 image 目录下，容器的 layer 数据则存放在 overlay2 目录下。 Image 目录image目录下的overlay2 代表着本地 docker 存储使用的是 overlay2 存储驱动，目前最新版本的 docker 默认优先采用 overlay2 作为存储驱动，对于已支持该驱动的 Linux 发行版，不需要任何进行任何额外的配置，可使用 lsmod 命令查看当前系统内核是否支持 overlay2 。overlay2目录下的层级结构如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[root@centos3 overlay2]# tree -L 4 .├── distribution│ ├── diffid-by-digest│ │ └── sha256│ │ ├── 12008541203a024dab3a30542c87421e5a27dbb601b9c6f029b3827af138ccb3│ │ ├── 1f41b2f2bf94740d411c54b48be7f5e9dfbe14f29d1a5cf64f39150d75f39740│ │ ├── 26abe223b186967eef77715b2c6efd147f3a203a0c7b19e61a8947dab4236397│ │ ├── 28549a15ba3eb287d204a7c67fdb84e9d7992c7af1ca3809b6d8c9e37ebc9877│ │ ├── 33847f680f63fb1b343a9fc782e267b5abdbdb50d65d4b9bd2a136291d67cf75│ │ ├── 363ab70c2143bc121f037ba432cede225b7053200abba2a7a4ca24c2915a3998│ │ ├── 42ec49ed44205e453626b369a55e9d55e714214b6382663499a030a9293079af│ │ ├── 4afb39f216bd4e336f9b78584bae0f6bcb77150107471d8d67d3b8abfbdea46a│ │ ├── 540db60ca9383eac9e418f78490994d0af424aab7bf6d0e47ac8ed4e2e9bcbba│ └── v2metadata-by-diffid│ └── sha256│ ├── 04d694391e96c105b671692a9135fa5e61a5498a5c55b07df0dd4ae8186fe776│ ├── 0ba2ba6b57851cc6f3c2508b39e4d3d86321598a87f496146d4e5f33aee91c2c│ ├── 1a86ff1d449f9522c0af1515d11082885b2f198b5ae99673920382d45c929e28│ ├── 2788d2f5dd8f8427ca7fa6124ca8fdf034ba358e0db75d533f448e0292966a5b│ ├── 2f4625690283453c620cbf5e13855148d73941df5c2f774eb03d54328c36edc8│ ├── 322e305537da267d4135a26993d7696e53201988a3059d33e2fabb83a4c81cf8│ ├── 3764c3e89288ef5786f440a66a5981593ddceb8b273ec9465f45c1758d64ced3│ ├── 3b6deb83ca3702881214614853644c4109ae6a217e3c0b0042d0a0e60725c69b├── imagedb│ ├── content│ │ └── sha256│ │ ├── 1fd8e1b0bb7ef6cc63e0094e2fb5a35259bdce7041ef5e7e4c99694ded1041d7│ │ └── 69593048aa3acfee0f75f20b77acb549de2472063053f6730c4091b53f2dfb02│ └── metadata│ └── sha256├── layerdb│ ├── mounts│ │ └── 72d706b5c93d78590875cce887c94351060eec659e281ae39113e0a887b003e2│ │ ├── init-id│ │ ├── mount-id│ │ └── parent│ ├── sha256│ │ ├── 518ebede9dd4e553bcee3157e2f935affbf857abaa6ac3856d7fbfa8e0402d1d│ │ │ ├── cache-id│ │ │ ├── diff│ │ │ ├── parent│ │ │ ├── size│ │ │ └── tar-split.json.gz│ │ ├── 5b8c72934dfc08c7d2bd707e93197550f06c0751023dabb3a045b723c5e7b373│ │ │ ├── cache-id│ │ │ ├── diff│ │ │ ├── size│ │ │ └── tar-split.json.gz│ │ ├── 8f346dcd27d6735db3fd6792708d9fc93424e8c68be6e89f432766d932beea0a│ │ │ ├── cache-id│ │ │ ├── diff│ │ │ ├── parent│ │ │ ├── size│ │ │ └── tar-split.json.gz│ │ ├── 9a5d14f9f5503e55088666beef7e85a8d9625d4fa7418e2fe269e9c54bcb853c│ │ │ ├── cache-id│ │ │ ├── diff│ │ │ ├── size│ │ │ └── tar-split.json.gz│ │ ├── d3abebe503aa4a993321a26a0ac46b20c45540640bbd55cb1e007a8ec6ce046e│ │ │ ├── cache-id│ │ │ ├── diff│ │ │ ├── parent│ │ │ ├── size│ │ │ └── tar-split.json.gz│ │ └── ebc8eb4880594e292509a2a504a4ffb957a0935c7682d20849800e3024e8507c│ │ ├── cache-id│ │ ├── diff│ │ ├── parent│ │ ├── size│ │ └── tar-split.json.gz│ └── tmp└── repositories.json repositories.json 文件repositories.json 就是存储镜像元数据信息，主要是 image name 和 image id 的对应，digest 和 image id 的对应。当 pull 完一个镜像的时候 docker 会更新这个文件。当我们 docker run 一个容器的时候也用到这个文件去索引本地是否存在该镜像，没有镜像的话就自动去 pull 这个镜像。 distribution文件夹存放着 layer 的 diff_id 和 digest 的对应关系: diffid-by-digest :存放 digest 到 diffid 的对应关系 v2metadata-by-diffid : 存放 diffid 到 digest 的对应关系 layerdblayerdb 存放在layer的原始数据。 需要注意的是：tar-split.json.gz 文件是 layer tar 包的 split 文件，记录了 layer 解压后的文件在 tar 包中的位置（偏移量），通过这个文件可以还原 layer 的 tar 包，在 docker save 导出 image 的时候会用到，由根据它可以开倒车把解压的 layer 还原回 tar 包。详情可参考 tar-split 123456789layerdb&#x2F;├── mounts├── sha256│ ├── 518ebede9dd4e553bcee3157e2f935affbf857abaa6ac3856d7fbfa8e0402d1d│ │ ├── cache-id # docker 下载镜像时随机生成的 id│ │ ├── diff # 存放 layer 的 diffid│ │ ├── parent # 放当前 layer 的父 layer 的 diffid，最底层的 layer 没有这个文件│ │ ├── size # 该 layer 的大小│ │ └── tar-split.json.gz Overlay2 目录overley2目录下存放的是每个layer解压后的内容，使用tree命令查看可以知道，镜像 layer 的内容都存放在一个 diff 的文件夹下，diff 的上级目录就是以镜像 layer 的 digest 为名的目录。其中还有个 l 文件夹，下面有一坨坨的硬链接文件指向上级目录的 layer 目录。这个 l 其实就是 link 的缩写，l 下的文件都是一些比 digest 文件夹名短一些的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263overlay2├── 259cf6934509a674b1158f0a6c90c60c133fd11189f98945c7c3a524784509ff│ └── diff│ ├── bin│ ├── dev│ ├── etc│ ├── home│ ├── lib│ ├── media│ ├── mnt│ ├── opt│ ├── proc│ ├── root│ ├── run│ ├── sbin│ ├── srv│ ├── sys│ ├── tmp│ ├── usr│ └── var├── 27f9e9b74a88a269121b4e77330a665d6cca4719cb9a58bfc96a2b88a07af805│ ├── diff│ └── work├── a0df3cc902cfbdee180e8bfa399d946f9022529d12dba3bc0b13fb7534120015│ ├── diff│ │ └── bin│ └── work├── b2fbebb39522cb6f1f5ecbc22b7bec5e9bc6ecc25ac942d9e26f8f94a028baec│ ├── diff│ │ ├── etc│ │ ├── lib│ │ ├── usr│ │ └── var│ └── work├── be8c12f63bebacb3d7d78a09990dce2a5837d86643f674a8fd80e187d8877db9│ ├── diff│ │ └── etc│ └── work├── e8f6e78aa1afeb96039c56f652bb6cd4bbd3daad172324c2172bad9b6c0a968d│ └── diff│ ├── bin│ ├── dev│ ├── etc│ ├── home│ ├── lib│ ├── media│ ├── mnt│ ├── proc│ ├── root│ ├── run│ ├── sbin│ ├── srv│ ├── sys│ ├── tmp│ ├── usr│ └── var└── l ├── 526XCHXRJMZXRIHN4YWJH2QLPY -&gt; ../b2fbebb39522cb6f1f5ecbc22b7bec5e9bc6ecc25ac942d9e26f8f94a028baec/diff ├── 5RZOXYR35NSGAWTI36CVUIRW7U -&gt; ../be8c12f63bebacb3d7d78a09990dce2a5837d86643f674a8fd80e187d8877db9/diff ├── LBWRL4ZXGBWOTN5JDCDZVNOY7H -&gt; ../a0df3cc902cfbdee180e8bfa399d946f9022529d12dba3bc0b13fb7534120015/diff ├── MYRYBGZRI4I76MJWQHN7VLZXLW -&gt; ../27f9e9b74a88a269121b4e77330a665d6cca4719cb9a58bfc96a2b88a07af805/diff ├── PCIS4FYUJP4X2D4RWB7ETFL6K2 -&gt; ../259cf6934509a674b1158f0a6c90c60c133fd11189f98945c7c3a524784509ff/diff └── XK5IA4BWQ2CIS667J3SXPXGQK5 -&gt; ../e8f6e78aa1afeb96039c56f652bb6cd4bbd3daad172324c2172bad9b6c0a968d/diff Registry 存储当我们执行docker push命令之后，就把镜像存储到远程Registry仓库中了，镜像在远程仓库中是如何存储的呢，这就回到了 OCI 规范中的镜像仓库规范 distribution-spec，该规范就定义着容器镜像如何存储在远端（即 registry）上。我们可以把 registry 看作镜像的仓库，使用该规范可以帮助我们把这些镜像按照约定俗成的格式来存放。 想要分析一下镜像是如何存放在 registry 上的，我们在本地使用 docker run 来起 registry 的容器即可： 1docker run -d --name registry -p 5000:5000 -v &#x2F;var&#x2F;lib&#x2F;registry:&#x2F;var&#x2F;lib&#x2F;registry registry 当我们在本地启动一个 registry 容器之后，容器内默认的存储位置为 /var/lib/registry ，所以我们在启动的时候加了参数 -v /var/lib/registry:/var/lib/registry 将本机的路径挂载到容器内。 接着我们往registry推送busybox镜像 123docker pull busybox:latestdocker tag busybox:latest localhost:5000&#x2F;busybox:latestdocker push localhost:5000&#x2F;busybox:latest 然后我们进入/var/lib/registry目录，使用 tree 命令查看一下这个目录的存储结构。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697[root@centos3 v2]# pwd&#x2F;var&#x2F;lib&#x2F;registry&#x2F;docker&#x2F;registry&#x2F;v2[root@centos3 v2]# tree.├── blobs│ └── sha256│ ├── 08│ │ └── 08bdaf2f88f90320cd3e92a469969efb1f066c6d318631f94e7864828abd7c75│ │ └── data│ ├── 26│ │ └── 26abe223b186967eef77715b2c6efd147f3a203a0c7b19e61a8947dab4236397│ │ └── data│ ├── 54│ │ └── 540db60ca9383eac9e418f78490994d0af424aab7bf6d0e47ac8ed4e2e9bcbba│ │ └── data│ ├── 59│ │ └── 59707d521c45c42bbb7dbbef463d5d3800859f20122fb54894f0c79d9b435483│ │ └── data│ ├── 5a│ │ └── 5a38b3726f4b24fa93b80450be63ad67fd3239c2f3b83695118d7b1a88447d84│ │ └── data│ ├── 63│ │ └── 63cb2df6dfe1fb041b952ddb9f75c69569331fa39577bc41a3e56cf8f79f7e2e│ │ └── data│ ├── 69│ │ └── 69593048aa3acfee0f75f20b77acb549de2472063053f6730c4091b53f2dfb02│ │ └── data│ ├── 9d│ │ └── 9d806bc203610fb061281b581a0b2522c9fcbd15e55c55ac0c496c9b1dbe63b0│ │ └── data│ ├── b7│ │ └── b71f96345d44b237decc0c2d6c2f9ad0d17fde83dad7579608f1f0764d9686f2│ │ └── data│ ├── c5│ │ └── c521bbbd9945cde2415eae95c3a2a604ea5ca18eb86ab5cadeb589d3b8a13185│ │ └── data│ ├── dc│ │ └── dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b│ │ └── data│ └── e5│ └── e5fa5deb334027202841b051d10e7c7137fa3b63e97734309cedf6b48804df5f│ └── data└── repositories ├── busybox │ ├── _layers │ │ └── sha256 │ │ ├── 69593048aa3acfee0f75f20b77acb549de2472063053f6730c4091b53f2dfb02 │ │ │ └── link │ │ └── b71f96345d44b237decc0c2d6c2f9ad0d17fde83dad7579608f1f0764d9686f2 │ │ └── link │ ├── _manifests │ │ ├── revisions │ │ │ └── sha256 │ │ │ └── dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b │ │ │ └── link │ │ └── tags │ │ └── latest │ │ ├── current │ │ │ └── link │ │ └── index │ │ └── sha256 │ │ └── dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b │ │ └── link │ └── _uploads └── docker ├── _layers │ └── sha256 │ ├── 08bdaf2f88f90320cd3e92a469969efb1f066c6d318631f94e7864828abd7c75 │ │ └── link │ ├── 26abe223b186967eef77715b2c6efd147f3a203a0c7b19e61a8947dab4236397 │ │ └── link │ ├── 540db60ca9383eac9e418f78490994d0af424aab7bf6d0e47ac8ed4e2e9bcbba │ │ └── link │ ├── 59707d521c45c42bbb7dbbef463d5d3800859f20122fb54894f0c79d9b435483 │ │ └── link │ ├── 5a38b3726f4b24fa93b80450be63ad67fd3239c2f3b83695118d7b1a88447d84 │ │ └── link │ ├── 9d806bc203610fb061281b581a0b2522c9fcbd15e55c55ac0c496c9b1dbe63b0 │ │ └── link │ ├── c521bbbd9945cde2415eae95c3a2a604ea5ca18eb86ab5cadeb589d3b8a13185 │ │ └── link │ └── e5fa5deb334027202841b051d10e7c7137fa3b63e97734309cedf6b48804df5f │ └── link ├── _manifests │ ├── revisions │ │ └── sha256 │ │ └── 63cb2df6dfe1fb041b952ddb9f75c69569331fa39577bc41a3e56cf8f79f7e2e │ │ └── link │ └── tags │ └── latest │ ├── current │ │ └── link │ └── index │ └── sha256 │ └── 63cb2df6dfe1fb041b952ddb9f75c69569331fa39577bc41a3e56cf8f79f7e2e │ └── link └── _uploads (有点看不懂，且看下面的层级结构图): blobs 目录blobs 目录用来存放镜像的三种文件： layer 的真实数据，镜像的 manifest 文件，镜像的 image config 文件。这些文件都是以 data 为名的文件存放在于该文件 sha256 相对应的目录下.对于 layer 来讲，这是个 data 文件的格式是 vnd.docker.image.rootfs.diff.tar.gzip ，我们可以使用 tar -xvf 命令将这个 layer 解开。当我们使用 docker pull 命令拉取镜像的时候，也是去下载这个 data文件，下载完成之后会有一个 docker-untar的进程将这个 data文件解开存放在/var/lib/docker/overlay2/${digest}/diff 目录下。 repositories 目录repositories 目录存放着各个镜像的manifest和layer信息，每个镜像版本都有_layers, _manifests, _uploads这三个文件夹。 1234[root@centos3 busybox]# pwd&#x2F;var&#x2F;lib&#x2F;registry&#x2F;docker&#x2F;registry&#x2F;v2&#x2F;repositories&#x2F;busybox[root@centos3 busybox]# ls_layers _manifests _uploads _upload文件夹 _uploads 文件夹是个临时的文件夹，主要用来存放 push 镜像过程中的文件数据，当镜像 layer 上传完成之后会清空该文件夹。其中的 data 文件上传完毕后会移动到 blobs 目录下。 _manifest 文件夹_manifests 文件夹是镜像上传完成之后由 registry 来生成的，并且该目录下的文件都是一个名为 link的文本文件，它的值指向 blobs 目录下与之对应的目录。在_manifests文件夹下包含着镜像的tags和revisions` 信息，每一个镜像的每一个 tag 对应着于 tag 名相同的目录， 每个 tag目录下面有 current 目录和 index 目录， current 目录下的 link 文件保存了该 tag 目前的 manifest 文件的 sha256 编码，对应在 blobs 中的 sha256 目录下的 data 文件，而 index 目录则列出了该 tag 历史上传的所有版本的 sha256 编码信息。_revisions 目录里存放了该 repository 历史上上传版本的所有 sha256 编码信息。 当我们 pull 镜像的时候如果不指定镜像的 tag名，默认就是 latest，registry 会从 HTTP 请求中解析到这个 tag 名，然后根据 tag 名目录下的 current link 文件找到该镜像的 manifest 的位置返回给客户端，客户端接着去请求这个 manifest 文件，最后客户端根据这个 manifest 文件来 pull 相应的镜像 layer 。 1234567[root@centos3 busybox]# find .&#x2F;_manifests&#x2F; -type f.&#x2F;_manifests&#x2F;revisions&#x2F;sha256&#x2F;dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b&#x2F;link.&#x2F;_manifests&#x2F;tags&#x2F;latest&#x2F;index&#x2F;sha256&#x2F;dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b&#x2F;link.&#x2F;_manifests&#x2F;tags&#x2F;latest&#x2F;current&#x2F;link[root@centos3 busybox]# cat .&#x2F;_manifests&#x2F;tags&#x2F;latest&#x2F;index&#x2F;sha256&#x2F;dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b&#x2F;linksha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b registry 存储目录里并不会存储与该 registry 相关的信息，比如我们 push 镜像的时候需要给镜像加上 localhost:5000 这个前缀，这个前缀并不会存储在 registry 存储中。假如要迁移一个很大的 registry 镜像仓库，镜像的数量在 5k 以上。最便捷的办法就是打包这个 registry 存储目录，将这个 tar 包 rsync 到另一台机器即可。需要强调一点，打包 registry 存储目录的时候不需要进行压缩，直接 tar -cvf 即可。因为 registry 存储的镜像 layer 已经是个 tar.gzip 格式的文件，再进行压缩的话效果甚微而且还浪费 CPU 时间得不偿失。 镜像是怎么搬运的当我们知道镜像是如何存储了之后，我们就该了解镜像是如何传输的了。 docker pull理解 docker pull 一个镜像的流程最好的办法是查看 OCI registry 规范中的这段文档 pulling-an-image, 下面流程图是镜像传输的流程: ocker pull 就和我们使用 git clone 一样效果，将远程的镜像仓库拉取到本地来给容器运行时使用，结合上图大致的流程如下： 第一步应该是使用~/.docker/config.json 中的 auth 认证信息在 registry 那里进行鉴权授权，拿到一个 token，后面的所有的 HTTP 请求中都要包含着该 token 才能有权限进行操作 dockerd 守护进程解析 docker 客户端参数，接着向 registry 请求 manifest 文件 dockerd 得到 manifest 后，读取里面 image config 文件的 digest，这个 sha256 值就是 image 的 ID 根据 ID 在本地的 repositories.json中查找找有没有存在同样 ID 的 image，有的话就不用下载了 如果没有，那么会给 registry 服务器发请求拿到 image config 文件 根据 image config 文件中的 diff_ids在本地找对应的 layer 是否存在 如果 layer 不存在，则根据 manifest 里面 layer 的 sha256 和 media type 去服务器拿相应的 layer（相当去拿压缩格式的包） dockerd 守护进程并行下载各 layer 拿到后进行解压，并检查解压(gzip -d)后 tar 包的 sha256 是否和 image config 中的 diff_id 相同，不相同就翻车了 等所有的 layer 都下载完成后，整个 image 的 layer 就下载完成，接着开始进行解压(tar -xf) layer 的 tar 包。 dockerd 起一个单独的进程 docker-untar 来 gzip 解压缩已经下载完成的 layer 文件 验证 image config 中的 RootFS.diffids 是否与解压后hash 相同 docker pushdoker push 的流程恰好和 docker pull 拉取镜像到本地的流程相反。docker pull 一个镜像的时候往往需要先获取镜像 的 manifest 文件，然后根据这个文件中的 layer 信息取相应的 layer。doker push 一个镜像，需要先将镜像的各个 layer 推送到 registry ，当所有的镜像 layer 上传完毕之后最后再 push Image manifest 到 registry。大体的流程如下: 第一步和 pull 一个镜像一样也是进行鉴权授权，拿到一个 token 向 registry 发送 POST /v2/&lt;name&gt;/blobs/uploads/请求，registry 返回一个上传镜像 layer 时要应到的 URL 客户端通过 HEAD /v2/&lt;name&gt;/blobs/&lt;digest&gt; 请求检查 registry 中是否已经存在镜像的 layer 客户端通过URL 使用 POST 方法来实时上传 layer 数据，上传镜像 layer 分为 Monolithic Upload （整体上传）和Chunked Upload（分块上传）两种方式 镜像的 layer 上传完成之后，客户端向 registry 发送一个 PUT HTTP 请求告知该 layer 已经上传完毕。 当所有的 layer 上传完之后，再将 manifest 推送上去 此处应该安利下skopeo, skopeo对跨registry传输的镜像场景下非常实用， 免去了像 docker pull –&gt; docker tag –&gt; docker push 这样的流程，感兴趣的同学可以去了解下~ 镜像是怎样食用的当我们拿到一个镜像之后, 容器运行时通过一个叫 OCI runtime filesytem bundle 的标准格式将 OCI 镜像通过工具转换为 bundle ，然后 OCI 容器引擎能够识别这个 bundle 来运行容器。 filesystem bundle 是个目录，用于给 runtime 提供启动容器必备的配置文件和文件系统。标准的容器 bundle 包含以下内容： config.json: 该文件包含了容器运行的配置信息，该文件必须存在 bundle 的根目录，且名字必须为 config.json 容器的根目录，可以由 config.json 中的 root.path 指定 当我们启动一个容器之后使用 tree 命令来分析一下 overlay2目录 就会发现，较之前的目录，容器启动之后 overlay2 目录下多了一个 merged 的文件夹，该文件夹就是容器内看到的。docker 通过 overlayfs (联合文件系统)联合挂载的技术将镜像的多层 layer 挂载为一层，这层的内容就是容器里所看到的，也就是 merged 文件夹。 什么是联合文件系统联合挂载是一种文件系统，它可以在不修改其原始（物理）源的情况下创建多个目录，并把内容合并为一个文件的错觉。联合挂载或联合文件系统是文件系统；但不是文件系统类型，而是一个包含许多实现的概念。docker 支持多种文件系统，现在默认使用overlay2作为默认的存储驱动程序(version &gt; 18.06)，如果是老版本则默认使用aufs作为存储驱动程序(version &lt;=18.06), 下面就介绍几种常见的联合文件系统： UnionFSUnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统『联合』到同一个挂载点的文件系统服务。 现在UnionFS 似乎不再积极开发，其最新提交是从 2014 年 8 月开始的。您可以在其网站https://unionfs.filesystems.org/上阅读更多有关它的信息。 AUFSAUFS 即 Advanced UnionFS 其实就是 UnionFS 的升级版, 它能够将将单个 Linux 主机上的多个目录分层并将它们呈现为单个目录。这些目录在 AUFS 术语中称为分支，在 Docker 术语中称为层，关于AUFS如何工作的在 docker 官方文档 里有说明 OverlayFSOverlayFS也是一个现代联合文件系统，类似于 AUFS，但速度更快，实现更简单。Docker 为 OverlayFS 提供了两个存储驱动程序：原始的overlay和更新更稳定的overlay2。 OverlayFS 由低层和高层的目录组成，下层目录称之为lowerdir, 上层称之为upperdir, 文件系统中低层的目录是只读的，而高层的文件系统则是可读可写的。 下图显示了 Docker 镜像和 Docker 容器是如何分层的。图像层是lowerdir，容器层是upperdir。merged 为镜像和容器中所有图层的合并视图。关于AUFS如何工作的在 docker 官方文档 里有说明 当我们使用docker inspect命令的时候可以看到镜像(容器)的的层信息 1234567[root@centos3 nginx]# docker inspect nginx | jq .[0].GraphDriver.Data&#123; &quot;LowerDir&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;26b544d41358c2c6818f5ec30855b997a85b7d7d5291bf3115def847626971a3&#x2F;diff:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;452f96de6d70b17221a06b9b81af3cb5b5b855231c018826f030dbc9393e1867&#x2F;diff:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;6a10e6330e511ef9f6ea94301366d3b303da6b751a9703fc223a73869aa98912&#x2F;diff:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;7c28defbbc32d0bd94dc675639a8e26ffd9e16903d1c69cce6a109d495bb230c&#x2F;diff:&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;32680ffbf919dbc1d2a4a626ce4bba1203dc0c3ec649dfb44f9c15657514e0e8&#x2F;diff&quot;, &quot;MergedDir&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;be030cc6ed9cfa5c9e7ba78d6d62a16842e09940f615617c0ed284bc80c8f3ce&#x2F;merged&quot;, &quot;UpperDir&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;be030cc6ed9cfa5c9e7ba78d6d62a16842e09940f615617c0ed284bc80c8f3ce&#x2F;diff&quot;, &quot;WorkDir&quot;: &quot;&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;be030cc6ed9cfa5c9e7ba78d6d62a16842e09940f615617c0ed284bc80c8f3ce&#x2F;work&quot;&#125; LowerDir: 是只读镜像层的目录，以冒号分隔 MergedDir：镜像和容器中所有图层的合并视图 UpperDir：写入更改的读写层 WorkDir：Linux OverlayFS 用于准备合并视图的工作目录 ZFS ZFS 是由 Sun Microsystems（现在是 Oracle）创建的联合文件系统。它有一些有趣的功能，如分层校验和、快照和备份/复制的本机处理或本机数据压缩和重复数据删除。但是，由 Oracle 维护，它具有非 OSS 友好许可 (CDDL)，因此不能作为 Linux 内核的一部分提供。但是，您可以在 Linux (ZoL)项目上使用 ZFS，Docker 文档中将其描述为健康和成熟的……，但尚未准备好用于生产。关于ZFS如何工作的在 docker 官方文档 里有说明 Btrfs Btrfs是多家公司（包括 SUSE、WD 或 Facebook ）的联合项目，在 GPL 许可下发布，是 Linux 内核的一部分。Btrfs 是 Fedora 33 的默认文件系统。它还具有一些有用的功能，例如块级操作、碎片整理、可写快照等等。Btrfs 作为下一代写时复制文件系统，非常适合 Docker。关于Btrfs如何工作的在 docker 官方文档 里有说明 动手实验部分docker overlay2创建联合挂载123456789101112131415161718192021222324#!/bin/bash# 删除所有镜像# docker image prune -af# 拉取nginx镜像(使用degist模式以保证layer都一致)docker pull nginx@sha256:8f335768880da6baf72b70c701002b45f4932acae8d574dedfddaf967fc3ac90# 查看GraphDriverdocker inspect nginx@sha256:8f335768880da6baf72b70c701002b45f4932acae8d574dedfddaf967fc3ac90 | jq .[0].GraphDriver.Data# 创建nginx文件夹mkdir -p /root/nginx# 联合挂载, lowerdir, uppperdir, workdir 使用 GraphDriver.Data中的数据mount -t overlay -o \\lowerdir=/var/lib/docker/overlay2/26b544d41358c2c6818f5ec30855b997a85b7d7d5291bf3115def847626971a3/diff:/var/lib/docker/overlay2/452f96de6d70b17221a06b9b81af3cb5b5b855231c018826f030dbc9393e1867/diff:/var/lib/docker/overlay2/6a10e6330e511ef9f6ea94301366d3b303da6b751a9703fc223a73869aa98912/diff:/var/lib/docker/overlay2/7c28defbbc32d0bd94dc675639a8e26ffd9e16903d1c69cce6a109d495bb230c/diff:/var/lib/docker/overlay2/32680ffbf919dbc1d2a4a626ce4bba1203dc0c3ec649dfb44f9c15657514e0e8/diff,\\upperdir=/var/lib/docker/overlay2/be030cc6ed9cfa5c9e7ba78d6d62a16842e09940f615617c0ed284bc80c8f3ce/diff,\\workdir=/var/lib/docker/overlay2/be030cc6ed9cfa5c9e7ba78d6d62a16842e09940f615617c0ed284bc80c8f3ce/work \\overlay /root/nginx# 取消挂载并删除目录# umount overlay# rm -rf /root/nginx 手动创建overlay2联合挂载1234567891011121314151617181920212223242526272829303132333435#!/bin/bash# 创建文件夹cd /root &amp;&amp; rm -rf overlay2 &amp;&amp; mkdir overlay2 &amp;&amp; cd $_# 创建layer1mkdir -p layer1/binecho \"layer1\" &gt; layer1/bin/bashecho \"layer1\" &gt; layer1/layer1# 创建layer2mkdir -p layer2/binecho \"layer2\" &gt; layer2/bin/bashecho \"layer2\" &gt; layer2/layer2# 创建layer3mkdir -p layer3/etcecho \"layer3\" &gt; layer3/etc/catecho \"layer3\" &gt; layer3/layer3mkdir -p layer4 &amp;&amp; \\mkdir -p workdir &amp;&amp; \\mkdir -p mergedmount -t overlay -o \\lowerdir=/root/overlay2/layer1:/root/overlay2/layer2:/root/overlay2/layer3,\\upperdir=/root/overlay2/layer4,\\workdir=/root/overlay2/workdir \\overlay1 /root/overlay2/merged 1234567891011121314151617181920212223[root@centos3 overlay2]# lslayer1 layer2 layer3 layer4 merged workdir# 切换到merged层并且创建layer文件[root@centos3 overlay2]# cd merged/[root@centos3 merged]# echo \"hello world\" &gt; layer[root@centos3 merged]# cat layerhello world# layer会复制一份到layer4[root@centos3 merged]# cd ../layer4[root@centos3 layer4]# lslayer[root@centos3 layer4]# cat layer hello world# 删除layer文件，layer4下的文件也会删除[root@centos3 layer4]# cd - [root@centos3 merged]# rm layerrm: remove regular file ‘layer’? y[root@centos3 overlay2]# cd layer4[root@centos3 layer4]# ls[root@centos3 layer4]# 使用runc 运行联合挂载的目录1234567891011121314151617181920212223242526#!/bin/bash# 拉取busybox镜像(使用degist模式以保证layer都一致)docker pull busybox@sha256:0f354ec1728d9ff32edcd7d1b8bbdfc798277ad36120dc3dc683be44524c8b60# 查看GraphDriverdocker inspect nginx@sha256:8f335768880da6baf72b70c701002b45f4932acae8d574dedfddaf967fc3ac90 | jq .[0].GraphDriver.Data# 创建busybox文件夹, 一定要在rootfs文件夹下，因为runc的config.json默认配置是rootfsmkdir -p /root/busybox/rootfs &amp;&amp; mkdir -p /root/busybox/workdir &amp;&amp; /root/busybox/upperdir# 联合挂载 lowerdir 使用 GraphDriver.Data中upperdir中的数据mount -t overlay -o \\lowerdir=/var/lib/docker/overlay2/f3640f23bac0bbbac072f3609bba70cd50a3160a8820828d9047173686fe170d/diff,\\upperdir=/root/busybox/upperdir,\\workdir=/root/busybox/workdir \\busybox /root/busybox/rootfs# 切换到busybox目录下cd /root/busybox# 创建config.jsonrunc spec# 通过runc运行busyboxrunc run mybusybox 通过runc 运行容器1234567891011# 创建目录mkdir -p /root/mybusybox/rootfs &amp;&amp; cd /root/mybusybox# 生成rootfsdocker export $(docker create busybox) | tar -C rootfs -xvf -# 生成config.json文件runc spec# 运行容器runc run mybusybox1 参考文档深入浅出容器镜像的一生🤔 // 木子李的博客，上面的内容基本拷贝的木子李的~ 关于存储驱动程序 //docker 官方文档,推荐阅读 虚拟化技术与Hypervisor回顾 //理解一下虚拟化也不错 镜像是怎样炼成的 // 伪架构师的博客 深入研究Docker联合文件系统 //linux云计算网络 OCI 和 runc：容器标准化和 docker //关于OCI的解读，以及如何通过runc 起一个容器，值得一看，推荐阅读 开放容器计划 (OCI) 规范 // 阿里国际站上的博客，关于OCI的解读，图不错(英文的) Docker 原理篇（三）Docker 基础操作和基础概念 // 命令的状态流转图还不错，值得一看 Docker 原理与核心概念 //docker 的基本命令，命令的状态流转图也不错 https://windsock.io/explaining-docker-image-ids/ // docker 镜像ID，英文的 https://segmentfault.com/a/1190000011294361 //关于container -&gt; docker-shim的","categories":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/tags/docker/"}]},{"title":"linux strace命令","slug":"linux-strace","date":"2021-07-18T16:13:28.000Z","updated":"2021-12-17T07:42:48.351Z","comments":true,"path":"2021/07/19/linux-strace/","link":"","permalink":"https://github.com/fafucoder/2021/07/19/linux-strace/","excerpt":"","text":"简述在Linux世界，进程不能直接访问硬件设备，当进程需要访问硬件设备(比如读取磁盘文件，接收网络数据等等)时，必须由用户态模式切换至内核态模式，通过系统调用访问硬件设备。strace是一个可用于诊断、调试和教学的Linux用户空间跟踪器。我们用它来监控用户空间进程和内核的交互，比如系统调用、信号传递、进程状态变更等。 strace底层使用内核的ptrace特性来实现其功能。 在Linux系统上，进程通过glibc库封装的函数，间接使用系统调用。Linux内核目前有300多个系统调用，详细的列表可以通过syscalls手册页查看。这些系统调用主要分为几类： 123456文件和设备访问类: 比如open&#x2F;close&#x2F;read&#x2F;write&#x2F;chmod等进程管理类: fork&#x2F;clone&#x2F;execve&#x2F;exit&#x2F;getpid等信号类: signal&#x2F;sigaction&#x2F;kill 等内存管理: brk&#x2F;mmap&#x2F;mlock等进程间通信IPC: shmget&#x2F;semget * 信号量，共享内存，消息队列等网络通信: socket&#x2F;connect&#x2F;sendto&#x2F;sendmsg 等 strace有两种运行模式： 通过它启动要跟踪的进程， 只需要在原本的命令前加上strace即可, 例如： strace ls /usr/local 通过它跟踪已经运行的进程，在不中断进程执行的情况下诊断进程，只需要指定-p pid 即可。例如: strace -p 1234 参数123456789101112131415161718192021-c 统计每一系统调用的所执行的时间,次数和出错的次数等. -d 输出strace关于标准错误的调试信息. -f 跟踪由fork调用所产生的子进程. -ff 如果提供-o filename,则所有进程的跟踪结果输出到相应的filename.pid中,pid是各进程的进程号. -F 尝试跟踪vfork调用.在-f时,vfork不被跟踪. -i 输出系统调用的入口指针. -q 禁止输出关于脱离的消息. -r 打印出相对时间关于每一个系统调用. -t 在输出中的每一行前加上时间信息. -tt 在输出中的每一行前加上时间信息,微秒级. -ttt 微秒级输出,以秒了表示时间. -T 显示每一调用所耗的时间. -v 输出所有的系统调用.一些调用关于环境变量,状态,输入输出等调用由于使用频繁,默认不输出. -V 输出strace的版本信息. -x 以十六进制形式输出非标准字符串 -xx 所有字符串以十六进制形式输出. -e expr 指定一个表达式,用来控制如何跟踪 -o filename 将输出写入文件filename -p pid 跟踪指定的进程pid. -s strsize 指定输出的字符串的最大长度.默认为32-u username 以username 的UID和GID执行被跟踪的命令 strace -e expr格式如下 [qualifier=][!]value1[,value2]...qualifier只能是 trace,abbrev,verbose,raw,signal,read,write其中之一, value是用来限定的符号或数字。默认的 qualifier是 trace。 例如: -eopen等价于 -e trace=open,表示只跟踪open调用.而-etrace!=open表示跟踪除了open以外的其他调用.有两个特殊的符号 all 和 none. 如果人工输入每一个具体的系统调用名称，可能容易遗漏。于是strace提供了几类常用的系统调用组合名字。 -e trace=file 跟踪和文件访问相关的调用(参数中有文件名)-e trace=process 和进程管理相关的调用，比如fork/exec/exit_group-e trace=network 和网络通信相关的调用，比如socket/sendto/connect-e trace=signal 信号发送和处理相关，比如kill/sigaction-e trace=desc 和文件描述符相关，比如write/read/select/epoll等-e trace=ipc 进程间通信相关，比如shmget等","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubernetes-rabc","slug":"kubernetes-rabc","date":"2021-05-08T10:34:15.000Z","updated":"2021-12-17T07:42:48.338Z","comments":true,"path":"2021/05/08/kubernetes-rabc/","link":"","permalink":"https://github.com/fafucoder/2021/05/08/kubernetes-rabc/","excerpt":"","text":"Role &amp;&amp; ClusterRoleRolebinding &amp;&amp; ClusterRolebindingServiceAccounthttps://kubernetes.io/zh/docs/reference/access-authn-authz/certificate-signing-requests/ https://www.openlogic.com/blog/granting-user-access-your-kubernetes-cluster Secret 参考文档 Kubernetes RBAC 详解 //阳明的博客 kubernetes的Service Account和secret Secret //kubernetes 官方文档 使用 RBAC 鉴权 //kubernetes 官方文档","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"linux 终端控制","slug":"linux-terminal","date":"2021-03-30T03:22:59.000Z","updated":"2021-12-17T07:42:48.352Z","comments":true,"path":"2021/03/30/linux-terminal/","link":"","permalink":"https://github.com/fafucoder/2021/03/30/linux-terminal/","excerpt":"","text":"问题 linux打开一个终端运行一个程序，在程序运行未结束的时候如果关掉终端的话，那么该程序也会跟着退出。 如果终端运行的程序堵塞了，则整个终端窗口将无法再执行其他程序。 为了使进程不堵塞整个终端，可使用如下命令使进程在后台运行: 1nohup &lt;command&gt; &amp; 其中： nohup表示让我们的程序进程忽略所有挂断（SIGHUP）信号。 “&amp;”表示让我们的程序进入后台运行 基础知识进程进程是linux进行分配资源的最小单位，分为前台进程和后台进程 前台进程：例如这样： hexo server 后台进程： 例如这样： hexo sever &amp; 进程组每个进程除了有一进程ID外，还属于一个进程组，进程组就是一个或多个进程的集合。进程组中有一个进程组长，组长的进程 ID 是进程组 ID(PGID)。 当我们在终端中敲下一条命令，然后按下回车的时候，Shell会开启一个新进程来执行这条命令。如果这条命令是由管道连接起来的多个命令组成的话，Shell便会开启多个进程来执行这一组任务。无论是单独的一条命令，还是由管道连接的多条命令，都会被放入到一个新的进程组(任务)中。只包含一条命令的时候，就会创建一个由一个进程组成的进程组。进程组中的每个进程都具有相同的进程组标识符(进程组ID)，这个进程组标识符其实就是进程组中某个进程(即进程组组长)的进程ID。一个进程组也被称为「作业」 如上截图所示： bash：进程和进程组ID都是2571(父进程是sshd进程) ps: 进程和进程组ID都是14245，父进程是bash(2571),因为是在shell上执行的命令 cat: 进程组ID和ps的进程组相同，父进程为bash(2571)进程 那为啥Linux里要有进程组呢？其实，提供进程组就是为了方便对进程进行管理。假设要完成一个任务，需要同时并发100个进程。当用户处于某种原因要终止 这个任务时，要是没有进程组，就需要手动的一个个去杀死这100个进程。 守护进程守护进程是一种长期运行的后台服务进程，通常以daemon的方式运行，通常以字母d结尾，与其他进程相比较，它大概有如下特点： 无需控制终端(不需要与用户交互) 在后台运行 生命周期比较长，一般随系统启动和关闭 会话由一个或者多个进程组组成的集合，一个会话中的所有进程都具有相同的会话标识符，会话首进程是指创建会话的进程，其进程ID会成为会话ID。 使用会话最多的是支持任务控制的shell,由shell创建的所有进程组及shell自身隶属与同一个会话，shell自身就是该会话的会话首进程。 在任意时刻，会话中总有一个前台进程组(前台任务)，可以从终端读取输入，向终端发送输出。如果用户输入Ctrl+C或者Ctrl+Z,就可以分别让任务终止或挂起。同时，一个会话还可以拥有任意多个后台进程组，后台进程组可以用’&amp;’结尾的命令行创建。 如图，该会话中有三个进程组。通常是由shell的管道将几个进程编成一组的。上图有可能是由下列形式的shell命令形成的: 12$ proc1 | proc2 &amp; $ proc3 | proc4 | proc5 终端终端(Terminal)也是一台物理设备，只用于输入输出，本身没有强大的计算能力。一台计算机只有一个控制台，在计算资源紧张的时代，人们想共享一台计算机，可以通过终端连接到计算机上，将指令输入终端，终端传送给计算机，计算机完成指令后，将输出传送给终端，终端将结果显示给用户。 登录终端在早期的计算机上面，用户用哑终端（用硬连接连到主机）进行登录，这种登录要经由内核的终端设备驱动程序。因为连到主机上的终端设备数是固定的，所以同时的登录数也就有了已知的上限。随着图形终端的出现，创建终端窗口的应用也被开发出来，它仿真了基于字符的终端，使用户可以用熟悉的方式（shell命令行）与主机进行交互。包括使用网络进行远程登录的远程终端也是使用的这种方式。 伪终端随着图形终端的出现，创建终端窗口的应用也被开发出来，它仿真了基于字符的终端，使用户可以用熟悉的方式（shell命令行）与主机进行交互。包括使用网络进行远程登录的远程终端也是使用的这种方式。网络登录与传统的串行终端登录的区别在于，前者必须等待一个网络连接请求到达，而不是使一个进程等待每一个可能的登录。为了使同一个软件既能处理终端登录又能处理网络登录，系统使用了一种称为伪终端（pseudo terminal）的软件驱动程序，它仿真串行终端的运行行为，并将终端操作映射为网络操作。 控制终端 当一个终端与一个会话相关联后，那么这个终端就称为该会话的控制终端(controlling terminal)。 建立与控制终端连接的会话首进程被称为控制进程(controlling process)。 一个会话中的几个进程组可被分成一个前台进程组(foreground process group)以及一个或多个后台进程组(background process group)。 如果一个会话有一个控制终端的话， 则它有一个前台进程组，其他进程组为后台进程组。 无论何时键入终端的中断键或退出键，都会将中断信号或退出信号发送至前台进程组的所有进程。 如果终端检测到调制解调器（或网络）断开，则挂断信号（SIGHUP）发送至控制进程（会话首进程），如果会话首进程退出,则将挂断信号（SIGHUP）发送至前台进程组的所有进程。 相关命令jobs查看当前所有的后台进程 fg将后台运行的命令调至前台继续运行, fg 命令格式为fg %作业号 其中 ％可以省略，但若将% 工作号全部省略，则此命令会将带有 + 号的工作恢复到前台。另外，使用此命令的过程中， % 可有可无。 bg将一个在后台暂停的命令变成继续执行状态 ctrl + z可以将一个正在前台执行的命令放到后台，并且暂停 ttytty命令看看当前bash关联到了哪个tty lsoflsof 用于查看你进程打开的文件，打开文件的相关进程，进程打开的端口(TCP、UDP)等，在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。 参数 参数 描述 -a 列出打开文件存在的进程 -c&lt;进程名&gt; 列出指定进程所打开的文件 -g 列出GID号进程详情 -d&lt;文件号&gt; 列出占用该文件号的进程 +d&lt;目录&gt; 列出目录下被打开的文件 +D&lt;目录&gt; 递归列出目录下被打开的文件 -n&lt;目录&gt; 列出使用NFS的文件 -i&lt;条件&gt; 列出符合条件的进程。（4、6、协议、:端口、 @ip ） -p&lt;进程号&gt; 列出指定进程号所打开的文件 -u 列出UID号进程详情 -h 显示帮助信息 -v 显示版本信息 查看哪个进程正在使用某个文件1lsof &#x2F;bin&#x2F;bash 查看哪个进程打开指定端口1lsof -i :22 查看指定进程打开的文件信息1lsof -p 1257 参考文档 https://www.linuxidc.com/Linux/2019-02/157126.htm https://segmentfault.com/a/1190000009082089 https://segmentfault.com/a/1190000009152815 http://shareinto.k8s.101.com/2016/11/17/linux-terminal/ https://blog.csdn.net/caoshangpa/article/details/80140888 https://andrewpqc.github.io/2018/10/31/linux-process-group-and-session-and-jobs-manage/ https://www.cnblogs.com/huchong/p/10095152.html","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux中的信号量","slug":"linux-signal","date":"2021-03-25T14:55:11.000Z","updated":"2021-12-17T07:42:48.351Z","comments":true,"path":"2021/03/25/linux-signal/","link":"","permalink":"https://github.com/fafucoder/2021/03/25/linux-signal/","excerpt":"","text":"参考文档 https://www.cnblogs.com/33debug/p/6993397.html","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux-ps","slug":"linux-ps","date":"2021-03-25T14:50:07.000Z","updated":"2021-12-17T07:42:48.350Z","comments":true,"path":"2021/03/25/linux-ps/","link":"","permalink":"https://github.com/fafucoder/2021/03/25/linux-ps/","excerpt":"","text":"概述ps命令能够给出当前系统中进程的快照。它能捕获系统在某一事件的进程状态。如果你想不断更新查看的这个状态，可以使用top命令。 ps 命令支持的语法风格 UNIX风格: 选项可以组合在一起，并且选项前必须有“-”连字符 BSD风格: 选项可以组合在一起，但是选项前不能有“-”连字符 GNU风格: 选项前有两个“-”连字符 参数–sort: 对结果集进行排序 -L: 显示指定进程的线程, 例如 ps -L 1234(1234 为pid) -o: 控制输入的列数据 -e: 显示所有进程信息 -f: 做一个更为完整的输出,通常跟-e参数一块使用 -u: 以用户为主的进程状态,通常跟-a一块使用 -a: 显示现行终端机下的所有进程，包括其他用户的进程(这是当前终端的所有进程，通过配合-aux查看所有的进程) -x: 显示较完整信息。通常与-a这个参数一起使用 -A: 列出所有的进程(这是所有进程) -C: 根据进程名做筛选, 例如， ps -C nginx -j: 以job的格式展示 ps -aux 与 ps -ef的区别输出的格式不一样，ps -aux输出包括进程的cpu跟内存的使用情况，而ps -ef只输出进程的信息(如pid, ppid等)，未输出进程的更详细信息，如果要通过--sort参数做排序的话，通常使用ps -aux –sort参数详解在--sort参数中，-减号是降序，+号表示升序，–sort的排序列跟-o 的列一致，默认是升序格式 -o 格式化的参数format格式较多，可通过man ps查看，主要的参数有pid, ppid, pgid, cmd(command,agrs), pcpu, pmem, tty等 显示进程树1ps -ejH 或者 ps axjf 显示线程12ps -aux | grep containerpx -L pid 格式化输出1ps -eo pid,ppid,pgid,tty,cmd 实时输出1watch -n 1 'ps -aux --sort -pcpu,-pmem | head -20' pstreepstree命令以树状图显示进程间的关系（display a tree of processes）。ps命令可以显示当前正在运行的那些进程的信息，但是对于它们之间的关系却显示得不够清晰。在Linux系统中，系统调用fork可以创建子进程，通过子shell也可以创建子进程，Linux系统中进程之间的关系天生就是一棵树，树的根就是进程PID为1的init进程。 参数-p: 显示pid -a: 显示进程参数 进程树显示进程pid1pstree -p 显示指定进程的进程树1pstree -p 1234 显示参数1pstree -ap 1234 参考文档 https://www.cnblogs.com/huchong/p/10065246.html https://zhuanlan.zhihu.com/p/93473461","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"dockerfile 最佳实践","slug":"docker-dockerfile","date":"2021-03-17T15:19:21.000Z","updated":"2021-12-17T07:42:48.323Z","comments":true,"path":"2021/03/17/docker-dockerfile/","link":"","permalink":"https://github.com/fafucoder/2021/03/17/docker-dockerfile/","excerpt":"","text":"Dockerfile概述1、Docker build 原理概述Docker 可以从 Dockerfile 中读取指令自动构建镜像，Dockerfile是一个包含构建指定镜像所有命令的文本文件。Docker坚持使用特定的格式并且使用特定的命令。你可以在 Dockerfile参考 页面学习基本知识。 Docker 镜像由只读层组成，每个层代表一个 Dockerfile 指令。这些层是堆叠的，每一层都是前一层变化的增量。如下的Dockerfile： 12345# syntax=docker/dockerfile:1FROM ubuntu:18.04COPY . /appRUN make /appCMD python /app/app.py 每条指令创建一层： FROM从ubuntu:18.04Docker 镜像创建一个层。 COPY 从 Docker 客户端的当前目录添加文件。 RUN执行make命令. CMD 指定要在容器内运行的命令。 当运行一个镜像并生成一个容器时，Docker会在底层的顶部添加一个新的可写层（“容器层”）。对正在运行的容器所做的所有更改，例如写入新文件、修改现有文件和删除文件，都将写入此可写容器层。有关镜像层（以及 Docker 如何构建和存储镜像）的更多信息，请参阅 关于存储驱动程序。 2、 了解构建上下文当发出docker build命令时，当前目录称为构建上下文，默认情况下，Dockerfile位于当前目录中(构建上下文中), 但是可以通过-f参数执行Dockerfile的位置。无论Dockerfile实际位于何处，当前目录中文件和目录的所有内容都将作为构建上下文发送到 Docker 守护进程。 如果构建镜像中所包含的不需要的文件越多，将会导致更大的构建上下文和更大的镜像，这会增加构建镜像的时间、拉取和推送镜像的时间以及容器运行时的大小。要查看构建上下文有多大，当编译dockerfile的时候可以看到如下输出: 1Sending build context to Docker daemon 187.8MB 3、通过管道来执行DockerfileDocker 执行的Dockerfile允许来自管道，stdin的内容可以是本地内容或者远程内容,在不将Dockerfile写到本地磁盘中，或者只执行一次Dockerfile的情况下，通过管道执行dockerfile将非常有意义。 12345678# example1echo -e 'FROM busybox\\nRUN echo \"hello world\"' | docker build -# example2docker build -&lt;&lt;EOFFROM busyboxRUN echo \"hello world\"EOF 使用来自stdin的Dockerfile执行构建将无需发送构建上下文到Docker守护进程，在Dockerfile 不需要将文件复制到映像中的情况下，省略构建上下文会很有用，并且可以提高构建速度，因为没有文件被发送到守护程序。如果想通过从构建上下文中排除某些文件来提高构建速度，请参考exclude with .dockerignore。 一般性的指南和建议1、容器应该是短暂的通过 Dockerfile 构建的镜像所启动的容器应该尽可能短暂（生命周期短）。「短暂」意味着可以停止和销毁容器，并且创建一个新容器并部署好所需的设置和配置工作量应该是极小的(说的是临时镜像生命周期尽可能短暂)。 2、 使用 .dockerignore 文件使用 Dockerfile 构建镜像时最好是将 Dockerfile 放置在一个新建的空目录下。然后将构建镜像所需要的文件添加到该目录中。为了提高构建镜像的效率，你可以在目录下新建一个 .dockerignore 文件来指定要忽略的文件和目录。.dockerignore 文件的排除模式语法和 Git 的 .gitignore 文件相似。 3、使用多阶段构建多阶段构建允许您大幅减少最终图像的大小，而无需费力减少中间层和文件的数量。因为镜像是在构建过程的最后阶段构建的，所以您可以通过利用构建缓存来最小化镜像层。 12345678910111213141516171819202122232425# syntax=docker/dockerfile:1FROM golang:1.16-alpine AS build# Install tools required for project# Run `docker build --no-cache .` to update dependenciesRUN apk add --no-cache gitRUN go get github.com/golang/dep/cmd/dep# List project dependencies with Gopkg.toml and Gopkg.lock# These layers are only re-built when Gopkg files are updatedCOPY Gopkg.lock Gopkg.toml /go/src/project/WORKDIR /go/src/project/# Install library dependenciesRUN dep ensure -vendor-only# Copy the entire project and build it# This layer is rebuilt when a file changes in the project directoryCOPY . /go/src/project/RUN go build -o /bin/project# This results in a single layer imageFROM scratchCOPY --from=build /bin/project /bin/projectENTRYPOINT [\"/bin/project\"]CMD [\"--help\"] 4、避免安装不必要的包为了降低复杂性、减少依赖、减小文件大小、节约构建时间，你应该避免安装任何不必要的包。例如，不要在数据库镜像中包含一个文本编辑器。 5、一个容器只运行一个进程应该保证在一个容器中只运行一个进程。将多个应用解耦到不同容器中，保证了容器的横向扩展和复用。例如，一个web应用栈会包含3个独立的容器，每个都有自己独立的镜像，以解耦的方式来管理web应用，数据库。 6、尽量减少镜像层数你需要在 Dockerfile 可读性（也包括长期的可维护性）和减少层数之间做一个平衡。 7、将多行参数排序将多行参数按字母顺序排序（比如要安装多个包时）。这可以帮助你避免重复包含同一个包，更新包列表时也更容易。也便于 PRs 阅读和审查。建议在反斜杠符号 \\ 之前添加一个空格，以增加可读性。 下面是来自 buildpack-deps 镜像的例子： 123456RUN apt-get update &amp;&amp; apt-get install -y \\ bzr \\ cvs \\ git \\ mercurial \\ subversion 8、利用构建缓存在镜像的构建过程中，Docker 会遍历 Dockerfile 文件中的指令，然后按顺序执行。在执行每条指令之前，Docker 都会在缓存中查找是否已经存在可重用的镜像，如果有就使用现存的镜像，不再重复创建。如果你不想在构建过程中使用缓存，你可以在 docker build 命令中使用 –no-cache=true 选项。 但是，如果你想在构建的过程中使用缓存，你得明白什么时候会，什么时候不会找到匹配的镜像，遵循的基本规则如下： 从一个基础镜像开始（FROM 指令指定），下一条指令将和该基础镜像的所有子镜像进行匹配，检查这些子镜像被创建时使用的指令是否和被检查的指令完全一样。如果不是，则缓存失效。 在大多数情况下，只需要简单地对比 Dockerfile 中的指令和子镜像。然而，有些指令需要更多的检查和解释。 对于 ADD 和 COPY 指令，镜像中对应文件的内容也会被检查，每个文件都会计算出一个校验和。文件的最后修改时间和最后访问时间不会纳入校验。在缓存的查找过程中，会将这些校验和和已存在镜像中的文件校验和进行对比。如果文件有任何改变，比如内容和元数据，则缓存失效。 除了 ADD 和 COPY 指令，缓存匹配过程不会查看临时容器中的文件来决定缓存是否匹配。例如，当执行完 RUN apt-get -y update 指令后，容器中一些文件被更新，但 Docker 不会检查这些文件。这种情况下，只有指令字符串本身被用来匹配缓存。 一旦缓存失效，所有后续的 Dockerfile 指令都将产生新的镜像，缓存不会被使用。 Dockerfile指令下面针对 Dockerfile 中各种指令的最佳编写方式给出建议。 FROM尽可能使用当前官方仓库作为你构建镜像的基础。推荐使用 Alpine 镜像，因为它被严格控制并保持最小尺寸（目前小于 5 MB），但它仍然是一个完整的发行版。 LABEL你可以给镜像添加标签来帮助组织镜像、记录许可信息、辅助自动化构建等。每个标签一行，由 LABEL 开头加上一个或多个标签对。下面的示例展示了各种不同的可能格式。# 开头的行是注释内容。 注意：如果你的字符串中包含空格，必须将字符串放入引号中或者对空格使用转义。如果字符串内容本身就包含引号，必须对引号使用转义。 12345678# Set one or more individual labelsLABEL com.example.version&#x3D;&quot;0.0.1-beta&quot;LABEL vendor&#x3D;&quot;ACME Incorporated&quot;LABEL com.example.release-date&#x3D;&quot;2015-02-12&quot;LABEL com.example.version.is-production&#x3D;&quot;&quot; 一个镜像可以包含多个标签，但建议将多个标签放入到一个 LABEL 指令中。 123456# Set multiple labels at once, using line-continuation characters to break long linesLABEL vendor=ACME\\ Incorporated \\ com.example.is-beta= \\ com.example.is-production=\"\" \\ com.example.version=\"0.0.1-beta\" \\ com.example.release-date=\"2015-02-12\" 关于标签可以接受的键值对，参考 Understanding object labels。关于查询标签信息，参考 Managing labels on objects。 RUN为了保持 Dockerfile 文件的可读性，可理解性，以及可维护性，建议将长的或复杂的 RUN 指令用反斜杠 \\ 分割成多行。 apt-getRUN 指令最常见的用法是安装包用的 apt-get。因为 RUN apt-get 指令会安装包，所以有几个问题需要注意。 不要使用 RUN apt-get upgrade 或 dist-upgrade，因为许多基础镜像中的「必须」包不会在一个非特权容器中升级。如果基础镜像中的某个包过时了，你应该联系它的维护者。如果你确定某个特定的包，比如 foo，需要升级，使用 apt-get install -y foo 就行，该指令会自动升级 foo 包。 永远将 RUN apt-get update 和 apt-get install 组合成一条 RUN 声明，例如： 1234RUN apt-get update &amp;&amp; apt-get install -y \\ package-bar \\ package-baz \\ package-foo 将 apt-get update 放在一条单独的 RUN 声明中会导致缓存问题以及后续的 apt-get install 失败。比如，假设你有一个 Dockerfile 文件： 12345FROM ubuntu:18.04RUN apt-get updateRUN apt-get install -y curl 构建镜像后，所有的层都在 Docker 的缓存中。假设你后来又修改了其中的 apt-get install 添加了一个包： 12345FROM ubuntu:18.04RUN apt-get updateRUN apt-get install -y curl nginx Docker 发现修改后的 RUN apt-get update 指令和之前的完全一样。所以，apt-get update 不会执行，而是使用之前的缓存镜像。因为 apt-get update 没有运行，后面的 apt-get install 可能安装的是过时的 curl 和 nginx 版本。 使用 RUN apt-get update &amp;&amp; apt-get install -y 可以确保你的 Dockerfiles 每次安装的都是包的最新的版本，而且这个过程不需要进一步的编码或额外干预。这项技术叫作 cache busting。你也可以显示指定一个包的版本号来达到 cache-busting，这就是所谓的固定版本，例如： 1234RUN apt-get update &amp;&amp; apt-get install -y \\ package-bar \\ package-baz \\ package-foo=1.3.* 固定版本会迫使构建过程检索特定的版本，而不管缓存中有什么。这项技术也可以减少因所需包中未预料到的变化而导致的失败。 下面是一个 RUN 指令的示例模板，展示了所有关于 apt-get 的建议。 1234567891011121314RUN apt-get update &amp;&amp; apt-get install -y \\ aufs-tools \\ automake \\ build-essential \\ curl \\ dpkg-sig \\ libcap-dev \\ libsqlite3-dev \\ mercurial \\ reprepro \\ ruby1.9.1 \\ ruby1.9.1-dev \\ s3cmd=1.1.* \\ &amp;&amp; rm -rf /var/lib/apt/lists/* 其中 s3cmd 指令指定了一个版本号 1.1.*。如果之前的镜像使用的是更旧的版本，指定新的版本会导致 apt-get udpate 缓存失效并确保安装的是新版本。 另外，清理掉 apt 缓存 var/lib/apt/lists 可以减小镜像大小。因为 RUN 指令的开头为 apt-get udpate，包缓存总是会在 apt-get install 之前刷新。 注意：官方的 Debian 和 Ubuntu 镜像会自动运行 apt-get clean，所以不需要显式的调用 apt-get clean。 CMDCMD 指令用于执行目标镜像中包含的软件，可以包含参数。CMD 大多数情况下都应该以 CMD [“executable”, “param1”, “param2”…] 的形式使用。因此，如果创建镜像的目的是为了部署某个服务(比如 Apache)，你可能会执行类似于 CMD [“apache2”, “-DFOREGROUND”] 形式的命令。我们建议任何服务镜像都使用这种形式的命令。 多数情况下，CMD 都需要一个交互式的 shell (bash, Python, perl 等)，例如 CMD [“perl”, “-de0”]，或者 CMD [“PHP”, “-a”]。使用这种形式意味着，当你执行类似 docker run -it python 时，你会进入一个准备好的 shell 中。CMD 应该在极少的情况下才能以 CMD [“param”, “param”] 的形式与 ENTRYPOINT 协同使用，除非你和你的镜像使用者都对 ENTRYPOINT 的工作方式十分熟悉。 EXPOSEEXPOSE 指令用于指定容器将要监听的端口。因此，你应该为你的应用程序使用常见的端口。例如，提供 Apache web 服务的镜像应该使用 EXPOSE 80，而提供 MongoDB 服务的镜像使用 EXPOSE 27017。 对于外部访问，用户可以在执行 docker run 时使用一个标志来指示如何将指定的端口映射到所选择的端口。 ENV为了方便新程序运行，你可以使用 ENV 来为容器中安装的程序更新 PATH 环境变量。例如使用 ENV PATH /usr/local/nginx/bin:$PATH 来确保 CMD [“nginx”] 能正确运行。 ENV 指令也可用于为你想要容器化的服务提供必要的环境变量，比如 Postgres 需要的 PGDATA。 最后，ENV 也能用于设置常见的版本号，比如下面的示例： 1234567ENV PG_MAJOR 9.3ENV PG_VERSION 9.3.4RUN curl -SL http:&#x2F;&#x2F;example.com&#x2F;postgres-$PG_VERSION.tar.xz | tar -xJC &#x2F;usr&#x2F;src&#x2F;postgress &amp;&amp; …ENV PATH &#x2F;usr&#x2F;local&#x2F;postgres-$PG_MAJOR&#x2F;bin:$PATH 类似于程序中的常量，这种方法可以让你只需改变 ENV 指令来自动的改变容器中的软件版本。 ADD 和 COPY虽然 ADD 和 COPY 功能类似，但一般优先使用 COPY。因为它比 ADD 更透明。COPY 只支持简单将本地文件拷贝到容器中，而 ADD 有一些并不明显的功能（比如本地 tar 提取和远程 URL 支持）。因此，ADD 的最佳用例是将本地 tar 文件自动提取到镜像中，例如 ADD rootfs.tar.xz。 如果你的 Dockerfile 有多个步骤需要使用上下文中不同的文件。单独 COPY 每个文件，而不是一次性的 COPY 所有文件，这将保证每个步骤的构建缓存只在特定的文件变化时失效。例如： 12345COPY requirements.txt /tmp/RUN pip install --requirement /tmp/requirements.txtCOPY . /tmp/ 如果将 COPY . /tmp/ 放置在 RUN 指令之前，只要 . 目录中任何一个文件变化，都会导致后续指令的缓存失效。 为了让镜像尽量小，最好不要使用 ADD 指令从远程 URL 获取包，而是使用 curl 和 wget。这样你可以在文件提取完之后删掉不再需要的文件来避免在镜像中额外添加一层。比如尽量避免下面的用法： 12345ADD http://example.com/big.tar.xz /usr/src/things/RUN tar -xJf /usr/src/things/big.tar.xz -C /usr/src/thingsRUN make -C /usr/src/things all 而是应该使用下面这种方法： 1234RUN mkdir -p /usr/src/things \\ &amp;&amp; curl -SL http://example.com/big.tar.xz \\ | tar -xJC /usr/src/things \\ &amp;&amp; make -C /usr/src/things all 上面使用的管道操作，所以没有中间文件需要删除。 对于其他不需要 ADD 的自动提取功能的文件或目录，你应该使用 COPY。 ENTRYPOINTENTRYPOINT 的最佳用处是设置镜像的主命令，允许将镜像当成命令本身来运行（用 CMD 提供默认选项）。 例如，下面的示例镜像提供了命令行工具 s3cmd: 123ENTRYPOINT [\"s3cmd\"]CMD [\"--help\"] ENTRYPOINT 指令也可以结合一个辅助脚本使用，和前面命令行风格类似，即使启动工具需要不止一个步骤。 例如，Postgres 官方镜像使用下面的脚本作为 ENTRYPOINT： 1234567891011121314#!/bin/bashset -eif [ \"$1\" = 'postgres' ]; then chown -R postgres \"$PGDATA\" if [ -z \"$(ls -A \"$PGDATA\")\" ]; then gosu postgres initdb fi exec gosu postgres \"$@\"fiexec \"$@\" 注意：该脚本使用了 Bash 的内置命令 exec，所以最后运行的进程就是容器的 PID 为 1 的进程。这样，进程就可以接收到任何发送给容器的 Unix 信号了。 该辅助脚本被拷贝到容器，并在容器启动时通过 ENTRYPOINT 执行： 123COPY ./docker-entrypoint.sh /ENTRYPOINT [\"/docker-entrypoint.sh\"] VOLUMEVOLUME 指令用于暴露任何数据库存储文件，配置文件，或容器创建的文件和目录。强烈建议使用 VOLUME 来管理镜像中的可变部分和用户可以改变的部分。 USER如果某个服务不需要特权执行，建议使用 USER 指令切换到非 root 用户。先在 Dockerfile 中使用类似 RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 的指令创建用户和用户组。 注意：在镜像中，用户和用户组每次被分配的 UID/GID 都是不确定的，下次重新构建镜像时被分配到的 UID/GID 可能会不一样。如果要依赖确定的 UID/GID，你应该显示的指定一个 UID/GID。 你应该避免使用 sudo，因为它不可预期的 TTY 和信号转发行为可能造成的问题比它能解决的问题还多。如果你真的需要和 sudo 类似的功能（例如，以 root 权限初始化某个守护进程，以非 root 权限执行它），你可以使用 gosu。 最后，为了减少层数和复杂度，避免频繁地使用 USER 来回切换用户。 WORKDIR为了清晰性和可靠性，你应该总是在 WORKDIR 中使用绝对路径。另外，你应该使用 WORKDIR 来替代类似于 RUN cd … &amp;&amp; do-something 的指令，后者难以阅读、排错和维护。 参考文档 Dockerfile 参考 // dockerfile各个字段含义 Dockerfile 最佳实践 // dockerfile 最佳实践 Dockerfile 最佳实践中文版本","categories":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/tags/docker/"}]},{"title":"linux openssl 命令详解","slug":"linux-openssl","date":"2021-03-09T12:28:02.000Z","updated":"2021-12-17T07:42:48.350Z","comments":true,"path":"2021/03/09/linux-openssl/","link":"","permalink":"https://github.com/fafucoder/2021/03/09/linux-openssl/","excerpt":"","text":"openssl 命令模块openssl 命令主要包括以下几个模块： version模块: 用于查看openssl版本信息 s_client/s_server模块: 通用SSL/TLS测试工具 genrsa: 用于生成私钥 x509： x509证书管理 verify: x509证书验证 rsa: RSA秘钥管理(例如对证书进行签名) rsautl: 用于完成RSA签名，验证，加密和解密功能 req: 生成证书签名请求(CSR) enc: 用于加解密 dgst: 生成信息摘要 passwd: 生成散列密码 rand: 生成伪随机数 pkcs7: PSCS#7协议数据管理 ca: CA管理(例如对证书进行签名) …….. openssl s_client 模块s_client为一个SSL/TLS客户端程序，与s_server对应，它不仅能与s_server进行通信，也能与任何使用ssl协议的其他服务程序进行通信。 使用s_client 获取百度的公钥: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114[root@node1 ~]# openssl s_client -connect www.baidu.com:443 -msgCONNECTED(00000003)depth=2 C = BE, O = GlobalSign nv-sa, OU = Root CA, CN = GlobalSign Root CAverify return:1depth=1 C = BE, O = GlobalSign nv-sa, CN = GlobalSign Organization Validation CA - SHA256 - G2verify return:1depth=0 C = CN, ST = beijing, L = beijing, OU = service operation department, O = \"Beijing Baidu Netcom Science Technology Co., Ltd\", CN = baidu.comverify return:1---Certificate chain 0 s:/C=CN/ST=beijing/L=beijing/OU=service operation department/O=Beijing Baidu Netcom Science Technology Co., Ltd/CN=baidu.com i:/C=BE/O=GlobalSign nv-sa/CN=GlobalSign Organization Validation CA - SHA256 - G2 1 s:/C=BE/O=GlobalSign nv-sa/CN=GlobalSign Organization Validation CA - SHA256 - G2 i:/C=BE/O=GlobalSign nv-sa/OU=Root CA/CN=GlobalSign Root CA---Server certificate-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----subject=/C=CN/ST=beijing/L=beijing/OU=service operation department/O=Beijing Baidu Netcom Science Technology Co., Ltd/CN=baidu.comissuer=/C=BE/O=GlobalSign nv-sa/CN=GlobalSign Organization Validation CA - SHA256 - G2---No client certificate CA names sentPeer signing digest: SHA256Server Temp Key: ECDH, P-256, 256 bits---SSL handshake has read 4392 bytes and written 415 bytes---New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES128-GCM-SHA256Server public key is 2048 bitSecure Renegotiation IS supportedCompression: NONEExpansion: NONENo ALPN negotiatedSSL-Session: Protocol : TLSv1.2 Cipher : ECDHE-RSA-AES128-GCM-SHA256 Session-ID: A9961C486ACB6F5924E86E822E48559DBD572EE990D790C845CBE512FE62B069 Session-ID-ctx: Master-Key: 8D0DC84049E24BDBB3B45B1A73375F04E4D65E51D59F17EBAD513C68D14B65E5B9218317050827352392808C9DFBC3DA Key-Arg : None Krb5 Principal: None PSK identity: None PSK identity hint: None TLS session ticket: 0000 - 45 aa 4e e0 47 04 bf ef-18 a2 10 48 8e d6 ec 94 E.N.G......H.... 0010 - 89 f4 c8 dc cc 86 5b 2b-fe f9 95 e6 97 14 28 45 ......[+......(E 0020 - 1c 3f 6e ad 7c 2d f5 d8-55 a5 5a 43 83 e0 af 80 .?n.|-..U.ZC.... 0030 - 7f a4 49 5a cd d0 ea f3-0c 18 f7 e1 36 7c 17 28 ..IZ........6|.( 0040 - 56 9a 9d 0c 6a 9f e7 c5-18 cf 28 c1 ba e5 b0 8b V...j.....(..... 0050 - 95 5a 83 27 95 9d ba 3e-2b 96 89 f2 3c 54 52 b0 .Z.'...&gt;+...&lt;TR. 0060 - 76 b0 75 6f 03 67 32 77-0b 62 f8 bc 7d 75 07 68 v.uo.g2w.b..&#125;u.h 0070 - 65 fd bb 88 80 83 d2 5b-80 2e 50 97 56 a0 94 5e e......[..P.V..^ 0080 - e6 4e 37 f6 e8 96 1c d2-cf fd 8b 74 0d 6b eb 98 .N7........t.k.. 0090 - 64 0c 4e 88 30 26 17 03-11 58 30 60 75 b5 c6 5f d.N.0&amp;...X0`u.._ Start Time: 1616261167 Timeout : 300 (sec) Verify return code: 0 (ok)--- openssl verify 模块openssl verify 用于验证证书是否合法，还可以用于验证子证书是否是父证书签的. 验证子证书是否由父证书签发的: 123456[root@node1 openssl2]# openssl verify rootCA.crtrootCA.crt: C = CN, ST = Fujian, L = Fuzhou, O = Root CA, CN = Root CAerror 18 at 0 depth lookup:self signed certificateOK[root@node1 openssl2]# openssl verify -CAfile rootCA.crt harbor.crtharbor.crt: OK openssl genrsa 模块openssl genrsa用于生成私钥(只生成私钥) -out filename: 将生成的私钥保存至filename文件,若未指定输出文件,则为标准输出。-des:生成的密钥使用des方式进行加密。-des3:生成的密钥使用des3方式进行加密。-passout args:加密私钥文件时,传递密码的格式,如果要加密私钥文件时单未指定该项,则提示输入密码。传递密码的args的格式,可从密码、环境变量、文件、终端等输入。 a. pass:password:password表示传递的明文密码 b. env:var:从环境变量var获取密码值 c .file:filename:filename文件中的第一行为要传递的密码。若filename同时传递给”-passin”和”-passout”选项，则filename的第一行为”-passin”的值，第二行为”-passout”的值 d. stdin:从标准输入中获取要传递的密码 numbits: 指定要生成的私钥的长度,默认为1024。该项必须为命令行的最后一项参数。 123456# 输入密码的方式openssl genrsa -des3 -out rootCA.key 2048# 环境变量的方式export passwd=adminopenssl genrsa -des3 -out rootCA.key -passout env:passwd 2048 openssl rsa 模块rsa模块用于生成的密码的管理，包括查看秘钥的信息，更改秘钥的密码等 12345# 去除genrsa生成的密码(每次都要输入密码怪麻烦的~)openssl rsa -in rootCA.key -out rootCA.key# 查看私钥的信息openssl rsa -in rootCA.key -noout -text openssl req 模块req大致有3个功能：生成证书请求文件、验证证书请求文件和创建根CA。 123456789101112131415# 查看csr的信息openssl req -in harbor.csr -noout -text# 查看csr 的subject 信息openssl req -new -in harbor.csr -subject -noout# 验证csr 是否合法openssl req -in harbor.csr -verify# 创建csropenssl req -new -key harbor.key -out harbor.csr# 创建csr和私钥(# -newkey选项和-new选项类似，只不过-newkey选项可以直接指定私钥的算法和长度，所以它主要用在openssl req自动创建私钥。openssl req -newkey rsa:2038 -keyout harbor.key -out harbor.csr openssl x509 模块x509 模块主要的功能有 签署证书请求文件、生成自签名证书、转换证书格式等。 1234567891011# 查看公钥信息openssl x509 -in harbor.crt -noout -text# 查看公钥颁发者信息/所有者信息openssl x509 -in harbor.crt -text -issuer/-subjet# 生成公钥openssl x509 -req -in harbor.csr -signkey harbor.key -out harbor.crt# 签署CA证书openssl x509 -req -CA rootCA.crt -CAkey rootCA.key -in sub.csr -out sub.crt -days 365 -CAcreateserial openssl ca 模块ca命令能够签发证书请求文件生成证书 1openssl ca -config openssl.cnf -days 365 -create_serial -in harbor.csr -out harbor.crt -extensions ca_ext -extensions req_ext -notext 创建一个自签名证书12345678910111213141516# 创建根证书openssl req -x509 -days 365 -newkey rsa:2048 -out rootCA.crt -keyout rootCA.key -nodes# 创建子证书的私钥跟rsaopenssl genrsa -des3 -out sub.key 2048openssl rsa -in sub.key -out sub.key #去除私钥的密码openssl req -new -key sub.key -out sub.csr# 使用根证书签名子证书openssl x509 -req -CA rootCA.crt -CAkey rootCA.key -in sub.csr -out sub.crt -days 365 -set_serial 123 #-set_serial 用于设置sub.crt 的SN序列号# 验证子证书openssl verify -CAfile rootCA.crt sub.crt# 查看子证书的公钥信息(可以看到issuer为根证书的信息)openssl x509 -in sub.crt -noout -text 创建带SAN的自签名证书什么叫SANSAN(Subject Alternative Name) 是 SSL 标准 x509 中定义的一个扩展。使用了 SAN 字段的 SSL 证书，可以扩展此证书支持的域名，使得一个证书可以支持多个不同域名的解析。(下面是百度证书中的SAN信息) 如何创建带SAN的自签证书​ 上个步骤中，创建的自签名证书由于没有带SAN信息，所以无法在浏览器中使用(一般作为中间证书使用)，如果要在浏览器中使用，必须带有SAN信息， ​ 在真实场景下，根证书自身就是可信的(内置在操作系统中), 然后由根证书签发中间证书，中间证书再签发最终的SAN证书, 所以一般的流程为先创建根证书，然后创建中间证书，由中间证书签发最终的证书。接下来将创建一个带SAN的自签名证书 创建根证书 1openssl req -x509 -days 365 -newkey rsa:2048 -out rootCA.crt -keyout rootCA.key -nodes 生成中间证书的私钥和CSR。 1openssl req -new -nodes -keyout intermediate.key -out intermediate.csr 添加san配置信息如下(只需要修改域名信息即可), 其中alt_names 保存域名信息(暂时保存为openssl.cnf) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253[ req ]default_bits = 2048distinguished_name = req_distinguished_namereq_extensions = req_ext[ ca ]default_ca = intermediate_ca[ intermediate_ca ]dir = .private_key = $dir/rootCA.keycertificate = $dir/rootCA.crtnew_certs_dir = $dir/serial = $dir/crt.srldatabase = $dir/db/indexdefault_md = sha256policy = policy_anyemail_in_dn = no[ req_distinguished_name ]countryName = Country Name (2 letter code)stateOrProvinceName = State or Province Name (full name)localityName = Locality Name (eg, city)organizationName = Organization Name (eg, company)commonName = Common Name (eg, your name or your server\\'s hostname)emailAddress = Email Address[ policy_any ]domainComponent = optionalcountryName = optionalstateOrProvinceName = optionallocalityName = optionalorganizationName = optionalorganizationalUnitName = optionalcommonName = optionalemailAddress = optional[ ca_ext ]keyUsage = critical,keyCertSign,cRLSign# 注意这里设置了CA:true，表明使用该配置生成的证书是CA证书，可以用于签发用户证书basicConstraints = critical,CA:truesubjectKeyIdentifier = hashauthorityKeyIdentifier = keyid:always[ req_ext ]subjectAltName = @alt_names[alt_names]# 填写域名DNS.1 = hello.comDNS.2 = san.comDNS.3 = harbor.comDNS.4 = *.baidu.com 创建数据库 12mkdir dbtouch db/index 生成证书的私钥和CSR 1openssl req -newkey rsa:2048 -nodes -out harbor.csr -keyout harbor.key -config openssl.cnf 验证CSR中是否包含文件 1openssl req -in harbor.csr -noout -text 生成公钥 1openssl ca -config openssl.cnf -days 365 -create_serial -in harbor.csr -out harbor.crt -extensions ca_ext -extensions req_ext -notext 验证公钥是否包含SAN 1openssl x509 -in harbor.crt -noout -text 参考文档 https://blog.csdn.net/baidu_36649389/article/details/54379935 //openssl 命令 openssl SAN证书 //如何创建带SAN证书 openssl SAN证书 //英文版 cakey.pem 找不到解决方法 //创建SAN证书中，无法找到cakey.pem的解决方法","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"关于x509证书的梳理","slug":"linux-ceritificate","date":"2021-03-09T08:31:54.000Z","updated":"2021-12-17T07:42:48.341Z","comments":true,"path":"2021/03/09/linux-ceritificate/","link":"","permalink":"https://github.com/fafucoder/2021/03/09/linux-ceritificate/","excerpt":"","text":"概念梳理参考文档 https://segmentfault.com/a/1190000020811310?utm_source=tag-newest //推荐阅读 https://www.jianshu.com/p/a9497de4cbff https://zhaohuabing.com/post/2020-03-19-pki/ //推荐阅读 https://zhaohuabing.com/post/2020-05-19-k8s-certificate/ 推荐阅读","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"docker 的日志实现","slug":"docker-log","date":"2021-03-07T13:40:31.000Z","updated":"2021-12-17T07:42:48.324Z","comments":true,"path":"2021/03/07/docker-log/","link":"","permalink":"https://github.com/fafucoder/2021/03/07/docker-log/","excerpt":"","text":"参考文档 「Allen 谈 Docker 系列」之 docker logs 实现剖析 「Allen 谈 Docker 系列」之 docker exec 与容器日志 「Allen 谈 Docker 系列」之 Docker 容器日志的那些事儿","categories":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/tags/docker/"}]},{"title":"linux中stdin, stdout, stderr","slug":"linux-std","date":"2021-03-07T03:45:51.000Z","updated":"2021-12-17T07:42:48.351Z","comments":true,"path":"2021/03/07/linux-std/","link":"","permalink":"https://github.com/fafucoder/2021/03/07/linux-std/","excerpt":"","text":"文件描述符对Linux进程来讲，每个打开的文件都是通过文件描述符(File Descriptor)来标识的，内核为每个进程维护了一个文件描述符表，这个表以FD为索引，再进一步指向文件的详细信息。在进程创建时，内核为进程默认创建了0、1、2三个特殊的FD，这就是STDIN、STDOUT和STDERR。 I/O重定向也就是让已创建的FD指向其他文件。比如，下面是对STDOUT重定向到testfile.txt前后内核文件描述符表变化的示意图 在I/O重定向的过程中，不变的是FD 0/1/2代表STDIN/STDOUT/STDERR，变化的是文件描述符表中FD 0/1/2对应的具体文件， Shell正是通过I/O重定向和管道这种特殊的文件把多个程序的STDIN和STDOUT串联在一起组成更复杂功能的，下面是Shell中通过管道的示意图： 示例： pgrep 命令pgrep查看当前正在运行的进程，并将与选择条件匹配的进程ID列出到stdout（屏幕）。当你想要某个进程的PID时，pgrep很方便。(通过ps -ef | grep xxx也能实现) 选项# -o：仅显示找到的最小（起始）进程号； -n：仅显示找到的最大（结束）进程号； -l：显示进程名称； -P：指定父进程号； -g：指定进程组； -t：指定开启进程的终端； -u：指定进程的有效用户ID。 参考文档 https://www.cnblogs.com/weidagang2046/p/io-redirection.html https://zhuanlan.zhihu.com/p/99120125","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux中进程，线程，协程","slug":"linux-process","date":"2021-03-06T15:47:19.000Z","updated":"2021-12-17T07:42:48.350Z","comments":true,"path":"2021/03/06/linux-process/","link":"","permalink":"https://github.com/fafucoder/2021/03/06/linux-process/","excerpt":"","text":"进程， 线程，协程进程进程是操作系统对一个正在运行的程序的一种抽象，进程是资源分配的最小单位。为什么会有 ”进程“ 呢？说白了还是为了合理压榨 CPU 的性能和分配运行的时间片，不能 “闲着“。 进程的控制结构在linux操作系统中，进程控制块PCB用来唯一标识一个进程，这意味一个进程 一定会有对应的PCB，进程消失，PCB也会随之消失。在不同的操作系统中对进程的控制和管理机制不同，PCB中的信息多少不一样，通常PCB应包含如下一些信息。 进程 标识符(name)：每个进程都必须有一个唯一的标识符，可以是字符串，也可以是一个数字。 进程控制和状态管理: 说明进程当前所处的状态。为了管理的方便，系统设计时会将相同的状态的进程组成一个队列，如就绪进程队列，等待进程队列等。 进程资源清单：列出所拥有的除CPU外的资源记录，如拥有的I/O设备，打开的文件列表等。 进程优先级priority：进程的优先级反映进程的紧迫程度，通常由用户指定和系统设置。 CPU相关信息: 当进程因某种原因不能继续占用CPU时（如等待打印机），释放CPU，这时就要将CPU的各种状态信息保护起来，为将来再次得到处理机恢复CPU的各种状态，继续运行。 进程相应的程序和数据地址，以便把PCB与其程序和数据联系起来。 与进程有关的其他信息。 如进程记账信息，进程占用CPU的时间等。 在linux 中每一个进程都由task_struct 数据结构来定义, task_struct就是我们通常所说的PCB。结构如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132struct task_struct &#123; volatile long state; //说明了该进程是否可以执行，还是可中断等信息 unsigned long flags; // flags 是进程号，在调用fork()时给出 int sigpending; // 进程上是否有待处理的信号 mm_segment_t addr_limit; //进程地址空间,区分内核进程与普通进程在内存存放的位置不同 //0-0xBFFFFFFF for user-thead //0-0xFFFFFFFF for kernel-thread //调度标志,表示该进程是否需要重新调度,若非0,则当从内核态返回到用户态,会发生调度 volatile long need_resched; int lock_depth; //锁深度 long nice; //进程的基本时间片 //进程的调度策略,有三种,实时进程:SCHED_FIFO,SCHED_RR, 分时进程:SCHED_OTHER unsigned long policy; struct mm_struct *mm; //进程内存管理信息 int processor; //若进程不在任何CPU上运行, cpus_runnable 的值是0，否则是1 这个值在运行队列被锁时更新 unsigned long cpus_runnable, cpus_allowed; struct list_head run_list; //指向运行队列的指针 unsigned long sleep_time; //进程的睡眠时间 //用于将系统中所有的进程连成一个双向循环链表, 其根是init_task struct task_struct *next_task, *prev_task; struct mm_struct *active_mm; struct list_head local_pages; //指向本地页面 unsigned int allocation_order, nr_local_pages; struct linux_binfmt *binfmt; //进程所运行的可执行文件的格式 int exit_code, exit_signal; int pdeath_signal; //父进程终止时向子进程发送的信号 unsigned long personality; //Linux可以运行由其他UNIX操作系统生成的符合iBCS2标准的程序 int did_exec:1; pid_t pid; //进程标识符,用来代表一个进程 pid_t pgrp; //进程组标识,表示进程所属的进程组 pid_t tty_old_pgrp; //进程控制终端所在的组标识 pid_t session; //进程的会话标识 pid_t tgid; int leader; //表示进程是否为会话主管 struct task_struct *p_opptr,*p_pptr,*p_cptr,*p_ysptr,*p_osptr; struct list_head thread_group; //线程链表 struct task_struct *pidhash_next; //用于将进程链入HASH表 struct task_struct **pidhash_pprev; wait_queue_head_t wait_chldexit; //供wait4()使用 struct completion *vfork_done; //供vfork() 使用 unsigned long rt_priority; //实时优先级，用它计算实时进程调度时的weight值 //it_real_value，it_real_incr用于REAL定时器，单位为jiffies, 系统根据it_real_value //设置定时器的第一个终止时间. 在定时器到期时，向进程发送SIGALRM信号，同时根据 //it_real_incr重置终止时间，it_prof_value，it_prof_incr用于Profile定时器，单位为jiffies。 //当进程运行时，不管在何种状态下，每个tick都使it_prof_value值减一，当减到0时，向进程发送 //信号SIGPROF，并根据it_prof_incr重置时间. //it_virt_value，it_virt_value用于Virtual定时器，单位为jiffies。当进程运行时，不管在何种 //状态下，每个tick都使it_virt_value值减一当减到0时，向进程发送信号SIGVTALRM，根据 //it_virt_incr重置初值。 unsigned long it_real_value, it_prof_value, it_virt_value; unsigned long it_real_incr, it_prof_incr, it_virt_value; struct timer_list real_timer; //指向实时定时器的指针 struct tms times; //记录进程消耗的时间 unsigned long start_time; //进程创建的时间 //记录进程在每个CPU上所消耗的用户态时间和核心态时间 long per_cpu_utime[NR_CPUS], per_cpu_stime[NR_CPUS]; //内存缺页和交换信息: //min_flt, maj_flt累计进程的次缺页数（Copy on Write页和匿名页）和主缺页数（从映射文件或交换 //设备读入的页面数）； nswap记录进程累计换出的页面数，即写到交换设备上的页面数。 //cmin_flt, cmaj_flt, cnswap记录本进程为祖先的所有子孙进程的累计次缺页数，主缺页数和换出页面数。 //在父进程回收终止的子进程时，父进程会将子进程的这些信息累计到自己结构的这些域中 unsigned long min_flt, maj_flt, nswap, cmin_flt, cmaj_flt, cnswap; int swappable:1; //表示进程的虚拟地址空间是否允许换出 //进程认证信息 //uid,gid为运行该进程的用户的用户标识符和组标识符，通常是进程创建者的uid，gid //euid，egid为有效uid,gid //fsuid，fsgid为文件系统uid,gid，这两个ID号通常与有效uid,gid相等，在检查对于文件 //系统的访问权限时使用他们。 //suid，sgid为备份uid,gid uid_t uid,euid,suid,fsuid; gid_t gid,egid,sgid,fsgid; int ngroups; //记录进程在多少个用户组中 gid_t groups[NGROUPS]; //记录进程所在的组 //进程的权能，分别是有效位集合，继承位集合，允许位集合 kernel_cap_t cap_effective, cap_inheritable, cap_permitted; int keep_capabilities:1; struct user_struct *user; struct rlimit rlim[RLIM_NLIMITS]; //与进程相关的资源限制信息 unsigned short used_math; //是否使用FPU char comm[16]; //进程正在运行的可执行文件名 int link_count, total_link_ count; //文件系统信息 //NULL if no tty 进程所在的控制终端，如果不需要控制终端，则该指针为空 struct tty_struct *tty; unsigned int locks; //进程间通信信息 struct sem_undo *semundo; //进程在信号灯上的所有undo操作 struct sem_queue *semsleeping; //当进程因为信号灯操作而挂起时，他在该队列中记录等待的操作 //进程的CPU状态，切换时，要保存到停止进程的task_struct中 struct thread_struct thread; struct fs_struct *fs; //文件系统信息 struct files_struct *files; //打开文件信息 spinlock_t sigmask_lock; //信号处理函数 struct signal_struct *sig; //信号处理函数 sigset_t blocked; //进程当前要阻塞的信号，每个信号对应一位 struct sigpending pending; //进程上是否有待处理的信号 unsigned long sas_ss_sp; size_t sas_ss_size; int (*notifier)(void *priv); void *notifier_data; sigset_t *notifier_mask; u32 parent_exec_id; u32 self_exec_id; spinlock_t alloc_lock; void *journal_info;&#125; PCB通过链表的方式进行组织，把具有相同状态的进程链在一起，组成各种队列, 例如将处于就绪状态的进程链在一块，形成就绪队列，将所有因等待某事件而处于等待队列的进程链在一块形成阻塞队列。 进程的状态进程的执行期间，至少具备三种基本状态，即运行态、就绪态、阻塞态。 运行态(Runing)：此刻进程占用CPU; 就绪态(Ready): 可运行，但因为其他进程正在运行而暂停停止 阻塞状态(Blocked)：该进程等待某个事件（比如IO读取）停止运行，这时，即使给它CPU控制权，它也无法运行 如上所示： CPU调度绪态进程执行，进入运行状态，时间片使用完了，回到就绪态，等待 CPU调度 CPU调度绪态进程执行，进入运行状态，执行IO请求，进入阻塞态，IO请求完成，CPU收到 中断 信号，进入就绪态，等待 CPU 调度 如果在三态基础上，做进一步细化，出现了另外两个基本状态，创建态和结束态。 创建态（new）：进程正在被创建 就绪态（Ready）：可运行，但因为其他进程正在运行而暂停停止 运行态（Runing）：时刻进程占用 C P U 结束态（Exit）：进程正在从系统中消失时的状态 阻塞状态（Blocked）：该进程等待某个事件（比如IO读取）停止运行，这时，即使给它CPU控制权，它也无法运行 CPU的上下文切换CPU上下文是指 CPU寄存器和程序计数器 CPU寄存器是CPU内置的容量小，速度极快的缓存 程序计数器是用来存储CPU正在执行的指令位置或即将执行的下一条指令位置 CPU上下文切就是把前一个任务的CPU上下文保存起来，然后在加载当前任务的CPU上下文，最后再跳转到 程序计数器 所指的新位置，运行任务。 上面说到所谓的「任务」，主要包含进程、线程和中断。所以，可以根据任务的不同，把 CPU 上下文切换分成： 进程上下文切换、线程上下文切换和中断上下文切换。 进程的上下文切换CPU把一个进程切换到另一个进程运行的过程，称为进程上下文切换。 首先进程是由内核管理与调度的，所以进程上下文切换发生在内核态，进程上下文切换的内容包含用户空间资源（虚拟内存、栈、全局变量等）与内核空间资源（内核堆栈、寄存器等）。在做上下文切换的时候，会把前一个 进程 的上下文保存到它的 PCB中，然后加载当前 进程 的 PCB上下文到 CPU中，使得 进程 继续执行。 进程上下文切换的场景： 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，切换到其它正在等待 CPU 的进程运行 进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。 当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。 当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行 发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。 线程有了多进程，想必在操作系统上可以同时运行多个进程。那么为什么有了进程，还要线程呢？ 原因如下： 进程间的信息难以共享数据，父子进程并未共享内存，需要通过进程间通信（IPC），在进程间进行信息交换，性能开销较大。 创建进程（一般是调用 fork 方法）的性能开销较大。 一个进程可以由多个称为线程的执行单元组成。每个线程都运行在进程的上下文中，共享着同样的代码和全局数据。多个进程，就可以有更多的线程。多线程比多进程之间更容易共享数据，在上下文切换中线程一般比进程更高效。每个线程都有独立一套的寄存器和栈，这样可以确保线程的控制流是相对独立的。 引入线程带来的好处有以下几点 一个进程中可以同时存在多个线程 让进程具备多任务并行处理能力 同进程下的各个线程之间可以共享进程资源 （同进程内的多线程通信十分简单高效） 更轻量与高效 线程的上下文切换当进程只有一个线程时，可以认为进程等于线程，线程上下文的切换分两种情况 不同进程的线程，切换的过程就跟进程上下文切换一样 两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据 线程模型线程的实现模型主要有3种：内核级线程模型、用户级线程模型和混合型线程模型。它们之间最大的区别在于线程与内核调度实体KSE(Kernel Scheduling Entity)之间的对应关系上。所谓的内核调度实体KSE 就是指可以被操作系统内核调度器调度的对象实体，有些地方也称其为内核级线程，是操作系统内核的最小调度单元。在linux中，这样一个KSE就是一个轻量级的进程（通过clone系统调用创建出来的）。 内核级线程模型因为内核线程是由内核空间管理，所以它的 结构线程控制块（Thread Control Block, TCB） 在内核空间，操作系统对 T C B 是可见的。内核线程与KSE是1对1关系(1:1)。大部分编程语言的线程库(如linux的pthread，Java的java.lang.Thread，C++11的std::thread等等)都是对操作系统的线程（内核级线程）的一层封装，创建出来的每个线程与一个不同的KSE静态关联，因此其调度完全由OS调度器来做。这种方式实现简单，直接借助OS提供的线程能力，并且不同用户线程之间一般也不会相互影响。但其创建，销毁以及多个线程之间的上下文切换等操作都是直接由OS层面亲自来做，在需要使用大量线程的场景下对OS的性能影响会很大。 用户级线程模型因为 用户线程 在用户空间，是由 用户态 通过线程库来管理，所以它的 结构线程控制块（Thread Control Block, TCB） 也是在线程库里面，对于操作系统而言是看不到 T C B 的，它只能看到整个进程的 P C B（内核无法管理用户线程，也感知不到用户线程）。 用户线程与KSE是多对1关系(M:1)，这种线程的创建，销毁以及多个线程之间的协调等操作都是由用户自己实现的线程库来负责，对OS内核透明，一个进程中所有创建的线程都与同一个KSE在运行时动态关联。现在有许多语言实现的 协程 基本上都属于这种方式。这种实现方式相比内核级线程可以做的很轻量级，对系统资源的消耗会小很多，因此可以创建的数量与上下文切换所花费的代价也会小得多。但该模型有个致命的缺点，如果我们在某个用户线程上调用阻塞式系统调用(如用阻塞方式read网络IO)，那么一旦KSE因阻塞被内核调度出CPU的话，剩下的所有对应的用户线程全都会变为阻塞状态（整个进程挂起）。所以这些语言的协程库会把自己一些阻塞的操作重新封装为完全的非阻塞形式，然后在以前要阻塞的点上，主动让出自己，并通过某种方式通知或唤醒其他待执行的用户线程在该KSE上运行，从而避免了内核调度器由于KSE阻塞而做上下文切换，这样整个进程也不会被阻塞了。 混合型线程模型用户线程与KSE是多对多关系(M:N), 这种实现综合了前两种模型的优点，为一个进程中创建多个KSE，并且线程可以与不同的KSE在运行时进行动态关联，当某个KSE由于其上工作的线程的阻塞操作被内核调度出CPU时，当前与其关联的其余用户线程可以重新与其他KSE建立关联关系。当然这种动态关联机制的实现很复杂，也需要用户自己去实现，这算是它的一个缺点吧。Go语言中的并发就是使用的这种实现方式，Go为了实现该模型自己实现了一个运行时调度器来负责Go中的”线程”与KSE的动态关联。此模型有时也被称为 两级线程模型，即用户调度器实现用户线程到KSE的“调度”，内核调度器实现KSE到CPU上的调度。 协程协程是用户态的线程(go 的协程属于混合型线程)。通常创建协程时，会从进程的堆中分配一段内存作为协程的栈。线程的栈有 8 MB，而协程栈的大小通常只有 KB，而 Go 语言的协程更夸张，只有 2-4KB，非常的轻巧。 协程的优势如下： 节省 CPU：避免系统内核级的线程频繁切换，造成的 CPU 资源浪费。好钢用在刀刃上。而协程是用户态的线程，用户可以自行控制协程的创建于销毁，极大程度避免了系统级线程上下文切换造成的资源浪费。 节约内存：在 64 位的Linux中，一个线程需要分配 8MB 栈内存和 64MB 堆内存，系统内存的制约导致我们无法开启更多线程实现高并发。而在协程编程模式下，可以轻松有十几万协程，这是线程无法比拟的。 稳定性：前面提到线程之间通过内存来共享数据，这也导致了一个问题，任何一个线程出错时，进程中的所有线程都会跟着一起崩溃。 开发效率：使用协程在开发程序之中，可以很方便的将一些耗时的IO操作异步化，例如写文件、耗时 IO 请求等。 孤儿进程，僵尸进程，守护进程，init进程init进程进程号为1的进程称为init进程，init进程的好处就是SIGKILL信号对它无效。 孤儿进程父进程退出了，但是他的子进程还在运行，这种情况下子进程将变成孤儿进程。孤儿进程将被init进程(进程号为1)接管，并由init进程对它完成状态收集(wait/waitpid)工作。由于孤儿进程会被init进程给收养，所以孤儿进程不会对系统造成危害 僵尸进程一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。父进程认为子进程还存活着，这种进程称之为僵死进程。 守护进程守护进程是运行在后台的一种特殊进程。它独立于控制终端并且周期性地执行某种任务或等待处理某些发生的事件。它不需要用户输入就能运行而且提供某种服务，不是对整个系统就是对某个用户程序提供服务。 systemd进程提到 systemd 不得不提一下 Linux 的启动流程，这样才能清楚 systemd 在 Linux 系统中的地位和作用 linux 启动流程 加电自检(检查硬件是否有问题) GRUB引导 内核加载 init 初始化(内核启动第一个用户空间应用程序，即 systemd 进程， 通过dmesg能查看) systemd简介systemd 是一个 Linux 系统基础组件的集合，提供了一个系统和服务管理器，运行为 PID 1 并负责启动其它程序, 所有的进程都会被挂在这个进程下，如果这个进程退出了，那么所有的进程都被 kill 。 systemd功能包括： 支持并行化任务； 同时采用 socket 式与 D-Bus 总线式激活服务； 按需启动守护进程（daemon）； 利用 Linux 的 cgroups 监视进程； 支持快照和系统恢复； 维护挂载点和自动挂载点； 各服务间基于依赖关系进行精密控制。 systemd* 支持 SysV 和 LSB 初始脚本，可以替代 sysvinit。 除此之外，功能还包括日志进程、控制基础系统配置，维护登陆用户列表以及系统账户、运行时目录和设置，可以运行容器和虚拟机，可以简单的管理网络配置、网络时间同步、日志转发和名称解析等。 通过pstree 能够查看进程数状态，用户空间的进程都挂在 PID 为 1 的 systemd 进程下。(似乎systemd进程无法被杀死，kill -9 1似乎无效！) systemd 体系架构 最底层：systemd 内核层面依赖 cgroup、autofs、kdbus 第二层：systemd libraries 是 systemd 依赖库 第三层：systemd Core 是 systemd 自己的库 第四层：systemd daemons 以及 targets 是自带的一些基本 unit、target，类似于 sysvinit 中自带的脚本 最上层就是和 systemd 交互的一些工具其中： systemctl 命令控制 systemd 的管理系统和服务的命令行工具 journalctl 命令查詢 systemd 日志系统 loginctl 命令控制 systemd 登入管理器 systemd-analyze 分析系统启动效能 systemd软连接当我们使用 reboot 、poweroff 、shutdown 等命令的时候，其实并不是执行该命令本身，背后是调用的 systemctl 命令。systemctl 命令会将 reboot 这些命令作为 $1 参数传递进去。所以执行 reboot 和 systemctl reboot 本质上是一样的。 参考文档 https://segmentfault.com/a/1190000040373756 // 煎鱼的博客 https://segmentfault.com/a/1190000039378412 // 程序员阿星 Linux 的小伙伴 systemd 详解 //木子的博客 http://shareinto.k8s.101.com/2019/01/30/docker-init(1)/ // shareinto LINUX PID 1 和 SYSTEMD //酷壳","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"golang io.Reader/io.Writer","slug":"golang-io","date":"2021-02-21T06:14:24.000Z","updated":"2021-12-17T07:42:48.326Z","comments":true,"path":"2021/02/21/golang-io/","link":"","permalink":"https://github.com/fafucoder/2021/02/21/golang-io/","excerpt":"","text":"概述在使用Go语言的过程中，无论是实现web应用程序，还是控制台输入输出，又或者是网络操作，不可避免的会遇到IO操作，使用到io.Reader和io.Writer接口。 io.Readerio.Reader 表示一个读取器，它将数据从某个资源读取到传输缓冲区。在缓冲区中，数据可以被流式传输和使用。 io.Reader接口定义如下： 123type Reader interface &#123; Read(p []byte) (n int, err error)&#125; io.Reader接口定义了Read(p []byte) (n int, err error)方法，我们可以使用它从Reader中读取一批数据。在Reader中： 一次最多读取len(p)长度的数据 读取遭遇到error(io.EOF或者其它错误), 会返回已读取的数据的字节数和error 即使读取字节数&lt; len(p),也不会更改p的大小 当输入流结束时，调用它可能返回 err == EOF 或者 err == nil，并且n &gt;=0, 但是下一次调用肯定返回 n=0, err=io.EOF io.Writerio.Writer 表示一个编写器，它从缓冲区读取数据，并将数据写入目标资源。 io.Writer接口定义如下： 123type Writer interface &#123; Write(p []byte) (n int, err error)&#125; Write() 方法有两个返回值，一个是写入到目标资源的字节数，一个是发生错误时的错误。 接口继承关系围绕着io.Reader io.Writer接口的实现，主要有： net.Conn, os.Stdin, os.File: 网络、标准输入输出、文件的流读取 strings.Reader: 把字符串抽象成Reader bytes.Reader: 把[]byte抽象成Reader bytes.Buffer: 把[]byte抽象成Reader和Writer bufio.Reader/Writer: 抽象成带缓冲的流读取（比如按行读写） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132func stringReader() &#123; reader := strings.NewReader(\"hello world\") b := make([]byte, 4) for &#123; n, err := reader.Read(b) if err != nil &#123; if err == io.EOF &#123; fmt.Println(\"finish read\") break &#125; fmt.Println(err) os.Exit(1) &#125; fmt.Println(n, string(b[:n])) &#125;&#125;func bufferWriter() &#123; providers := []string&#123; \"hello\", \"world\", \"golang\", \"is great\", &#125; var writer bytes.Buffer for _, s := range providers &#123; n, err := writer.Write([]byte(s)) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; if n != len(s) &#123; fmt.Println(\"failed to write data\") os.Exit(1) &#125; &#125; fmt.Println(writer.String())&#125;func fileWriter() &#123; file, err := os.Create(\"./files.txt\") if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; defer file.Close() providers := []string&#123; \"hello\", \"world\", \"golang\", \"is great\", &#125; for _, p := range providers &#123; c, err := file.Write([]byte(p)) if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; if c != len(p) &#123; fmt.Println(\"failed to write data\") os.Exit(1) &#125; &#125;&#125;//file readerfunc fileReader() &#123; file, err := os.Open(\"./files.txt\") if err != nil &#123; fmt.Println(err) os.Exit(1) &#125; defer file.Close() p := make([]byte, 4) for &#123; n, err := file.Read(p) if err != nil &#123; if err == io.EOF &#123; break &#125; fmt.Println(err) &#125; fmt.Println(string(p[:n])) &#125;&#125;//buffiofunc bufReader() &#123; file, err := os.Open(\"./files.txt\") if err != nil &#123; panic(err) &#125; defer file.Close() p := make([]byte, 4) reader := bufio.NewReader(file) for &#123; n, err := reader.Read(p) if err != nil &#123; if err == io.EOF &#123; break &#125; panic(err) &#125; fmt.Println(string(p[:n])) &#125;&#125;//ioutil 包func ioutilReader() &#123; bytes, err := ioutil.ReadFile(\"./files.txt\") if err != nil &#123; panic(err) &#125; fmt.Println(string(bytes))&#125; 参考文档 Go中io包的使用方法 Go编程技巧–io.Reader/Writer 从 io.Reader 中读数据","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"golang-context","slug":"golang-context","date":"2021-02-08T02:18:50.000Z","updated":"2021-12-17T07:42:48.325Z","comments":true,"path":"2021/02/08/golang-context/","link":"","permalink":"https://github.com/fafucoder/2021/02/08/golang-context/","excerpt":"","text":"context 作用context用来解决goroutine 之间传递上下文信息，包括：取消信号、超时时间、截止时间、k-v 等。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package mainimport ( \"context\" \"fmt\" \"time\")func main() &#123; ctx, cel := context.WithCancel(context.Background()) go pushDockerToHarbor(ctx) go pushImageToMinio(ctx) time.Sleep(6 * time.Second) cel() time.Sleep(5 * time.Second)&#125;func pushImageToMinio(ctx context.Context) &#123; go func() &#123; fmt.Println(\"i doing push image to minio worker\") time.Sleep(5 * time.Second) &#125;() for &#123; select &#123; case &lt;-ctx.Done(): fmt.Println(\"canceled\") return case &lt;-time.After(30 * time.Minute): fmt.Println(\"timeout\") return &#125; &#125;&#125;func pushDockerToHarbor(ctx context.Context) &#123; go func() &#123; fmt.Println(\"i doing push docker to harbor worker\") time.Sleep(5 * time.Second) &#125;() for &#123; select &#123; case &lt;-ctx.Done(): fmt.Println(\"canceled\") return case &lt;-time.After(30 * time.Minute): fmt.Println(\"timeout\") return &#125; &#125;&#125; 参考文档 深度解密Go语言之context","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"linux 虚拟网卡解析","slug":"linux-dummy","date":"2021-02-03T11:11:18.000Z","updated":"2021-12-17T07:42:48.341Z","comments":true,"path":"2021/02/03/linux-dummy/","link":"","permalink":"https://github.com/fafucoder/2021/02/03/linux-dummy/","excerpt":"","text":"虚拟网卡解析通过 ip link 命令可以创建多种类型的虚拟网络设备，在 man ip link中可以得知有以下类型的device: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051bridge - Ethernet Bridge devicebond - Bonding device can - Controller Area Network interfacedummy - Dummy network interfacehsr - High-availability Seamless Redundancy deviceifb - Intermediate Functional Block deviceipoib - IP over Infiniband devicemacvlan - Virtual interface base on link layer address (MAC)macvtap - Virtual interface based on link layer address (MAC) and TAP.vcan - Virtual Controller Area Network interfaceveth - Virtual ethernet interfacevlan - 802.1q tagged virtual LAN interfacevxlan - Virtual eXtended LANip6tnl - Virtual tunnel interface IPv4|IPv6 over IPv6ipip - Virtual tunnel interface IPv4 over IPv4sit - Virtual tunnel interface IPv6 over IPv4gre - Virtual tunnel interface GRE over IPv4gretap - Virtual L2 tunnel interface GRE over IPv4ip6gre - Virtual tunnel interface GRE over IPv6ip6gretap - Virtual L2 tunnel interface GRE over IPv6vti - Virtual tunnel interfacenlmon - Netlink monitoring deviceipvlan - Interface for L3 (IPv6/IPv4) based VLANslowpan - Interface for 6LoWPAN (IPv6) over IEEE 802.15.4 / Bluetoothgeneve - GEneric NEtwork Virtualization Encapsulationmacsec - Interface for IEEE 802.1AE MAC Security (MACsec)vrf - Interface for L3 VRF domains 在kubernetes容器网络虚拟化中，常用的网卡类型包括macvlan, ipvlan, veth, ipip, vlan, macvtap, bridge等。通过 ip link add命令能够快速的创建一个虚拟网卡。 1ip link add ethx type dummy macvlanmacvlan 本身是 linxu kernel 模块，其功能是允许在同一个物理网卡上配置多个 MAC 地址，即多个 interface，每个 interface 可以配置自己的 IP。macvlan 本质上是一种网卡虚拟化技术(最大优点是性能极好) 可以在linux命令行执行lsmod | grep macvlan 查看当前内核是否加载了该driver；如果没有查看到，可以通过modprobe macvlan来载入，然后重新查看。内核代码路径/drivers/net/macvlan.c Macvlan 允许你在主机的一个网络接口上配置多个虚拟的网络接口，这些网络 interface 有自己独立的 MAC 地址，也可以配置上 IP 地址进行通信。Macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。Macvlan 和 Bridge 比较相似，但因为它省去了 Bridge 的存在，所以配置和调试起来比较简单，而且效率也相对高。除此之外，Macvlan 自身也完美支持 VLAN。 macvlan的工作方式如下： 在macvlan中，实体网卡称为父接口(parent interface), 创建出来的虚拟网卡称为子接口(sub interface)，其中子接口无法与父接口通讯 (带有子接口 的 VM 或容器无法与 host 直接通讯, 这是因为在macvlan模式设计的时候为了安全而禁止了宿主机和容器直接通信)，如果vm或者容器需要与host通讯，就必须额外建立一个 sub interface给 host 用。 macvlan 模式macvlan支持三种模式，bridge、vepa、private，在创建的时候设置mode XXX。 Bridge模式：属于同一个parent接口的macvlan接口之间挂到同一个bridge上，可以二层互通（macvlan接口都无法与parent 接口互通）。 VPEA模式：所有接口的流量都需要到外部switch才能够到达其他接口。 Private模式：接口只接受发送给自己MAC地址的报文。 创建macvlan123456789101112131415161718192021222324252627# 创建macvlan 网卡ip link add link eth0 name mac1@eth0 type macvlan mode bridgeip link add link eth0 name mac2@eth0 type macvlan mode bridge# 创建命名空间ip netns add ns1ip netns add ns2# 把虚拟网卡移入到命名空间ip link set mac1@eth0 netns ns1ip link set mac2@eth0 netns ns2# 进入netns为虚拟网卡分配ip地址ip netns exec ns1 ip addr add 192.168.0.100/24 dev mac1@eth0ip netns exec ns2 ip addr add 192.168.0.200/24 dev mac2@eth0# 设置虚拟网卡状态为upip netns exec ns1 ip link set mac1@eth0 upip netns exec ns2 ip link set mac2@eth0 up# 创建macvlan网卡，不移入到命名空间，并且添加路由ip link add link eth0 name mac3@eth0 type macvlan mode bridgeip addr add 192.168.0.150/24 dev mac3@eth0ip link set mac3@eth0 upip route add 192.168.0.100/32 dev mac3@eth0ip route add 192.168.0.200/32 dev mac3@eth0 验证macvlan12345678910111213141516171819202122232425262728293031323334353637383940414243# 查看ip地址[root@master ~]# ip netns exec ns2 ifconfigmac2@eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 192.168.0.200 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::1860:edff:fecd:43c1 prefixlen 64 scopeid 0x20&lt;link&gt; ether 1a:60:ed:cd:43:c1 txqueuelen 1000 (Ethernet) RX packets 5 bytes 322 (322.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 13 bytes 1006 (1006.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0# ping ns1中的网卡[root@master ~]# ip netns exec ns2 ping -c 5 192.168.0.100PING 192.168.0.100 (192.168.0.100) 56(84) bytes of data.64 bytes from 192.168.0.100: icmp_seq=1 ttl=64 time=0.090 ms64 bytes from 192.168.0.100: icmp_seq=2 ttl=64 time=0.044 ms64 bytes from 192.168.0.100: icmp_seq=3 ttl=64 time=0.052 ms64 bytes from 192.168.0.100: icmp_seq=4 ttl=64 time=0.080 ms64 bytes from 192.168.0.100: icmp_seq=5 ttl=64 time=0.053 ms--- 192.168.0.100 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4088msrtt min/avg/max/mdev = 0.044/0.063/0.090/0.020 ms# 无法ping通宿主机[root@master ~]# ip netns exec ns2 ping 192.168.0.171PING 192.168.0.171 (192.168.0.171) 56(84) bytes of data.^X^C--- 192.168.0.171 ping statistics ---4 packets transmitted, 0 received, 100% packet loss, time 3071ms# 能够ping通宿主机上的mac3@eth0虚拟网卡[root@master ~]# ip netns exec ns2 ping -c 5 192.168.0.150PING 192.168.0.150 (192.168.0.150) 56(84) bytes of data.64 bytes from 192.168.0.150: icmp_seq=1 ttl=64 time=0.063 ms64 bytes from 192.168.0.150: icmp_seq=2 ttl=64 time=0.071 ms64 bytes from 192.168.0.150: icmp_seq=3 ttl=64 time=0.062 ms64 bytes from 192.168.0.150: icmp_seq=4 ttl=64 time=0.079 ms64 bytes from 192.168.0.150: icmp_seq=5 ttl=64 time=0.090 ms--- 192.168.0.150 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4104msrtt min/avg/max/mdev = 0.062/0.073/0.090/0.010 ms macvlan 环境清理123ip netns del ns1ip netns del ns2ip link del mac3@eth0 ipvlanIPVlan 和 macvlan 类似，都是从一个主机接口虚拟出多个虚拟网络接口。一个重要的区别就是所有的虚拟接口都有相同的 macv 地址，而拥有不同的 ip 地址。因为所有的虚拟接口要共享 mac 地址，所以有些需要注意的地方： DHCP 协议分配 ip 的时候一般会用 mac 地址作为机器的标识。这个情况下，客户端动态获取 ip 的时候需要配置唯一的 ClientID 字段，并且 DHCP server 也要正确配置使用该字段作为机器标识，而不是使用 mac 地址 Ipvlan 是 linux kernel 比较新的特性，linux kernel 3.19 开始支持 ipvlan，但是比较稳定推荐的版本是 &gt;=4.2 ipvlan模式L2模式：ipvlan L2 模式和 macvlan bridge 模式工作原理很相似，父接口作为交换机来转发子接口的数据。同一个网络的子接口可以通过父接口来转发数据，而如果想发送到其他网络，报文则会通过父接口的路由转发出去。 L3模式： ipvlan 有点像路由器的功能，它在各个虚拟网络和主机网络之间进行不同网络报文的路由转发工作。只要父接口相同，即使虚拟机/容器不在同一个网络，也可以互相 ping 通对方，因为 ipvlan 会在中间做报文的转发工作。 创建ipvlan12345678910111213141516171819202122232425262728# 创建 ipvlan 虚拟网卡ip link add link eth0 ipvlan1@eth0 type ipvlan mode l3ip link add link eth0 ipvlan2@eth0 type ipvlan mode l3# 创建命名空间ip netns add ns1ip netns add ns2# 把ipvlan 移入到命名空间ip link set ipvlan1@eth0 netns ns1ip link set ipvlan2@eth0 netns ns2# 配置虚拟网卡ip地址ip netns exec ns1 ip addr add 192.168.0.100/24 dev ipvlan1@eth0ip netns exec ns2 ip addr add 192.168.0.200/24 dev ipvlan2@eth0# 设置虚拟网卡为up状态ip netns exec ns1 ip link set ipvlan1@eth0 upip netns exec ns2 ip link set ipvlan2@eth0 up# 创建虚拟网卡，放置在宿主机ip link add link eth0 ipvlan3@eth0 type ipvlan mode l3ip addr add 192.168.0.150/24 dev ipvlan3@eth0ip link set ipvlan3@eth0 up# 添加路由ip route add 192.168.0.100/32 dev ipvlan3@eth0ip route add 192.168.0.200/32 dev ipvlan3@eth0 验证ipvlan1234567891011121314151617181920212223242526272829303132333435363738394041424344# 查看ip地址[root@master ~]# ip netns exec ns1 ifconfigipvlan1@eth0: flags=4291&lt;UP,BROADCAST,RUNNING,NOARP,MULTICAST&gt; mtu 1500 inet 192.168.0.100 netmask 255.255.255.0 broadcast 0.0.0.0 inet6 fe80::16:3e00:10d:30c7 prefixlen 64 scopeid 0x20&lt;link&gt; ether 00:16:3e:0d:30:c7 txqueuelen 1000 (Ethernet) RX packets 3 bytes 336 (336.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 784 (784.0 B) TX errors 0 dropped 5 overruns 0 carrier 0 collisions 0# 能ping通 192.168.0.200[root@master ~]# ip netns exec ns1 ping -c 5 192.168.0.200PING 192.168.0.200 (192.168.0.200) 56(84) bytes of data.64 bytes from 192.168.0.200: icmp_seq=1 ttl=64 time=0.056 ms64 bytes from 192.168.0.200: icmp_seq=2 ttl=64 time=0.055 ms64 bytes from 192.168.0.200: icmp_seq=3 ttl=64 time=0.045 ms64 bytes from 192.168.0.200: icmp_seq=4 ttl=64 time=0.046 ms64 bytes from 192.168.0.200: icmp_seq=5 ttl=64 time=0.055 ms--- 192.168.0.200 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4126msrtt min/avg/max/mdev = 0.045/0.051/0.056/0.008 ms# 无法ping通宿主机[root@master ~]# ip netns exec ns1 ping -c 5 192.168.0.171PING 192.168.0.171 (192.168.0.171) 56(84) bytes of data.--- 192.168.0.171 ping statistics ---5 packets transmitted, 0 received, 100% packet loss, time 4080ms# 能ping通宿主机上的ipvlan3@eth0[root@master ~]# ip netns exec ns1 ping -c 5 192.168.0.150PING 192.168.0.150 (192.168.0.150) 56(84) bytes of data.64 bytes from 192.168.0.150: icmp_seq=1 ttl=64 time=0.100 ms64 bytes from 192.168.0.150: icmp_seq=2 ttl=64 time=0.088 ms64 bytes from 192.168.0.150: icmp_seq=3 ttl=64 time=0.067 ms64 bytes from 192.168.0.150: icmp_seq=4 ttl=64 time=0.091 ms64 bytes from 192.168.0.150: icmp_seq=5 ttl=64 time=0.088 ms--- 192.168.0.150 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4122msrtt min/avg/max/mdev = 0.067/0.086/0.100/0.016 ms ipvlan 环境清理123ip netns del ns1ip netns del ns2ip link del ipvlan3@eth0 macvtapMACVTAP 是对 MACVLAN的改进，把 MACVLAN 与 TAP 设备的特点综合一下，使用 MACVLAN 的方式收发数据包，但是收到的包不交给 network stack 处理，而是生成一个 /dev/tapX 文件，交给这个文件。由于 MACVLAN 是工作在 MAC 层的，所以 MACVTAP 也只能工作在 MAC 层。 tunTun是Linux系统里的虚拟网络设备, TUN设备模拟网络层设备(network layer)，处理三层报文，IP报文等，用于将报文注入到网络协议栈 应用程序(app)可以从物理网卡上读写报文，经过处理后通过TUN回送，或者从TUN读取报文处理后经物理网卡送出。 dummydummy网卡是内核虚拟出来的网卡，没有实际的用途，要创建一个dummy类型的网卡只需要通过如下命令: 12345678[root@master ~]# ip link add mydummy type dummy[root@master ~]# ifconfig dummydummy0: flags=130&lt;BROADCAST,NOARP&gt; mtu 1500 ether 12:cf:8a:87:81:ed txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 创建dummy网卡后可以为dummy网卡设置ip地址： 123456789[root@master ~]# ip addr add 192.168.200.200/24 dev mydummy[root@master ~]# ifconfig mydummymydummy: flags=130&lt;BROADCAST,NOARP&gt; mtu 1500 inet 192.168.200.200 netmask 255.255.255.0 broadcast 0.0.0.0 ether ae:de:93:50:84:2f txqueuelen 1000 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 参考文档 macvlan网络模式下容器与宿主机互通 macvlan和ipvlan Linux上的物理网卡与虚拟网络设备","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"哈希表： 散列查找","slug":"datastruct-hash","date":"2021-02-02T02:55:33.000Z","updated":"2021-12-17T07:42:48.323Z","comments":true,"path":"2021/02/02/datastruct-hash/","link":"","permalink":"https://github.com/fafucoder/2021/02/02/datastruct-hash/","excerpt":"","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167package hash_mapimport ( \"github.com/OneOfOne/xxhash\" \"math\" \"sync\")const expandFactor = 0.75type hashTable struct &#123; items []*hashMap len int capacity int capacityMask int lock sync.Mutex&#125;type hashMap struct &#123; key string value interface&#123;&#125; next *hashMap&#125;func NewHashTable(capacity int) *hashTable &#123; defaultCapacity := 1 &lt;&lt; 4 if capacity &lt;= defaultCapacity &#123; capacity = defaultCapacity &#125; else &#123; capacity = 1 &lt;&lt; int(math.Ceil(math.Log2(float64(capacity)))) &#125; hTable := new(hashTable) hMap := make([]*hashMap, capacity, capacity) hTable.items = hMap hTable.capacity = capacity hTable.capacityMask = capacity - 1 return hTable&#125;func (c *hashTable) hashIndex(key string) int &#123; hash := xxHash([]byte(key)) return int(hash &amp; uint64(c.capacityMask))&#125;func (c *hashTable) Add(key string, value interface&#123;&#125;) &#123; c.lock.Lock() defer c.lock.Unlock() index := c.hashIndex(key) element := c.items[index] if element == nil &#123; c.items[index] = &amp;hashMap&#123; key: key, value: value, &#125; &#125; else &#123; var lastElement *hashMap for element != nil &#123; if element.key == key &#123; element.value = value return &#125; lastElement = element element = element.next &#125; lastElement.next = &amp;hashMap&#123; key: key, value: value, &#125; newLen := c.len + 1 if float64(newLen)/float64(c.capacity) &gt;= expandFactor &#123; newHashMap := new(hashTable) newHashMap.items = make([]*hashMap, 2*c.capacity, 2*c.capacity) newHashMap.capacity = 2 * c.capacity newHashMap.capacityMask = 2*c.capacity - 1 for _, item := range c.items &#123; for item != nil &#123; newHashMap.Add(item.key, item.value) item = item.next &#125; &#125; c.items = newHashMap.items c.capacity = newHashMap.capacity c.capacityMask = newHashMap.capacityMask &#125; c.len = newLen &#125;&#125;func (c *hashTable) Get(key string) interface&#123;&#125; &#123; c.lock.Lock() defer c.lock.Unlock() index := c.hashIndex(key) element := c.items[index] for element != nil &#123; if element.key == key &#123; return element.value &#125; element = element.next &#125; return nil&#125;func (c *hashTable) Delete(key string) &#123; c.lock.Lock() defer c.lock.Unlock() index := c.hashIndex(key) element := c.items[index] if element == nil &#123; return &#125; if element.key == key &#123; c.items[index] = element.next c.len = c.len - 1 return &#125; nextElement := element.next for nextElement != nil &#123; if nextElement.key == key &#123; element.next = nextElement.next c.len = c.len - 1 return &#125; element = nextElement nextElement = nextElement.next &#125;&#125;func (c *hashTable) Range() map[string]interface&#123;&#125; &#123; c.lock.Lock() defer c.lock.Unlock() hashMaps := make(map[string]interface&#123;&#125;, c.len) for _, item := range c.items &#123; for item != nil &#123; hashMaps[item.key] = item.value item = item.next &#125; &#125; return hashMaps&#125;func xxHash(key []byte) uint64 &#123; h := xxhash.New64() h.Write(key) return h.Sum64()&#125; 参考文档 https://goa.lenggirl.com/algorithm/search/hash_find.html","categories":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://github.com/fafucoder/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://github.com/fafucoder/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"}]},{"title":"kubernetes api聚合机制aggregation","slug":"kubernetes-aggregator","date":"2021-01-29T11:11:37.000Z","updated":"2021-12-17T07:42:48.333Z","comments":true,"path":"2021/01/29/kubernetes-aggregator/","link":"","permalink":"https://github.com/fafucoder/2021/01/29/kubernetes-aggregator/","excerpt":"","text":"概念​ kubernetes的 Aggregated API是什么呢？API Aggregation 允许k8s的开发人员编写一个自己的服务，可以把这个服务注册到k8s的api里面，这样，就像k8s自己的api一样，你的服务只要运行在k8s集群里面，k8s 的Aggregate通过service名称就可以转发到你写的service里面去了。(另外一种扩展 Kubernetes API 的方法是使用CustomResourceDefinition, 编写的自定义CRD, k8s会自动的注册到apiservice上 ) 这个设计理念： 增加了api的扩展性，这样k8s的开发人员就可以编写自己的API服务器来公开他们想要的API。集群管理员应该能够使用这些服务，而不需要对核心库存储库进行任何更改。 丰富了APIs，核心kubernetes团队阻止了很多新的API提案。通过允许开发人员将他们的API作为单独的服务器公开，并使集群管理员能够在不对核心库存储库进行任何更改的情况下使用它们，这样就无须社区繁杂的审查了 开发分阶段实验性API的地方，新的API可以在单独的聚集服务器中开发，当它稳定之后，那么把它们封装起来安装到其他集群就很容易了。 确保新API遵循kubernetes约定：如果没有这里提出的机制，社区成员可能会被迫推出自己的东西，这可能会或可能不遵循kubernetes约定。 原理​ 当APIAggregator接收到请求之后，如果发现对应的是一个service的请求，则会直接转发到对应的服务上否则委托给apiserver进行处理，apiserver中根据当前URL来选择对应的REST接口处理，如果未能找到对应的处理，则会交由CRD server处理， CRD server检测是否已经注册对应的CRD资源，如果注册就处理 ​ 当在集群中创建了对应的CRD资源的时候，k8s通过内部的controller来感知对应的CRD资源信息，然后为其创建对应的REST处理接口，这样后续接收到对应的资源就可以进行处理 ​ APIAggreagtor中会通过informer 监听后端Service的变化，如果发现有新的服务，就会创建对应的代理转发，从而实现对应的服务注册 注册自定义service资源12345678910111213apiVersion: apiregistration.k8s.io/v1beta1kind: APIServicemetadata: name: v1beta1.custom.metrics.k8s.iospec: service: name: custom-metrics-server namespace: custom-metrics group: custom.metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 在这个APIService中设置的API组名为custom.metrics.k8s.io，版本号为v1beta1，这两个字段将作API路径的子目录注册到API路径“/apis/”下。注册成功后，就能通过Master API路径“/apis/custom.metrics.k8s.io/v1beta1”访问自定义的API Server了, API聚合层会代理转发到后端服务custom-metrics-server.custom-metrics.svc上。 官方提供有一个 自定义api service 样例, 通过官方样例可以创建一个自定义聚合api 注意点 service监听的端口必须是https,否则认证通过不了。(通过kubectl get apiservice能看到是否通过认证) 请求的根路径需要应答请求，否则会报404错误（kubectl get apiservice v1alpha3.demo.com.cn -o yaml 能拷看到状态） 请求的根路径的应答结果需要包含kind, apiVersion, groupVersion, resources： 123456&#123;\"kind\": \"APIResourceList\",\"apiVersion\": \"v1\",\"groupVersion\": \"metrics.k8s.io/v1beta1\",\"resources\": []&#125; 参考文档 图解kubernetes中api聚合机制的实现 解析kubernetes Aggregated API Servers Kubernetes之使用API聚合机制扩展API资源 K8s Aggregation API体验","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"golang-unsafe","slug":"golang-unsafe","date":"2021-01-27T15:06:16.000Z","updated":"2021-12-17T07:42:48.328Z","comments":true,"path":"2021/01/27/golang-unsafe/","link":"","permalink":"https://github.com/fafucoder/2021/01/27/golang-unsafe/","excerpt":"","text":"参考文档 https://mp.weixin.qq.com/s/p4ik3GsM3uGb6yvgVVxMdQ https://www.qcrao.com/2019/06/03/dive-into-go-unsafe https://studygolang.com/articles/27244?fr=sidebar","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"golang map数据结构","slug":"golang-map","date":"2021-01-27T06:02:33.000Z","updated":"2021-12-17T07:42:48.327Z","comments":true,"path":"2021/01/27/golang-map/","link":"","permalink":"https://github.com/fafucoder/2021/01/27/golang-map/","excerpt":"","text":"简述在Go语言中，通过hash查找表实现map, 用链表发解决哈希冲突问题。通过key的哈希值将散落到不同的桶（bucket)中，每个桶有8个槽位（cell)。哈希值的低位决定key落入哪个桶，高位标识同一个桶中的不同 key。 map数据结构​ map的底层结构是hmap,他是hashmap的缩写 1234567891011121314151617181920// A header for a Go map.type hmap struct &#123; // 元素个数，调用 len(map) 时，直接返回此值 count int flags uint8 // buckets 的对数 log_2 B uint8 // overflow 的 bucket 近似数 noverflow uint16 // 计算 key 的哈希的时候会传入哈希函数 hash0 uint32 // 指向 buckets 数组，大小为 2^B, 如果元素个数为0，就为 nil buckets unsafe.Pointer // 扩容的时候，buckets 长度会是 oldbuckets 的两倍 oldbuckets unsafe.Pointer // 指示扩容进度，小于此地址的 buckets 迁移完成 nevacuate uintptr // optional fields extra *mapextra&#125; B 是 buckets 数组的长度的对数，也就是说 buckets 数组的长度就是 2^B， 相当于截断hash二进制后保留后面的B位。 buckets是一个指针（unsafe.Pointer —&gt; 指针类型)，最终指向的是一个结构体,在编译期间会增加几个字段，最终字段如下： 1234567891011type bmap struct &#123; tophash [bucketCnt]uint8&#125;type bmap struct &#123; topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr&#125; bmap 就是我们常说的“桶”，桶里面会最多装 8 个 key，这些 key 之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的。在桶内，又会根据 key 计算出来的 hash 值的高 8 位来决定 key 到底落入桶内的哪个位置（一个桶内最多有8个位置）。 bmap是存放k-v的地方，key跟value放在一块，目的是节省内存空间，结构如下： map整体数据结构如下： map key定位​ 对map的添加更改主要涉及到key的定位过程，key经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32。如下图，首先计算出待查找 key 的哈希，使用低 5 位 00110，找到对应的 6 号 bucket，使用高 8 位 10010111，对应十进制 151，在 6 号 bucket 中寻找 tophash 值（HOB hash）为 151 的 key，找到了 2 号槽位，这样整个查找过程就结束了。如果在 bucket 中没找到，并且 overflow 不为空，还要继续去 overflow bucket 中寻找，直到找到或是所有的 key 槽位都找遍了，包括所有的 overflow bucket。 map存值​ 首先用 key 的 hash 值低 8 位找到 bucket，然后在 bucket 内部比对 tophash 和高 8 位与其对应的 key 值与入参 key 是否相等，若找到则更新这个值。若 key 不存在，则 key 优先存入在查找的过程中遇到的空的 tophash 数组位置。若当前的 bucket 已满则需要另外分配空间给这个 key，新分配的 bucket 将挂在 overflow 链表后。 map扩容触发 map 扩容的时机：在向 map 插入新 key 的时候，会进行条件检测，符合下面这 2 个条件，就会触发扩容： 装载因子超过阈值，源码里定义的阈值是 6.5 (也就是每个buket都要装满了，总共8个) overflow 的 bucket 数量过多：当 B 小于 15，也就是 bucket 总数 2^B 小于 2^15 时，如果 overflow 的 bucket 数量超过 2^B；当 B &gt;= 15，也就是 bucket 总数 2^B 大于等于 2^15，如果 overflow 的 bucket 数量超过 2^15。(也就是某个bucket链太长了，而其他的bucket可能没有分配数据) 扩容分为等量扩容和 2 倍容量扩容。扩容后，原来一个 bucket 中的 key 一分为二，会被重新分配到两个桶中。扩容过程是渐进的，主要是防止一次扩容需要搬迁的 key 数量过多，引发性能问题。触发扩容的时机是增加了新元素，bucket 搬迁的时机则发生在赋值、删除期间，每次最多搬迁两个 bucket。 在遍历 map 时，并不是固定地从 0 号 bucket 开始遍历，每次都是从一个随机值序号的 bucket 开始遍历，并且是从这个 bucket 的一个随机序号的 cell 开始遍历。这样，即使是一个写死的 map，仅仅只是遍历它，也不太可能会返回一个固定序列的 key/value 对。 map删除12345678910111213// 对 key 清零if t.indirectkey &#123; *(*unsafe.Pointer)(k) = nil&#125; else &#123; typedmemclr(t.key, k)&#125;// 对 value 清零if t.indirectvalue &#123; *(*unsafe.Pointer)(v) = nil&#125; else &#123; typedmemclr(t.elem, v)&#125; 注意点 map 并不是一个线程安全的数据结构。同时读写一个 map 是未定义的行为，如果被检测到，会直接 panic。 ap删除操作仅仅将对应的 tophash[i]设置为 empty，并非释放内存。若要释放内存只能等待指针无引用后被系统 gc key必须是支持==或!=比较运算的类型，不可以是函数、map或slice(Go 语言中只要是可比较的类型都可以作为 key。除slice，map，functions 这几种类型，其他类型都是 OK 的。具体包括：布尔值、数字、字符串、指针、通道、接口类型、结构体、只包含上述类型的数组。) 参考文档 https://blog.csdn.net/wade3015/article/details/100149338 https://www.qcrao.com/2019/05/22/dive-into-go-map //来源于这篇文章 https://draveness.me/golang/docs/part2-foundation/ch03-datastructure/golang-hashmap/","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"golang channel解析","slug":"golang-channel","date":"2021-01-16T14:55:20.000Z","updated":"2021-12-17T07:42:48.325Z","comments":true,"path":"2021/01/16/golang-channel/","link":"","permalink":"https://github.com/fafucoder/2021/01/16/golang-channel/","excerpt":"","text":"channel的基本概念1. channel的基本概念channel，通道。golang中用于数据传递的一种数据结构， 是golang中一种传递数据的方式，常用于goroutine之间的通信。 像管道一样，一个goroutineA向channelA中放数据，另外一个goroutineB从channelA中取数据。 2. channel的申明、传值、关闭channel是指针类型的数据类型，可通过make来分配内存。例如：ch := make(chan int)，这表示创建一个channel，这个channel中只能保存int类型的数据。也就是说一端只能向此channel中放进int类型的值，另一端只能从此channel中读出int类型的值。 使用chan关键字声明一个通道，在使用前必须先创建，操作符 &lt;- 用于指定通道的方向，发送或接收。如果未指定方向，则为双向通道。 123456789101112131415161718//声明和创建var ch chan int // 声明一个传递int类型的channelch := make(chan int) // 使用内置函数make()定义一个channelch2 := make(chan interface&#123;&#125;) // 创建一个空接口类型的通道, 可以存放任意格式type Equip struct&#123; /* 一些字段 */ &#125;ch2 := make(chan *Equip) // 创建Equip指针类型的通道, 可以存放*Equip//传值ch &lt;- value // 将一个数据value写入至channel，这会导致阻塞，直到有其他goroutine从这个channel中读取数据value := &lt;-ch // 从channel中读取数据，如果channel之前没有写入数据，也会导致阻塞，直到channel中被写入数据为止ch := make(chan interface&#123;&#125;) // 创建一个空接口通道ch &lt;- 0 // 将0放入通道中ch &lt;- \"hello\" // 将hello字符串放入通道中//关闭close(ch) // 关闭channel 3. buffer channel和unbuffer channelchannel分为两种：unbuffered channel和buffered channel unbuffered channel：阻塞、同步模式 sender端向channel中send一个数据，然后阻塞，直到receiver端将此数据receive receiver端一直阻塞，直到sender端向channel发送了一个数据 12345678910111213141516171819package mainimport ( \"fmt\")var done chan boolfunc HelloWorld() &#123; fmt.Println(\"Hello world goroutine\") done &lt;- true&#125;func main() &#123; done = make(chan bool) // 创建一个channel go HelloWorld() &lt;-done&#125;// output \"hello world goroutine\" buffered channel：非阻塞、异步模式 sender端可以向channel中send多个数据(只要channel容量未满)，容量满之前不会阻塞 receiver端按照队列的方式(FIFO,先进先出)从buffered channel中按序receive其中数据 123ch := make(chan string, 3) // 创建了缓冲区为3的通道fmt.Println(len(ch))fmt.Println(cap(ch)) 4. channel foreach(遍历)可以通过for无限循环来读取channel中的数据，但是可以使用range来迭代channel， 它会返回每次迭代过程中所读取的数据，直到channel被关闭。 注意： 只要channel未关闭，range迭代channel就会一直被阻塞。 12345678910111213channel := make(chan int, 10)func printChannel1() &#123; for &#123; c := &lt;-channel fmt.Println(result) &#125;&#125;func printChannel2(count chan int) &#123; for c := range count &#123; fmt.Println(c) &#125;&#125; 5. channel死锁把数据往通道中发送时，如果接收方一直都没有接收，那么发送操作将持续阻塞。Go 程序运行时能智能地发现一些永远无法发送成功的语句并报错, 例如以下情形将将出现deadlock: 只要所有goroutine都被阻塞(包括main函数)，就会出现死锁 12fatal error: all goroutines are asleep - deadlock! //运行时发现所有的 goroutine（包括main）都处于等待 goroutine。 实例一： 123456789package mainimport \"fmt\"func main() &#123; ch := make(chan string) ch &lt;- \"hello world\" fmt.Println(&lt;-ch)&#125; 执行这段代码后将会出现如下错误： 1234567fatal error: all goroutines are asleep - deadlock!goroutine 1 [chan send]:main.main() /Users/xxx/go/src/examples/worker_pool/main.go:7 +0x59Process finished with exit code 2 出现deadlock的原因是因为ch&lt;-&quot;hello world&quot;这行导致了main goroutine被挂起，导致fmt.Println(&lt;ch)这一行得不到执行，且没有任何其他goroutine去消费这个channel, 因此造成了deadlock。 示例二： 123456789101112package mainimport \"fmt\"func main() &#123; ch := make(chan string) go func() &#123; fmt.Println(&lt;-ch) &#125;() ch &lt;- \"hello\" ch &lt;- \"world\"&#125; 执行完这段代码后，也会出现死锁的情况如下： 12345678hellofatal error: all goroutines are asleep - deadlock!goroutine 1 [chan send]:main.main() /Users/dawn/go/src/examples/worker_pool/main.go:11 +0x90Process finished with exit code 2 出现这样的结果是因为通道实际上是类型化消息的队列，它是先进先出(FIFO)的结构，可以保证发送给它们的元素的顺序。所以上面代码只取出了第一次传的值，即”hello”，而第二次传入的值没有一个配对的接收者来接收，因此就出现了deadlock。 示例三： 123456789101112131415package mainimport \"fmt\"func main() &#123; ch := make(chan string) go func() &#123; ch &lt;- \"hello\" ch &lt;- \"world\" &#125;() for &#123; fmt.Printf(\"%s \\n\", &lt;-ch) &#125;&#125; 执行完这段代码也会出现deadlock的情况如下： 123456789hello world fatal error: all goroutines are asleep - deadlock!goroutine 1 [chan receive]:main.main() /Users/dawn/go/src/examples/worker_pool/main.go:13 +0x7cProcess finished with exit code 2 出现上面的结果是因为for循环一直在获取通道中的值，但是在读取完 hello和world后，通道中没有新的值传入，这样接收者就阻塞了。 channel原理解析channel的结构如下所示(channel本质是消息传递的数据结构): 1234567891011121314151617181920212223242526type hchan struct &#123; // chan 里元素数量 qcount uint // chan 底层循环数组的长度 dataqsiz uint // 指向底层循环数组的指针 // 只针对有缓冲的 channel buf unsafe.Pointer // chan 中元素大小 elemsize uint16 // chan 是否被关闭的标志 closed uint32 // chan 中元素类型 elemtype *_type // element type // 已发送元素在循环数组中的索引 sendx uint // send index // 已接收元素在循环数组中的索引 recvx uint // receive index // 等待接收的 goroutine 队列 recvq waitq // list of recv waiters // 等待发送的 goroutine 队列 sendq waitq // list of send waiters // 保护 hchan 中所有字段 lock mutex&#125; buf :指向底层循环数组，只有缓冲型的 channel 才有。 sendx，recvx :指向底层循环数组，表示当前可以发送和接收的元素位置索引值（相对于底层数组） sendq，recvq :表示被阻塞的 goroutine，这些 goroutine 由于尝试读取 channel 或向 channel 发送数据而被阻塞。 lock :用来保证每个读 channel 或写 channel 的操作都是原子的。 waitq是sudog的一个双向链表，而sudog` 实际上是对 goroutine 的一个封装： 1234type waitq struct &#123; first *sudog last *sudog&#125; 例如，创建一个容量为 6 的，元素为 int 型的 channel 数据结构如下 ： 创建channel创建channel 在底层使用的是函数makechan: 1func makechan(t *chantype, size int64) *hchan 从函数原型来看，创建的 chan 是一个指针。所以我们能在函数间直接传递 channel，而不用传递 channel 的指针。 具体代码如下： 123456789101112131415161718192021222324252627282930313233343536373839const hchanSize = unsafe.Sizeof(hchan&#123;&#125;) + uintptr(-int(unsafe.Sizeof(hchan&#123;&#125;))&amp;(maxAlign-1))func makechan(t *chantype, size int64) *hchan &#123; elem := t.elem // 省略了检查 channel size，align 的代码 // …… var c *hchan // 如果元素类型不含指针 或者 size 大小为 0（无缓冲类型） // 只进行一次内存分配 if elem.kind&amp;kindNoPointers != 0 || size == 0 &#123; // 如果 hchan 结构体中不含指针，GC 就不会扫描 chan 中的元素 // 只分配 \"hchan 结构体大小 + 元素大小*个数\" 的内存 c = (*hchan)(mallocgc(hchanSize+uintptr(size)*elem.size, nil, true)) // 如果是缓冲型 channel 且元素大小不等于 0（大小等于 0的元素类型：struct&#123;&#125;） if size &gt; 0 &amp;&amp; elem.size != 0 &#123; c.buf = add(unsafe.Pointer(c), hchanSize) &#125; else &#123; // race detector uses this location for synchronization // Also prevents us from pointing beyond the allocation (see issue 9401). // 1. 非缓冲型的，buf 没用，直接指向 chan 起始地址处 // 2. 缓冲型的，能进入到这里，说明元素无指针且元素类型为 struct&#123;&#125;，也无影响 // 因为只会用到接收和发送游标，不会真正拷贝东西到 c.buf 处（这会覆盖 chan的内容） c.buf = unsafe.Pointer(c) &#125; &#125; else &#123; // 进行两次内存分配操作 c = new(hchan) c.buf = newarray(elem, int(size)) &#125; c.elemsize = uint16(elem.size) c.elemtype = elem // 循环数组长度 c.dataqsiz = uint(size) // 返回 hchan 指针 return c&#125; 说完channel的结构后，以一个例子说明channel如何接收和发送的 12345678910111213141516171819func goroutineA(a &lt;-chan int) &#123; val := &lt;- a fmt.Println(\"G1 received data: \", val) return&#125;func goroutineB(b &lt;-chan int) &#123; val := &lt;- b fmt.Println(\"G2 received data: \", val) return&#125;func main() &#123; ch := make(chan int) go goroutineA(ch) go goroutineB(ch) ch &lt;- 3 time.Sleep(time.Second)&#125; 接收channel的接收操作有两种写法，一种带 “ok”，反应 channel 是否关闭；一种不带 “ok”，这种写法，当接收到相应类型的零值时无法知道是真实的发送者发送过来的值，还是 channel 被关闭后，返回给接收者的默认类型的零值。两种写法，都有各自的应用场景。 12result := &lt;-chresult, ok := &lt;-ch 经过编译器的处理后，这两种写法最后对应源码里的这两个函数： 123456789// entry points for &lt;- c from compiled codefunc chanrecv1(c *hchan, elem unsafe.Pointer) &#123; chanrecv(c, elem, true)&#125;func chanrecv2(c *hchan, elem unsafe.Pointer) (received bool) &#123; _, received = chanrecv(c, elem, true) return&#125; chanrecv1 函数处理不带 “ok” 的情形，chanrecv2 则通过返回 “received” 这个字段来反应 channel 是否被关闭。接收值则比较特殊，会“放到”参数 elem 所指向的地址了，这很像 C/C++ 里的写法。如果代码里忽略了接收值，这里的 elem 为 nil。 无论如何，最终转向了 chanrecv 函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148// 位于 src/runtime/chan.go// chanrecv 函数接收 channel c 的元素并将其写入 ep 所指向的内存地址。// 如果 ep 是 nil，说明忽略了接收值。// 如果 block == false，即非阻塞型接收，在没有数据可接收的情况下，返回 (false, false)// 否则，如果 c 处于关闭状态，将 ep 指向的地址清零，返回 (true, false)// 否则，用返回值填充 ep 指向的内存地址。返回 (true, true)// 如果 ep 非空，则应该指向堆或者函数调用者的栈func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) &#123; // 省略 debug 内容 ………… // 如果是一个 nil 的 channel if c == nil &#123; // 如果不阻塞，直接返回 (false, false) if !block &#123; return &#125; // 否则，接收一个 nil 的 channel，goroutine 挂起 gopark(nil, nil, \"chan receive (nil chan)\", traceEvGoStop, 2) // 不会执行到这里 throw(\"unreachable\") &#125; // 在非阻塞模式下，快速检测到失败，不用获取锁，快速返回 // 当我们观察到 channel 没准备好接收： // 1. 非缓冲型，等待发送列队 sendq 里没有 goroutine 在等待 // 2. 缓冲型，但 buf 里没有元素 // 之后，又观察到 closed == 0，即 channel 未关闭。 // 因为 channel 不可能被重复打开，所以前一个观测的时候 channel 也是未关闭的， // 因此在这种情况下可以直接宣布接收失败，返回 (false, false) if !block &amp;&amp; (c.dataqsiz == 0 &amp;&amp; c.sendq.first == nil || c.dataqsiz &gt; 0 &amp;&amp; atomic.Loaduint(&amp;c.qcount) == 0) &amp;&amp; atomic.Load(&amp;c.closed) == 0 &#123; return &#125; var t0 int64 if blockprofilerate &gt; 0 &#123; t0 = cputicks() &#125; // 加锁 lock(&amp;c.lock) // channel 已关闭，并且循环数组 buf 里没有元素 // 这里可以处理非缓冲型关闭 和 缓冲型关闭但 buf 无元素的情况 // 也就是说即使是关闭状态，但在缓冲型的 channel， // buf 里有元素的情况下还能接收到元素 if c.closed != 0 &amp;&amp; c.qcount == 0 &#123; if raceenabled &#123; raceacquire(unsafe.Pointer(c)) &#125; // 解锁 unlock(&amp;c.lock) if ep != nil &#123; // 从一个已关闭的 channel 执行接收操作，且未忽略返回值 // 那么接收的值将是一个该类型的零值 // typedmemclr 根据类型清理相应地址的内存 typedmemclr(c.elemtype, ep) &#125; // 从一个已关闭的 channel 接收，selected 会返回true return true, false &#125; // 等待发送队列里有 goroutine 存在，说明 buf 是满的 // 这有可能是： // 1. 非缓冲型的 channel // 2. 缓冲型的 channel，但 buf 满了 // 针对 1，直接进行内存拷贝（从 sender goroutine -&gt; receiver goroutine） // 针对 2，接收到循环数组头部的元素，并将发送者的元素放到循环数组尾部 if sg := c.sendq.dequeue(); sg != nil &#123; // Found a waiting sender. If buffer is size 0, receive value // directly from sender. Otherwise, receive from head of queue // and add sender's value to the tail of the queue (both map to // the same buffer slot because the queue is full). recv(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true, true &#125; // 缓冲型，buf 里有元素，可以正常接收 if c.qcount &gt; 0 &#123; // 直接从循环数组里找到要接收的元素 qp := chanbuf(c, c.recvx) // ………… // 代码里，没有忽略要接收的值，不是 \"&lt;- ch\"，而是 \"val &lt;- ch\"，ep 指向 val if ep != nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; // 清理掉循环数组里相应位置的值 typedmemclr(c.elemtype, qp) // 接收游标向前移动 c.recvx++ // 接收游标归零 if c.recvx == c.dataqsiz &#123; c.recvx = 0 &#125; // buf 数组里的元素个数减 1 c.qcount-- // 解锁 unlock(&amp;c.lock) return true, true &#125; if !block &#123; // 非阻塞接收，解锁。selected 返回 false，因为没有接收到值 unlock(&amp;c.lock) return false, false &#125; // 接下来就是要被阻塞的情况了 // 构造一个 sudog gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; // 待接收数据的地址保存下来 mysg.elem = ep mysg.waitlink = nil gp.waiting = mysg mysg.g = gp mysg.selectdone = nil mysg.c = c gp.param = nil // 进入channel 的等待接收队列 c.recvq.enqueue(mysg) // 将当前 goroutine 挂起 goparkunlock(&amp;c.lock, \"chan receive\", traceEvGoBlockRecv, 3) // 被唤醒了，接着从这里继续执行一些扫尾工作 if mysg != gp.waiting &#123; throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil if mysg.releasetime &gt; 0 &#123; blockevent(mysg.releasetime-t0, 2) &#125; closed := gp.param == nil gp.param = nil mysg.c = nil releaseSudog(mysg) return true, !closed&#125; 如果 channel 是一个空值（nil），在非阻塞模式下，会直接返回。在阻塞模式下，会调用 gopark 函数挂起 goroutine，这个会一直阻塞下去。因为在 channel 是 nil 的情况下，要想不阻塞，只有关闭它，但关闭一个 nil 的 channel 又会发生 panic，所以没有机会被唤醒了。 和发送函数一样，接下来搞了一个在非阻塞模式下，不用获取锁，快速检测到失败并且返回的操作。顺带插一句，我们平时在写代码的时候，找到一些边界条件，快速返回，能让代码逻辑更清晰，因为接下来的正常情况就比较少，更聚焦了，看代码的人也更能专注地看核心代码逻辑了。 当我们观察到 channel 没准备好接收： 非缓冲型，等待发送列队里没有 goroutine 在等待 缓冲型，但 buf 里没有元素 之后，又观察到 closed == 0，即 channel 未关闭。 因为 channel 不可能被重复打开，所以前一个观测的时候， channel 也是未关闭的，因此在这种情况下可以直接宣布接收失败，快速返回。因为没被选中，也没接收到数据，所以返回值为 (false, false)。 接下来的操作，首先会上一把锁，粒度比较大。如果 channel 已关闭，并且循环数组 buf 里没有元素。对应非缓冲型关闭和缓冲型关闭但 buf 无元素的情况，返回对应类型的零值，但 received 标识是 false，告诉调用者此 channel 已关闭，你取出来的值并不是正常由发送者发送过来的数据。但是如果处于 select 语境下，这种情况是被选中了的。很多将 channel 用作通知信号的场景就是命中了这里。 接下来，如果有等待发送的队列，说明 channel 已经满了，要么是非缓冲型的 channel，要么是缓冲型的 channel，但 buf 满了。这两种情况下都可以正常接收数据。 于是，调用 recv 函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445func recv(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; &#x2F;&#x2F; 如果是非缓冲型的 channel if c.dataqsiz &#x3D;&#x3D; 0 &#123; if raceenabled &#123; racesync(c, sg) &#125; &#x2F;&#x2F; 未忽略接收的数据 if ep !&#x3D; nil &#123; &#x2F;&#x2F; 直接拷贝数据，从 sender goroutine -&gt; receiver goroutine recvDirect(c.elemtype, sg, ep) &#125; &#125; else &#123; &#x2F;&#x2F; 缓冲型的 channel，但 buf 已满。 &#x2F;&#x2F; 将循环数组 buf 队首的元素拷贝到接收数据的地址 &#x2F;&#x2F; 将发送者的数据入队。实际上这时 revx 和 sendx 值相等 &#x2F;&#x2F; 找到接收游标 qp :&#x3D; chanbuf(c, c.recvx) &#x2F;&#x2F; ………… &#x2F;&#x2F; 将接收游标处的数据拷贝给接收者 if ep !&#x3D; nil &#123; typedmemmove(c.elemtype, ep, qp) &#125; &#x2F;&#x2F; 将发送者数据拷贝到 buf typedmemmove(c.elemtype, qp, sg.elem) &#x2F;&#x2F; 更新游标值 c.recvx++ if c.recvx &#x3D;&#x3D; c.dataqsiz &#123; c.recvx &#x3D; 0 &#125; c.sendx &#x3D; c.recvx &#125; sg.elem &#x3D; nil gp :&#x3D; sg.g &#x2F;&#x2F; 解锁 unlockf() gp.param &#x3D; unsafe.Pointer(sg) if sg.releasetime !&#x3D; 0 &#123; sg.releasetime &#x3D; cputicks() &#125; &#x2F;&#x2F; 唤醒发送的 goroutine。需要等到调度器的光临 goready(gp, skip+1)&#125; 如果是非缓冲型的，就直接从发送者的栈拷贝到接收者的栈。否则，就是缓冲型 channel，而 buf 又满了的情形。说明发送游标和接收游标重合了，因此需要先找到接收游标： 1234567891011func recvDirect(t *_type, sg *sudog, dst unsafe.Pointer) &#123; // dst is on our stack or the heap, src is on another stack. src := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125;// chanbuf(c, i) is pointer to the i'th slot in the buffer.func chanbuf(c *hchan, i uint) unsafe.Pointer &#123; return add(c.buf, uintptr(i)*uintptr(c.elemsize))&#125; 将该处的元素拷贝到接收地址。然后将发送者待发送的数据拷贝到接收游标处。这样就完成了接收数据和发送数据的操作。接着，分别将发送游标和接收游标向前进一，如果发生“环绕”，再从 0 开始。 最后，取出 sudog 里的 goroutine，调用 goready 将其状态改成 “runnable”，待发送者被唤醒，等待调度器的调度。 然后，如果 channel 的 buf 里还有数据，说明可以比较正常地接收。注意，这里，即使是在 channel 已经关闭的情况下，也是可以走到这里的。这一步比较简单，正常地将 buf 里接收游标处的数据拷贝到接收数据的地址。 到了最后一步，走到这里来的情形是要阻塞的。当然，如果 block 传进来的值是 false，那就不阻塞，直接返回就好了。 先构造一个 sudog，接着就是保存各种值了。注意，这里会将接收数据的地址存储到了 elem 字段，当被唤醒时，接收到的数据就会保存到这个字段指向的地址。然后将 sudog 添加到 channel 的 recvq 队列里。调用 goparkunlock 函数将 goroutine 挂起。 我们继续之前的例子。前面说到第 14 行，创建了一个非缓冲型的 channel，接着，第 15、16 行分别创建了一个 goroutine，各自执行了一个接收操作。通过前面的源码分析，我们知道，这两个 goroutine （后面称为 G1 和 G2 好了）都会被阻塞在接收操作。G1 和 G2 会挂在 channel 的 recq 队列中，形成一个双向循环链表。因此此时chan的状态如下： G1 和 G2 被挂起了，状态是 WAITING, goroutine 是用户态的协程，由 Go runtime 进行管理，作为对比，内核线程由 OS 进行管理。Goroutine 更轻量，因此我们可以轻松创建数万 goroutine。一个内核线程可以管理多个 goroutine，当其中一个 goroutine 阻塞时，内核线程可以调度其他的 goroutine 来运行，内核线程本身不会阻塞。这就是通常我们说的 M:N 模型： M:N 模型通常由三部分构成：M、P、G。M 是内核线程，负责运行 goroutine；P 是 context，保存 goroutine 运行所需要的上下文，它还维护了可运行（runnable）的 goroutine 列表；G 则是待运行的 goroutine。M 和 P 是 G 运行的基础。 继续回到例子。假设我们只有一个 M，当 G1（go goroutineA(ch)） 运行到 val := &lt;- a 时，它由本来的 running 状态变成了 waiting 状态（调用了 gopark 之后的结果）： G1 脱离与 M 的关系，但调度器可不会让 M 闲着，所以会接着调度另一个 goroutine 来运行： G2 也是同样的遭遇。现在 G1 和 G2 都被挂起了，等待着一个 sender 往 channel 里发送数据，才能得到解救。 发送第 17 行向 channel 发送了一个元素 3。发送操作最终转化为 chansend 函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122// 位于 src/runtime/chan.gofunc chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool &#123; // 如果 channel 是 nil if c == nil &#123; // 不能阻塞，直接返回 false，表示未发送成功 if !block &#123; return false &#125; // 当前 goroutine 被挂起 gopark(nil, nil, \"chan send (nil chan)\", traceEvGoStop, 2) throw(\"unreachable\") &#125; // 省略 debug 相关…… // 对于不阻塞的 send，快速检测失败场景 // // 如果 channel 未关闭且 channel 没有多余的缓冲空间。这可能是： // 1. channel 是非缓冲型的，且等待接收队列里没有 goroutine // 2. channel 是缓冲型的，但循环数组已经装满了元素 if !block &amp;&amp; c.closed == 0 &amp;&amp; ((c.dataqsiz == 0 &amp;&amp; c.recvq.first == nil) || (c.dataqsiz &gt; 0 &amp;&amp; c.qcount == c.dataqsiz)) &#123; return false &#125; var t0 int64 if blockprofilerate &gt; 0 &#123; t0 = cputicks() &#125; // 锁住 channel，并发安全 lock(&amp;c.lock) // 如果 channel 关闭了 if c.closed != 0 &#123; // 解锁 unlock(&amp;c.lock) // 直接 panic panic(plainError(\"send on closed channel\")) &#125; // 如果接收队列里有 goroutine，直接将要发送的数据拷贝到接收 goroutine if sg := c.recvq.dequeue(); sg != nil &#123; send(c, sg, ep, func() &#123; unlock(&amp;c.lock) &#125;, 3) return true &#125; // 对于缓冲型的 channel，如果还有缓冲空间 if c.qcount &lt; c.dataqsiz &#123; // qp 指向 buf 的 sendx 位置 qp := chanbuf(c, c.sendx) // …… // 将数据从 ep 处拷贝到 qp typedmemmove(c.elemtype, qp, ep) // 发送游标值加 1 c.sendx++ // 如果发送游标值等于容量值，游标值归 0 if c.sendx == c.dataqsiz &#123; c.sendx = 0 &#125; // 缓冲区的元素数量加一 c.qcount++ // 解锁 unlock(&amp;c.lock) return true &#125; // 如果不需要阻塞，则直接返回错误 if !block &#123; unlock(&amp;c.lock) return false &#125; // channel 满了，发送方会被阻塞。接下来会构造一个 sudog // 获取当前 goroutine 的指针 gp := getg() mysg := acquireSudog() mysg.releasetime = 0 if t0 != 0 &#123; mysg.releasetime = -1 &#125; mysg.elem = ep mysg.waitlink = nil mysg.g = gp mysg.selectdone = nil mysg.c = c gp.waiting = mysg gp.param = nil // 当前 goroutine 进入发送等待队列 c.sendq.enqueue(mysg) // 当前 goroutine 被挂起 goparkunlock(&amp;c.lock, \"chan send\", traceEvGoBlockSend, 3) // 从这里开始被唤醒了（channel 有机会可以发送了） if mysg != gp.waiting &#123; throw(\"G waiting list is corrupted\") &#125; gp.waiting = nil if gp.param == nil &#123; if c.closed == 0 &#123; throw(\"chansend: spurious wakeup\") &#125; // 被唤醒后，channel 关闭了。坑爹啊，panic panic(plainError(\"send on closed channel\")) &#125; gp.param = nil if mysg.releasetime &gt; 0 &#123; blockevent(mysg.releasetime-t0, 2) &#125; // 去掉 mysg 上绑定的 channel mysg.c = nil releaseSudog(mysg) return true&#125; 上面的代码注释地比较详细了，我们来详细看看。 如果检测到 channel 是空的，当前 goroutine 会被挂起。 对于不阻塞的发送操作，如果 channel 未关闭并且没有多余的缓冲空间（说明：a. channel 是非缓冲型的，且等待接收队列里没有 goroutine；b. channel 是缓冲型的，但循环数组已经装满了元素） 如果检测到 channel 已经关闭，直接 panic。 如果能从等待接收队列 recvq 里出队一个 sudog（代表一个 goroutine），说明此时 channel 是空的，没有元素，所以才会有等待接收者。这时会调用 send 函数将元素直接从发送者的栈拷贝到接收者的栈，关键操作由 sendDirect 函数完成。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// send 函数处理向一个空的 channel 发送操作// ep 指向被发送的元素，会被直接拷贝到接收的 goroutine// 之后，接收的 goroutine 会被唤醒// c 必须是空的（因为等待队列里有 goroutine，肯定是空的）// c 必须被上锁，发送操作执行完后，会使用 unlockf 函数解锁// sg 必须已经从等待队列里取出来了// ep 必须是非空，并且它指向堆或调用者的栈func send(c *hchan, sg *sudog, ep unsafe.Pointer, unlockf func(), skip int) &#123; // 省略一些用不到的 // …… // sg.elem 指向接收到的值存放的位置，如 val &lt;- ch，指的就是 &amp;val if sg.elem != nil &#123; // 直接拷贝内存（从发送者到接收者） sendDirect(c.elemtype, sg, ep) sg.elem = nil &#125; // sudog 上绑定的 goroutine gp := sg.g // 解锁 unlockf() gp.param = unsafe.Pointer(sg) if sg.releasetime != 0 &#123; sg.releasetime = cputicks() &#125; // 唤醒接收的 goroutine. skip 和打印栈相关，暂时不理会 goready(gp, skip+1)&#125;// 向一个非缓冲型的 channel 发送数据、从一个无元素的（非缓冲型或缓冲型但空）的 channel// 接收数据，都会导致一个 goroutine 直接操作另一个 goroutine 的栈// 由于 GC 假设对栈的写操作只能发生在 goroutine 正在运行中并且由当前 goroutine 来写// 所以这里实际上违反了这个假设。可能会造成一些问题，所以需要用到写屏障来规避func sendDirect(t *_type, sg *sudog, src unsafe.Pointer) &#123; // src 在当前 goroutine 的栈上，dst 是另一个 goroutine 的栈 // 直接进行内存\"搬迁\" // 如果目标地址的栈发生了栈收缩，当我们读出了 sg.elem 后 // 就不能修改真正的 dst 位置的值了 // 因此需要在读和写之前加上一个屏障 dst := sg.elem typeBitsBulkBarrier(t, uintptr(dst), uintptr(src), t.size) memmove(dst, src, t.size)&#125; 然后，解锁、唤醒接收者，等待调度器的光临，接收者也得以重见天日，可以继续执行接收操作之后的代码了。 在发送小节里我们说到 G1 和 G2 现在被挂起来了，等待 sender 的解救。在第 17 行，主协程向 ch 发送了一个元素 3，来看下接下来会发生什么。 根据前面源码分析的结果，我们知道，sender 发现 ch 的 recvq 里有 receiver 在等待着接收，就会出队一个 sudog，把 recvq 里 first 指针的 sudo “推举”出来了，并将其加入到 P 的可运行 goroutine 队列中。 然后，sender 把发送元素拷贝到 sudog 的 elem 地址处，最后会调用 goready 将 G1 唤醒，状态变为 runnable。 channel触发panic的三种情况 向一个关闭的 channel 进行写操作(读操作不会触发panic，会返回channel 类型的零值) 关闭一个 nil 的 channel 重复关闭一个 channel。 参考文档 Go基础系列：channel入门 Go Channel 详解 //鸟窝大佬 go中通道channel的使用及原理 // 推荐阅读 深度解密Go语言之channel // 强烈推荐阅读绕全成大佬的所有文章 golang runtime源码阅读 channal实现 // 这篇文章也挺不错的","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"kubernetes常见的网络插件","slug":"kubernetes-cni-plugins","date":"2021-01-03T06:02:52.000Z","updated":"2021-12-17T07:42:48.334Z","comments":true,"path":"2021/01/03/kubernetes-cni-plugins/","link":"","permalink":"https://github.com/fafucoder/2021/01/03/kubernetes-cni-plugins/","excerpt":"","text":"概述常见的容器网络方案可以从协议栈层级、穿越形态、隔离方式这三种形式进行划分。 协议栈层级: 第一种：协议栈二层。 第二种：协议栈三层（纯路由转发）。 第三种：协议栈二层加三层。 穿越形态： 按穿越的形态划分，这个与实际部署环境十分相关。穿越形态分为两种：Underlay、Overlay。 Underlay：在一个较好的一个可控的网络场景下，我们一般利用 Underlay。可以这样通俗的理解，无论下面是裸机还是虚拟机，只要网络可控，整个容器的网络便可直接穿过去 ，这就是 Underlay。 Overlay：Overlay 在云化场景比较常见。Overlay 下面是受控的 VPC 网络，当出现不属于 VPC 管辖范围中的 IP 或者 MAC，VPC 将不允许此 IP/MAC 穿越。出现这种情况时，我们都利用 Overlay 方式来做。 隔离方式： 隔离方式与多种户型相关（用户与用户之间的隔离方式），隔离方式分为 FLAT、VLAN、VXLAN 三种： FLAT：纯扁平网络，无隔离； VLAN：VLAN 机房中使用偏多，但实际上存在一个问题？就是它总的租户数量受限。众所周知，VLAN 具有数量限制。 VXLAN：VXLAN 是现下较为主流的一种隔离方式。因为它的规模性较好较大，且它基于 IP 穿越方式较好。 fannelKubernetes中解决网络跨主机通信的一个经典插件就是Flannel。flannel因为其简单的特性而广为熟知，在现在的flannel实现方案中，有三种方式，分别为udp模式，host-gw模式，vxlan模式 udp模式UDP是最早的实现方式，但是由于其性能原因，现已经被废弃，但是UDP模式是最直接，也最容易理解的跨主机实现方式。 假如有两台Node，如下： Node01上有容器nginx01，其IP为172.20.1.107，其docker0的地址为172.20.1.1/24； Node02上有容器nginx02，其IP为172.20.2.133，其docker0的地址为172.20.2.1/24； 那么现在nginx01要访问nginx02，其流程应该是怎么样的呢？ 首先从nginx01发送IP包，源IP是172.20.1.107，目的IP是172.20.2.133。 由于目的IP并不在Node01上的docker0网桥里，所以会将包通过默认路由转发到docker0网桥所在的宿主机上； 它会通过本地的路由规则，转发到下一个目的IP，我们可以通过ip route查看本地的路由信息，通过路由信息可以看到它被转发到一个flannel0的设备中； flannel0设备会把这个IP包交给创建这个设备的应用程序，也就是Flannel进程（从内核状态向用户状态切换）； Flannel进程收到IP包后，将这个包封装在UDP中，就根据其目的地址将其转发给Node02（通过每个宿主机上监听的8285端口），这时候的源地址是Node01的地址，目的地址是Node02的地址； Node02收到包后，就会直接将其转发给flannel0设备，然后进行解包，匹配本地路由规则转发给docker0网桥，然后docker0网桥就扮演二层交换机的功能，将包转发到最终的目的地； 注： 1、flannel0是一个TUN设备，它的作用是在操作系统和应用程序之间传递IP包； 2、Flannel是根据子网（Subnet）来查看IP地址对应的容器是运行在那个Node上； 3、这些子网和Node的对应关系，是保存在Etcd中（仅限UDP模式）； 4、UDP模式其实是一个三层的Overlay网络；它首先对发出的IP包进行UDP封装，然后接收端对包进行解封拿到原始IP，进而把这个包转发给目标容器。这就好比Flannel在不同的宿主机上的两容器之间打通了一条隧道，使得这个两个IP可以通信，而无需关心容器和宿主机的分布情况； UDP之所以被废弃是主要是由于其仅在发包的过程中就在用户态和内核态进行来回的数据交换，这样的性能代价是很高的. vxlan 模式VXLAN：Virtual Extensible LAN（虚拟可扩展局域网），是Linux内核本身就支持的一种虚拟化网络技术，它可以完全在内核态实现上述的封装和解封装过程，减少用户态到内核态的切换次数，把核心的处理逻辑都放到内核态，其通过与前面相似的隧道技术，构建出覆盖网络或者叠加网络（Overlay Network）。 其设计思想为在现有的三层网络下，叠加一层虚拟的并由内核VXLAN维护的二层网络，使得连接在这个二层网络上的主机可以像在局域网一样通信。 为了能够在二层网络中打通隧道，VXLAN会在宿主机上设置一个特殊的网络设备作为隧道的两端，这个隧道就叫VTEP（Virtual Tunnel End Point 虚拟隧道端点）。而VTEP的作用跟上面的flanneld进程非常相似，只不过它进行封装和解封的对象是二层的数据帧，而且这个工作的执行流程全部在内核中完成。 我们可以看到每台Node上都有一个flannel.1的网卡，它就是VXLAN所需要的VTEP设备，它既有IP地址，也有MAC地址。 现在我们nginx01要访问nginx02，其流程如下： nginx01发送请求包会被转发到docker0； 然后会通过路由转发到本机的flannel,1； flannel.1收到包后通过ARP记录找到目的MAC地址，并将其加原始包上，封装成二层数据帧（将源MAC地址和目的MAC地址封装在它们对应的IP头外部）； Linux内核把这个数据帧封装成普通的可传输的数据帧，通过宿主机的eth0进行传输（也就是在原有的数据帧上面加一个VXLAN头VNI，它是识别某个数据帧是不是归自己处理的的重要标识，而在flannel中，VNI的默认值就是1，这是由于宿主机上的VTEP设备名称叫flannel.1，这里的1就是VNI的值）； 然后Linux内核会把这个数据帧封装到UDP包里发出去； Node02收到包后发现VNI为 1，Linux内核会对其进行解包，拿到里面的数据帧，然后根据VNI的值把它交给Node02上的flannel.1设备，然后继续进行接下来的处理； 在这种场景下，flannel.1设备实际扮演的是一个网桥的角色，在二层网络进行UDP包的转发，在Linux内核中，网桥设备进行转发的依据是一个叫做FDB（Foewarding Database）的转发数据库，它的内容可以通过bridge fdb命令可以查看。 host-gw模式前面的两种模式都是二层网络的解决方案，对于三层网络，Flannel提供host-gw解决方案。 如上所示，如果我nginx01要访问nginx02，流程如下： 转发请求包会被转发到cni0； 到达本机后会匹配本机的路由，如上的路由信息，然后发现要到172.20.2.0/24的请求要经过eth0出去，并且吓一跳地址为172.16.1.130； 到达Node2过后，通过路由规则到node02的cni0，再转发到nginx02； 其工作流程比较简单，主要是会在节点上生成许多路由规则。 host-gw的工作原理就是将Flannel的所有子网的下一跳设置成该子网对应的宿主机的IP地址，也就是说Host会充当这条容器通信路径的网关，当然，Flannel子网和主机的信息会保存在Etcd中，flanneld进程只需要WATCH这个数据的变化，然后实时更新路由表。 在这种模式下，就免除了额外的封包解包的性能损耗，在这种模式下，性能损耗大约在10%左右，而XVLAN隧道的机制，性能损耗大约在20%~30%。 从上面可以知道，host-gw的工作核心为IP包在封装成帧发送出去的时候会在使用路由表中写下一跳来设置目的的MAC地址，这样它就会经过二层转发到达目的宿主机。这就要求集群宿主机必须是二层连通的。 calicoCalico是Kubernetes生态系统中另一种流行的网络选择。虽然Flannel被公认为是最简单的选择，但Calico以其性能、灵活性而闻名。Calico的功能更为全面，不仅提供主机和pod之间的网络连接，还涉及网络安全和管理。在现有的calico插件中，也提供的三种模式，分别是BGP模式，RR模式和IPIP模式。下面主要说明BGP模式和IPIP模式。 IPIP模式Calico 的ipip模式和flannel的vxlan差不多，也是基于二层数据的封装和解封；也会创建一个虚拟网卡 tunl0。在前面提到过，Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于 Calico 来说，也同样存在。 如上所示, Pod 1 访问 Pod 2大致流程如下： 数据包从容器1出到达Veth Pair另一端（宿主机上，以cali前缀开头）； 进入IP隧道设备（tunl0），由Linux内核IPIP驱动封装在宿主机网络的IP包中（新的IP包目的地之是原IP包的下一跳地址，即192.168.31.63），这样，就成了Node1 到Node2的数据包； 数据包经过路由器三层转发到Node2； Node2收到数据包后，网络协议栈会使用IPIP驱动进行解包，从中拿到原始IP包； 然后根据路由规则，根据路由规则将数据包转发给cali设备，从而到达容器2。 BGP模式 如上所示，Pod 1 访问 Pod 2大致流程如下： 数据包从容器1出到达Veth Pair另一端（宿主机上，以cali前缀开头）； 宿主机根据路由规则，将数据包转发给下一跳（网关）； 到达Node2，根据路由规则将数据包转发给cali设备，从而到达容器2 其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。 不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。 总结k8s的容器虚拟化网络方案大体分为两种： 基于隧道方案和基于路由方案 一、隧道方案 flannel的 vxlan模式、calico的ipip模式都是隧道方案。隧道模式分为两个过程：分配网段和封包/解包两个过程 分配网络: 宿主机利用etcd（etcd中维护ip）会为当前主机上运行的容器分配一个虚拟ip，并且宿主机上运行一个代理网络进程agent，代理出入的数据包。 封包/解包: 宿主上的agent进程会改变容器的发出的数据包的源ip和目的ip，目的宿主机上的agent收到数据包进行拆包然后送到目的容器。 二、路由方案 flannel的host-gw模式，calico的bgp模式都是路由方案。路由方案也分为两个过程：分配网段、广播路由两个阶段 分配网段: 类似隧道模式，每台宿主上的agent会从etcd中为每个容器分配一个虚ip。 广播路由: agent会在宿主机上增加一套路由规则，凡是目的地址是该容器的ip的就发往容器的虚拟网卡上，同时会通过BGP广播协议将自己的虚拟ip发往集群中其他node节点，其他的node节点收到广播同样在本机创建一条路由规则：该虚拟ip的数据包发至他的宿主机ip上。 优缺点对比: 由于隧道模式存在封包和拆包的过程而路由模式没有，所以路由模式性能高于隧道模式； 隧道模式通过agent代理工作在ip层而路由模型工作在mac层下； 路由模式会因为路由表膨胀性能下降； 参考文档 https://aijishu.com/a/1060000000081891 // fannel网络性能测试 https://blog.csdn.net/u010230794/article/details/103608777 // 总结的参考处 https://cloud.tencent.com/developer/article/1602843 // kubernetes中常用网络插件之Flannel https://zhdya.okay686.cn/2019/12/22/K8S%E9%9B%86%E7%BE%A4%E7%BD%91%E8%B7%AFCalico%E7%BD%91%E7%BB%9C%E7%BB%84%E4%BB%B6%E5%AE%9E%E8%B7%B5(BGP%E3%80%81RR%E3%80%81IPIP)/ // Calico网络组件实践(BGP、RR、IPIP)","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"etcd 获取 kubernetes 中的数据","slug":"kubernetes-etcd","date":"2020-12-14T12:35:44.000Z","updated":"2021-12-17T07:42:48.335Z","comments":true,"path":"2020/12/14/kubernetes-etcd/","link":"","permalink":"https://github.com/fafucoder/2020/12/14/kubernetes-etcd/","excerpt":"","text":"etcdctl 命令etcd厂商提供了命令行客户端 etcdctl，可以使用客户端直接跟etcd交互 etcdctl使用方法: kubernetes etcd pod中执行命令在kubernetes中执行etcdctl命令需要知道endpoint地址以及证书 查看endpoint地址可以通过kube-apiserver与etcd交互获取endpoint地址，通过 ps -ef | grep kube-apiserver能够获取到kube-apiserver的启动参数 获取证书etcd证书一般在/etc/kubernetes/pki目录下，或者在 /etc/ssl/etcd/ssl目录下 etcd命令操作 设置etcdctl版本export ETCDCTL_API=3 执行etcdctl 命令(命令跟上面的一致) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113# 获取 endpoint statusetcdctl --endpoints&#x3D;https:&#x2F;&#x2F;192.168.0.171:2379 --cert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1.pem --key&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1-key.pem --cacert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;ca.pem -w table endpoint status+----------------------------+------------------+---------+---------+-----------+-----------+------------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |+----------------------------+------------------+---------+---------+-----------+-----------+------------+| https:&#x2F;&#x2F;192.168.0.171:2379 | 7c4294e7b9a48ff3 | 3.3.12 | 19 MB | true | 2 | 1916345 |+----------------------------+------------------+---------+---------+-----------+-----------+------------+# 获取endpoint healthetcdctl --endpoints&#x3D;https:&#x2F;&#x2F;192.168.0.171:2379 --cert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1.pem --key&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1-key.pem --cacert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;ca.pem -w table endpoint healthhttps:&#x2F;&#x2F;192.168.0.171:2379 is healthy: successfully committed proposal: took &#x3D; 826.779µs# 获取k8s注册的数据etcdctl --endpoints&#x3D;https:&#x2F;&#x2F;192.168.0.171:2379 --cert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1.pem --key&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1-key.pem --cacert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;ca.pem get &#x2F; --prefix --keys-only&#x3D;true&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;alertmanagers.monitoring.coreos.com&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;applications.app.k8s.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;blockdeviceclaims.openebs.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;blockdevices.openebs.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;cdiconfigs.cdi.kubevirt.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;cdis.cdi.kubevirt.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;clusterconfigurations.installer.kubesphere.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;clusterpropagatedversions.core.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;clusters.cluster.kubesphere.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;dashboards.monitoring.kubesphere.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;datavolumes.cdi.kubevirt.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;devopsprojects.devops.kubesphere.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;dnsendpoints.multiclusterdns.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;domains.multiclusterdns.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;emailconfigs.notification.kubesphere.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;emailreceivers.notification.kubesphere.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;exporters.events.kubesphere.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;externalips.kubeovn.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;externalsubnets.kubeovn.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedapplications.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedclusterrolebindings.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedclusterroles.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedconfigmaps.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federateddeployments.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedglobalrolebindings.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedglobalroles.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedingresses.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedjobs.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedlimitranges.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatednamespaces.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedpersistentvolumeclaims.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedreplicasets.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedsecrets.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedserviceaccounts.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedservices.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedservicestatuses.core.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedstatefulsets.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedtypeconfigs.core.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedusers.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedworkspacerolebindings.types.kubefed.io&#x2F;registry&#x2F;apiextensions.k8s.io&#x2F;customresourcedefinitions&#x2F;federatedworkspaceroles.types.kubefed.io# 获取具体的数据(先拿到key,然后获取key的内容)etcdctl --endpoints&#x3D;https:&#x2F;&#x2F;192.168.0.171:2379 --cert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1.pem --key&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1-key.pem --cacert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;ca.pem get &#x2F;registry&#x2F;kubeovn.io&#x2F;subnets&#x2F;ovn-default&#x2F;registry&#x2F;kubeovn.io&#x2F;subnets&#x2F;ovn-default&#123;&quot;apiVersion&quot;:&quot;kubeovn.io&#x2F;v1&quot;,&quot;kind&quot;:&quot;Subnet&quot;,&quot;metadata&quot;:&#123;&quot;creationTimestamp&quot;:&quot;2020-12-11T09:25:44Z&quot;,&quot;finalizers&quot;:[&quot;kube-ovn-controller&quot;],&quot;generation&quot;:1,&quot;name&quot;:&quot;ovn-default&quot;,&quot;uid&quot;:&quot;89ac06b6-7ca8-4606-843e-6630eb818317&quot;&#125;,&quot;spec&quot;:&#123;&quot;cidrBlock&quot;:&quot;10.233.64.0&#x2F;18&quot;,&quot;default&quot;:true,&quot;excludeIps&quot;:[&quot;10.233.64.1&quot;],&quot;gateway&quot;:&quot;10.233.64.1&quot;,&quot;gatewayNode&quot;:&quot;&quot;,&quot;gatewayType&quot;:&quot;distributed&quot;,&quot;natOutgoing&quot;:true,&quot;private&quot;:false,&quot;protocol&quot;:&quot;IPv4&quot;,&quot;provider&quot;:&quot;ovn&quot;,&quot;underlayGateway&quot;:false&#125;,&quot;status&quot;:&#123;&quot;activateGateway&quot;:&quot;&quot;,&quot;availableIPs&quot;:16314,&quot;conditions&quot;:[&#123;&quot;lastTransitionTime&quot;:&quot;2020-12-11T09:25:50Z&quot;,&quot;lastUpdateTime&quot;:&quot;2020-12-11T09:25:50Z&quot;,&quot;reason&quot;:&quot;ResetLogicalSwitchAclSuccess&quot;,&quot;status&quot;:&quot;True&quot;,&quot;type&quot;:&quot;Validated&quot;&#125;,&#123;&quot;lastTransitionTime&quot;:&quot;2020-12-11T09:25:50Z&quot;,&quot;lastUpdateTime&quot;:&quot;2020-12-11T09:25:50Z&quot;,&quot;reason&quot;:&quot;ResetLogicalSwitchAclSuccess&quot;,&quot;status&quot;:&quot;True&quot;,&quot;type&quot;:&quot;Ready&quot;&#125;],&quot;usingIPs&quot;:67&#125;&#125;# curd......# etcd 数据备份etcdctl --endpoints&#x3D;https:&#x2F;&#x2F;192.168.0.171:2379 --cert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1.pem --key&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1-key.pem --cacert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;ca.pem snapshot save &#x2F;root&#x2F;backupSnapshot saved at &#x2F;root&#x2F;backup# etcd 数据恢复etcdctl --endpoints&#x3D;https:&#x2F;&#x2F;192.168.0.171:2379 --cert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1.pem --key&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;admin-node1-key.pem --cacert&#x3D;&#x2F;etc&#x2F;ssl&#x2F;etcd&#x2F;ssl&#x2F;ca.pem snapshot restore &#x2F;root&#x2F;backup 参考文档 etcd 及 etcd 在 k8s中的用法 [译]走进Kubernetes集群的大脑：Etcd etcd","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes exec原理","slug":"kubernetes-exec","date":"2020-11-30T12:28:30.000Z","updated":"2021-12-17T07:42:48.335Z","comments":true,"path":"2020/11/30/kubernetes-exec/","link":"","permalink":"https://github.com/fafucoder/2020/11/30/kubernetes-exec/","excerpt":"","text":"参考文档 https://segmentfault.com/a/1190000022163850 https://juejin.cn/post/6844904168860155911 https://blog.fleeto.us/post/how-kubectl-exec-works https://segmentfault.com/a/1190000022163808","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"golang build 调试技巧","slug":"golang-build2","date":"2020-11-29T09:35:17.000Z","updated":"2021-12-17T07:42:48.325Z","comments":true,"path":"2020/11/29/golang-build2/","link":"","permalink":"https://github.com/fafucoder/2020/11/29/golang-build2/","excerpt":"","text":"编译技巧go build mod使用 go build -mod=vendor 来构建项目，因为在 go modules 模式下 go build 是屏蔽 vendor 机制的，所以需要特定参数重新开启 vendor 机制, 这样子断点调试的时候，就能够在vendor文件夹下断点了。如果golang的版本大于等于1.14, 则默认使用-mod=vendor go build tag-ldflags 可以传递指令,也可以通过// +build 标记去做，如下所示： 12345// +build releasepackage mainconst version = \"RELEASE\" 上面代码的关键是 // +build release这行代码，注意这行代码前后须有一个空行隔开，例如在第一行时，接下来要空出一行。这个文件只会被go bulid识别到，而go run等命令不会去识别这个文件，而且vscode, golang等编辑器也会略过这个文件。 通过引入 // +build的好处就是可以作为文件因为包， 比如main包要引入其他main 包的方案中，可以创建一个空的文件,然后引入文件。 1234567// +build toolspackage libimport ( _ \"k8s.io/kube-openapi/cmd/openapi-gen\") 参考文档 go mod refer go build -tags 使用","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"kubernetes生成swagger ui","slug":"kubernetes-swagger","date":"2020-11-29T09:02:37.000Z","updated":"2021-12-17T07:42:48.339Z","comments":true,"path":"2020/11/29/kubernetes-swagger/","link":"","permalink":"https://github.com/fafucoder/2020/11/29/kubernetes-swagger/","excerpt":"","text":"如何获取kubernetes的Openapi百度上搜索kubernetes获取Openapi基本上都是老版本的获取方法，非常繁琐。 kubernetes会自己生成openapi，不需要任何配置只需要： 1kubectl proxy --port=8081 配置kubectl proxy即可，通过localhost:8081就可以查看api的定义，要获取到openapi.json只需要访问http://localhost:8081/openapi/v2 保存为json就行， 获取通过curl curl localhost:8081/openapi/v2 &gt; swagger.json 运行Swagger UI通过直接跑docker 可以运行swgger ui 123456docker run \\ --rm \\ -p 80:8080 \\ -e SWAGGER_JSON=/swagger.json \\ -v $(pwd)/swagger.json:/swagger.json \\ swaggerapi/swagger-ui 生成自定义Swagger UIk8s 的swagger ui 包含了所有crd的api, 有时候只想生成自己定义的CRD的swgger ui，这时候就需要编写代码生成， 分为go生成方式跟java生成方式，推荐使用go的生成方式。 生成的方法可以参考文档2, 整体的思想就是自己定义一个apiserver, 然后把自定义的api 通过scheme注册到apiserver中。 参考文档 kubernetes: 如何查看Swagger UI OpenAPI学习笔记","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes scheme(apimachinery)原理解析","slug":"kubernetes-scheme","date":"2020-11-29T05:11:23.000Z","updated":"2021-12-17T07:42:48.339Z","comments":true,"path":"2020/11/29/kubernetes-scheme/","link":"","permalink":"https://github.com/fafucoder/2020/11/29/kubernetes-scheme/","excerpt":"","text":"综述 K8s 通过scheme注册自定义资源到api-machinery，本质上是空间换时间的一种做法，如果apimachery上没有对应的resourceVersion，直接报错，而不会再去请求k8s-apiserver apimachinery提供了resourceVersion的序列化和反序列化，kubectl 提交的资源类型可以是json格式的，也可以是yaml格式的，在apimachery中会把他解析成对应的struct结构(支持json, yaml, protobuf) apimachery还提供了编码和解码的功能，可以把指定资源的版本进行转化，例如deployment刚开始的版本(忘记是哪个版本了)是v1beta1, 之后是v1版本，通过kubectl convert可以把v1beta1版本转化为v1版本。在apimachinery的内部，都是先把resourceVersion变成internalVersion, 再有internalVersion变成目标版本。(zz_deepcopy就是为了实现Object接口，方便版本转化) 参考文档 https://blog.csdn.net/shida_csdn/article/details/83060586 https://blog.csdn.net/zhonglinzhang/article/details/105482996 https://blog.csdn.net/weixin_42663840/article/details/102558455","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"linux netstat命令","slug":"linux-netstat","date":"2020-11-17T12:19:13.000Z","updated":"2021-12-17T07:42:48.344Z","comments":true,"path":"2020/11/17/linux-netstat/","link":"","permalink":"https://github.com/fafucoder/2020/11/17/linux-netstat/","excerpt":"","text":"简介netstat 是控制台命令，是一个监控tcp/ip网络有用的工具， n用于显示各种网络相关信息， 如网络连接， 路由表，接口状态, 多播成员， masquerade连接等。一般用于检测本机端口的网络连接情况。 常用命令参数 参数 说明 -a 列出所有的连接 -t 列出所有的tcp连接 -u 列出所有的udp连接 -n 禁用域名解析 -l 仅列出有在 Listen (监听) 的服务状态。 -p 显示PID -c 持续输出，定时执行netstat命令 检查网卡状态netstat 能够列举出所有的网卡，或者看网卡详情，与此功能相同的命令为ip 或者 ethtool。 12-I, --interfaces=&lt;Iface&gt; 展示网卡详情-i, --interfaces 列出所有的网卡 通过netstat -I 参数可以统计网卡的包统计情况，可用于网卡丢包排查 123[root@xxx ~]# netstat -Ieth0Iface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 1500 4976182 0 0 0 2752891 0 0 0 BMRU 统计包信息通过netstat -s 显示网络的统计信息，包括接收到的ip/icmp/tcp/udp包数，转发的ip/icmp/tcp/udp包数等 123456789101112131415161718192021222324252627[root@node1 ~]# netstat -sIp: 321020293 total packets received 4096798 forwarded 17 with unknown protocol 0 incoming packets discarded 316923403 incoming packets delivered 326072495 requests sent out 3651 outgoing packets dropped 108 dropped because of missing routeIcmp: 315961 ICMP messages received 629 input ICMP message failed. ICMP input histogram: destination unreachable: 1208 timeout in transit: 7 echo requests: 296605 echo replies: 18140 timestamp request: 1 318614 ICMP messages sent 0 ICMP messages failed ICMP output histogram: destination unreachable: 3869 echo request: 18139 echo replies: 296605 timestamp replies: 1 ...... 列举路由表netstat能列出当前主机的路由表，曾几何时就知道查看路由表只有ip route 跟 route ,不曾想netstat也能列出路由表 netstat -r 用于列出当前主机的路由表 12345[root@xxx ~]# netstat -rDestination Gateway Genmask Flags MSS Window irtt Ifacedefault gateway 0.0.0.0 UG 0 0 0 eth0link-local 0.0.0.0 255.255.0.0 U 0 0 0 eth0192.168.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0 参考文档 netstat命令详解 Netstat 的10个基本用法","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux网络问题排查思路","slug":"linux-network","date":"2020-11-17T12:13:41.000Z","updated":"2021-12-17T07:42:48.348Z","comments":true,"path":"2020/11/17/linux-network/","link":"","permalink":"https://github.com/fafucoder/2020/11/17/linux-network/","excerpt":"","text":"网卡收包流程Linux 网卡收包流程如下： 网卡收到数据包 将数据包从网卡硬件缓存移动到服务器内存中(DMA方式，不经过CPU) 通过硬中断通知CPU处理 CPU通过软中断通知内核处理 经过TCP/IP协议栈处理 应用程序通过read()从socket buffer读取数据 网卡到内存中的问题排查网卡丢包我们先看下ifconfig的输出： 123456789# ifconfig eth0eth0: flags&#x3D;4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.5.224.27 netmask 255.255.255.0 broadcast 10.5.224.255 inet6 fe80::5054:ff:fea4:44ae prefixlen 64 scopeid 0x20&lt;link&gt; ether 52:54:00:a4:44:ae txqueuelen 1000 (Ethernet) RX packets 9525661556 bytes 10963926751740 (9.9 TiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8801210220 bytes 12331600148587 (11.2 TiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0Copy RX（receive） 代表接收报文， TX(transmit) 表示发送报文。 RX errors: 表示总的收包的错误数量，这包括 too-long-frames 错误，Ring Buffer 溢出错误，crc 校验错误，帧同步错误，fifo overruns 以及 missed pkg 等等。 RX dropped: 表示数据包已经进入了 Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。 RX overruns: 表示 fifo 的 overruns，由于 Ring Buffer(aka Driver Queue) 传输的 IO 大于 kernel 能够处理的 IO 导致的，而 Ring Buffer 则是指在发起 IRQ 请求之前的那块 buffer。很明显，overruns 的增大意味着数据包没到 Ring Buffer 就被网卡物理层给丢弃了，CPU 无法及时的处理中断是造成 Ring Buffer 满的原因之一，可能原因是因为 interruprs 分布的不均匀，没有做 affinity 而造成的丢包。 RX frame: 表示 misaligned 的 frames。 dropped 与 overruns 的区别： dropped，表示这个数据包已经进入到网卡的接收缓存 fifo 队列，并且开始被系统中断处理准备进行数据包拷贝（从网卡缓存 fifo 队列拷贝到系统内存），但由于此时的系统原因（比如内存不够等）导致这个数据包被丢掉，即这个数据包被 Linux 系统丢掉。 overruns，表示这个数据包还没有被进入到网卡的接收缓存 fifo 队列就被丢掉，因此此时网卡的 fifo 是满的。为什么 fifo 会是满的？因为系统繁忙，来不及响应网卡中断，导致网卡里的数据包没有及时的拷贝到系统内存， fifo 是满的就导致后面的数据包进不来，即这个数据包被网卡硬件丢掉。所以，如果遇到 overruns 非0，需要检测cpu负载与cpu中断情况。 环形队列Ring Buffer溢出当网卡的缓存区（ring buffer）设置的太小。网络数据包到达（生产）的速率快于内核处理（消费）的速率时， Ring Buffer 很快会被填满，新来的数据包将被丢弃。 通过 ethtool 或 /proc/net/dev 可以查看因Ring Buffer满而丢弃的包统计 12345[root@xxx ~]# ethtool -S ens2 | grep fifo rx_fifo_errors: 0 tx_fifo_errors: 0[root@xxx ~]# cat /proc/net/dev | grep ens2 ens2: 659229 8107 0 0 0 0 0 0 249827 2833 0 0 0 0 0 0 可以通过ethtool 设置ring buffer 的缓冲区大小 123456789101112# 修改网卡eth0接收与发送硬件缓存区大小$ ethtool -G eth0 rx 4096 tx 4096Pre-set maximums:RX: 4096 RX Mini: 0RX Jumbo: 0TX: 4096 Current hardware settings:RX: 4096 RX Mini: 0RX Jumbo: 0TX: 4096 中断过程中的问题什么是中断中断有两种：一种硬中断；一种软中断。硬中断是由硬件产生的，比如，像磁盘，网卡，键盘；软中断是由当前正在运行的进程所产生的。 硬中断，是一种由硬件产生的电信号直接发送到中断控制器上，然后由中断控制器向 CPU 发送信号，CPU 检测到该信号后，会中断当前的工作转而去处理中断。然后，处理器会通知内核已经产生中断，这样内核就会对这个中断进行适当的处理。 当网卡收到数据包时会产生中断请求(硬中断)通知到 CPU，CPU 会中断当前正在运行的任务，然后通知内核有新数据包，内核调用中断处理程序(软中断)进行响应，把数据包从网卡缓存及时拷贝到内存，否则会因为缓存溢出被丢弃。剩下的处理和操作数据包的工作就会交给软中断。 什么是多队列网卡当网卡不断的接收数据包，就会产生很多中断，一个中断请求只能被一个CPU处理， 而现在的机器都是用多个CPU，同时只有一个 CPU 去处理 Ring Buffer 数据会很低效，这个时候就产生了叫做 Receive Side Scaling(RSS) 或者叫做 multiqueue 的机制来处理这个问题， 这就是为啥需要多队列的原因。 RSS（Receive Side Scaling）是网卡的硬件特性，实现了多队列。通过多队列网卡驱动加载，获取网卡型号，得到网卡的硬件 queue 的数量，并结合 CPU 核的数量，最终通过 Sum=Min（网卡 queue，CPU core）得出所要激活的网卡 queue 数量。 NIC 收到 Frame 的时候能通过 Hash Function 来决定 Frame 该放在哪个 Ring Buffer 上，触发的 IRQ 也可以通过操作系统或者手动配置 IRQ affinity 将 IRQ 分配到多个 CPU 上。这样 IRQ 能被不同的 CPU 处理，从而做到 Ring Buffer 上的数据也能被不同的 CPU 处理，从而提高数据的并行处理能力。 RSS 除了会影响到 NIC 将 IRQ 发到哪个 CPU 之外，不会影响别的逻辑。 什么是RPSReceive Packet Steering(RPS) 是在 NIC 不支持 RSS 时候在软件中实现 RSS 类似功能的机制。其好处就是对 NIC 没有要求，任何 NIC 都能支持 RPS，但缺点是 NIC 收到数据后 DMA 将数据存入的还是一个 Ring Buffer，NIC 触发 IRQ 还是发到一个 CPU，还是由这一个 CPU 调用 driver 的 poll 来将 Ring Buffer 的数据取出来。RPS 是在单个 CPU 将数据从 Ring Buffer 取出来之后才开始起作用，它会为每个 Packet 计算 Hash 之后将 Packet 发到对应 CPU 的 backlog 中，并通过 Inter-processor Interrupt(IPI) 告知目标 CPU 来处理 backlog。后续 Packet 的处理流程就由这个目标 CPU 来完成。从而实现将负载分到多个 CPU 的目的。通常如果开启了RPS会加重所有 CPU 的负担. IRQ 中断请求 亲和绑定/proc/interrupts 文件中可以看到各个 CPU 上的中断情况。 /proc/irq/[irq_num]/smp_affinity_list 可以查看指定中断当前绑定的 CPU。 可以通过配置 IRQ affinity 指定 IRQ 由哪个 CPU 来处理中断, 先通过 /proc/interrupts 找到 IRQ 号之后，将希望绑定的 CPU 号写入 /proc/irq/IRQ_NUMBER/smp_affinity，写入的是 16 进制的 bit mask。比如看到队列 rx_0 对应的中断号是 41 那就执行： 12echo 6 &gt; &#x2F;proc&#x2F;irq&#x2F;41&#x2F;smp_affinity6 表示的是 CPU2 和 CPU1 0 号 CPU 的掩码是 0x1 (0001)，1 号 CPU 掩码是 0x2 (0010)，2 号 CPU 掩码是 0x4 (0100)，3 号 CPU 掩码是 0x8 (1000) 依此类推。 softirq 数统计通过 /proc/softirqs 能看到每个 CPU 上 softirq 数量统计： 123456789101112cat /proc/softirqs CPU0 CPU1 HI: 1 0 TIMER: 1650579324 3521734270 NET_TX: 10282064 10655064 NET_RX: 3618725935 2446 BLOCK: 0 0BLOCK_IOPOLL: 0 0 TASKLET: 47013 41496 SCHED: 1706483540 1003457088 HRTIMER: 1698047 11604871 RCU: 4218377992 3049934909 NET_RX 表示网卡收到包时候触发的 softirq，一般看这个统计是为了看看 softirq 在每个 CPU 上分布是否均匀，不均匀的话可能就需要做一些调整。比如上面看到 CPU0 和 CPU1 两个差距很大，原因是这个机器的 NIC 不支持 RSS，没有多个 Ring Buffer。开启 RPS 后就均匀多了。 如何开启RPSRPS 默认是关闭的，当机器有多个 CPU 并且通过 softirqs 的统计 /proc/softirqs 发现 NET_RX 在 CPU 上分布不均匀或者发现网卡不支持 mutiqueue 时，就可以考虑开启 RPS。 开启 RPS 需要调整 /sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus 的值。比如执行: 1echo f &gt; &#x2F;sys&#x2F;class&#x2F;net&#x2F;eth0&#x2F;queues&#x2F;rx-0&#x2F;rps_cpus 表示的含义是处理网卡 eth0 的 rx-0 队列的 CPU 数设置为 f 。即设置有 15 个 CPU 来处理 rx-0 这个队列的数据，如果你的 CPU 数没有这么多就会默认使用所有 CPU 。 netdev_max_backlog调优netdev_max_backlog 是内核从 NIC 收到包后，交由协议栈（如 IP、TCP ）处理之前的缓冲队列, 通过softnet_stat可以确定是否发生了netdev backlog队列溢出 1234567891011121314151617[root@xxx ~]# cat &#x2F;proc&#x2F;net&#x2F;softnet_stat000000bf 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000028 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000000000c7 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000031 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000000021d8 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 0000000000000929 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 每一行代表每个 CPU 核的状态统计，从 CPU0 依次往下。 每一列代表一个 CPU 核的各项统计：第一列代表中断处理程序收到的包总数；第二列即代表由于 netdev_max_backlog 队列溢出而被丢弃的包总数。 第3列表示软中断一次取走netdev_budget个数据包，或取数据包时间超过2ms的次数。 第4~8列固定为0，没有意义。 第9列表示发送数据包时，对应的队列被锁住的次数。 netdev_max_backlog 的默认值是 1000，我们可以修改内核参数来调优： 1sysctl -w net.core.netdev_max_backlog=2000 协议栈过程中的问题socket buffer过程中的问题参考文档 网络协议栈收包过程 关于网卡中断不均衡问题及其解决方案 Linux 丢包那些事 网络性能优化之套路篇","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux-nic2","slug":"linux-nic2","date":"2020-11-17T12:13:32.000Z","updated":"2021-12-17T07:42:48.349Z","comments":true,"path":"2020/11/17/linux-nic2/","link":"","permalink":"https://github.com/fafucoder/2020/11/17/linux-nic2/","excerpt":"","text":"网卡收发包流程参考文档 https://ylgrgyq.github.io/2017/07/24/linux-receive-packet-2/ https://ylgrgyq.github.io/2017/08/01/linux-receive-packet-3/","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"git 常用短语","slug":"git-comment","date":"2020-11-17T11:59:44.000Z","updated":"2021-12-17T07:42:48.324Z","comments":true,"path":"2020/11/17/git-comment/","link":"","permalink":"https://github.com/fafucoder/2020/11/17/git-comment/","excerpt":"","text":"git常见短语 PR: Pull Request 拉取请求，给其他项目提交代码 LGTM: Looks Good To Me. 朕知道了 代码已经过 review，可以合并 SGTM: Sounds Good To Me. 和上面那句意思差不多，也是已经通过了 review 的意思 WIP: Work In Progress. 如果你有个改动很大的 PR，可以在写了一部分的情况下先提交，但是在标题里写上 WIP，以告诉项目维护者这个功能还未完成，方便维护者提前 review 部分提交的代码。 PTAL: Please Take A Look. 帮我看下，一般都是请别人 review 自己的 PR RFC: request for comments. 我觉得这个想法很好, 我们来一起讨论下 TBR: To Be Reviewed. 提示维护者进行 review TL;DR: Too Long; Didn&#39;t Read. 太长懒得看。也有很多文档在做简略描述之前会写这么一句 TBD: To Be Done(or Defined/Discussed/Decided/Determined). 根据语境不同意义有所区别，但一般都是还没搞定的意思 CC: Carbon copy 一般代表抄送别人的意思 IIRC: if I recall correctly. 如果我没记错 ACK: acknowledgement. 我确认了或者我接受了,我承认了 NACK/NAK: negative acknowledgement. 我不同意","categories":[{"name":"git","slug":"git","permalink":"https://github.com/fafucoder/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://github.com/fafucoder/tags/git/"}]},{"title":"kubernetes 资源分配之 request 和 limit","slug":"kubernetes-qos","date":"2020-11-05T11:46:27.000Z","updated":"2021-12-17T07:42:48.338Z","comments":true,"path":"2020/11/05/kubernetes-qos/","link":"","permalink":"https://github.com/fafucoder/2020/11/05/kubernetes-qos/","excerpt":"","text":"request 容器使用的最小资源需求， 作为容器调度时资源分配的判断依据 只有当前节点上可分配的资源 &gt;= request 才允许将容器调度到该节点 request参数不限制容器的最大可用资源 limit 容器能使用资源的最大值 设置为0表示对使用的资源不做限制，可无限的使用 request 跟 limit 的关系request能保证pod有足够的资源来运行, 而limit则是防止某个pod无限制的使用资源, 导致其他pod崩溃. 两者的关系必须满足: 10 &lt;= request &lt;= limit &lt;= Infinity kubernetes Qos类型container request值表示容器保证可被分配到资源。containerlimit表示容器可允许使用的最大资源。Pod级别的request和limit是其所有容器的request和limit之和。 在机器资源超卖的情况下（limit的总量大于机器的资源容量），即CPU或内存耗尽，将不得不杀死(驱逐)部分不重要的容器。kubernetes通过kubelet来进行回收策略控制，保证节点上pod在节点资源比较小时可以稳定运行。 对于CPU，如果pod中服务使用CPU超过设置的limits，pod不会被kill掉但会被限制。如果没有设置limits，pod可以使用全部空闲的cpu资源。 对于内存，当一个pod使用内存超过了设置的limits，pod中container的进程会被kernel因OOM kill掉。当container因为OOM被kill掉时，系统倾向于在其原所在的机器上重启该container或本机或其他重新创建一个pod。 在Kubernetes中，pod的QoS级别包括：Guaranteed, Burstable*与 *Best-Effort ，三个级别的优先级依次递减。 Guaranteed所有的容器的limit值和request值被配置且两者相等（如果只配置limit没有request，则request取值于limit）。 12345678910111213141516171819202122232425262728293031# 示例1containers: name: foo resources: limits: cpu: 10m memory: 1Gi name: bar resources: limits: cpu: 100m memory: 100Mi# 示例2containers: name: foo resources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Gi name: bar resources: limits: cpu: 100m memory: 100Mi requests: cpu: 100m memory: 100Mi Burstable如果一个或多个容器的limit和request值被配置且两者不相等。 12345678910111213141516171819202122232425262728293031323334# 示例1containers: name: foo resources: limits: cpu: 10m memory: 1Gi requests: cpu: 10m memory: 1Gi name: bar# 示例2containers: name: foo resources: limits: memory: 1Gi name: bar resources: limits: cpu: 100m# 示例3containers: name: foo resources: requests: cpu: 10m memory: 1Gi name: bar Best-Effort所有的容器的limit和request值都没有配置。 12345containers: name: foo resources: name: bar resources: 参考文档 https://cloud.tencent.com/developer/article/1004976 https://www.cnblogs.com/sunsky303/p/12626608.html https://www.huweihuang.com/kubernetes-notes/resource/quality-of-service.html","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"arp地址解析协议","slug":"linux-arp","date":"2020-10-09T11:45:22.000Z","updated":"2021-12-17T07:42:48.340Z","comments":true,"path":"2020/10/09/linux-arp/","link":"","permalink":"https://github.com/fafucoder/2020/10/09/linux-arp/","excerpt":"","text":"概念ARP(Address Resolution Protocol) 即地址解析协议，用于实现从IP地址到MAC地址映射。 ARP工作流程相同网段 PC1 要和PC3 通行，首先查看自己的ARP表，查看其中是否包含PC3的MAC地址信息，如果找到对应关系，直接利用ARP表中的MAC地址对IP数据包进行封装。并将数据包发送给PC3。 如果PC1在ARP表中未找到PC3对应的MAC地址，则先缓存数据报文，然后利用广播方式（目标MAC地址FF:FF:FF:FF:FF:FF)发送一个ARP报文请求，ARP请求中的发送端MAC地址分别是PC1的IP地址和MAC地址，接收端的IP地址为PC3的IP地址，MAC地址全为0，因为ARP请求报文是以广播方式发送，所以该网段上的所有主机都可以接收到该请求包，但只有其IP地址与目的IP地址一致的PC3才会对该请求进行处理。 PC3将ARP请求报文中的发送端（即PC1）的IP地址和MAC地址存入自己的ARP表中。然后以单播方式向PC1发送一个ARP相应报文，应答报文中就包含了自己的MAC地址，也就是原来在请求报文中要请求的目的MAC地址。 PC1收到来自PC3的ARP响应报文之后，将PC3的MAC地址加入到自己的ARP表中以用于后续报文的转发，同时将原来缓存的IP数据包再次修改（在目的MAC地址字段填上PC3的MAC地址）后发送出去。 跨网段 如果主机A不知道网关的MAC地址（也就是主机A的ARP表中没有网关对应的MAC地址表项），则主机A先在本网段中发出一个ARP请求广播，ARP请求报文中的目的IP地址为网关的IP地址,代表其目的就是想获得网关的MAC地址。如果主机A已经知道网关的MAC地址，则略过此步。 网关收到ARP广播包后同样会向主机A发回一个ARP应答包。当主机A收到的应答包中获得网关的MAC地址后，在主机A向主机B发送的原报文的目的MAC地址字段填上网关的MAC地址后发给网关。(目的IP还是主机B) 如果网关的ARP表中已有主机B对应的MAC地址，则网关直接将在来自主机A的报文中的目的MAC地址字段填上主机B的MAC地址后转发给B。 如果网关ARP表中没有主机B的MAC地址，网关会再次向主机B所在的网段发送ARP广播请求，此时目的IP地址为主机B的IP地址，当网关从收到来自主机B的应答报文中获得主机B的MAC地址后，就可以将主机A发来的报文重新再目的MAC地址字段填上主机B的MAC地址后发送给主机B。 ARP协议格式 以太网目的地址: 目的主机的硬件地址。目的地址全为1表示广播地址 以太网源地址：源主机的硬件地址 帧类型：ARP：0x0806、 RARP：0x8035 协议类型：IP类型：0x0800 硬件地址长度：对于以太网II来说，MAC地址作为硬件地址，因此该字段值为十六进制06 协议地址长度：对于IPv4来，IP地址长度位32个字节，因此该字段值为十六进制04 操作类型：ARP定义了两种操作：0x0001(请求)、0x0002(应答) 发送端以太网地址：对于以太网II来说，MAC地址作为硬件地址 发送端IP地址：对于IPv4来， 值为IPv4地址 目的以太网地址：对于以太网II来说，MAC地址作为硬件地址 目的IP地址：对于IPv4来， 值为IPv4地址 常见命令arparp 命令用于显示和修改 IP 到 MAC 转换表 使用方法如下： 1-a : 显示 arp 缓冲区的所有条目 arpingarping用来向局域网内的其它主机发送ARP请求，它可以用来测试局域网内的某个IP是否已被使用。 使用方法如下： 12345678910111213Usage: arping [-fqbDUAV] [-c count] [-w timeout] [-I device] [-s source] destination -f : quit on first reply （收到第一个响应包后退出) -q : be quiet（静默模式） -b : keep broadcasting, don&#39;t go unicast （发送以太网广播帧，arping在开始时使用广播地址，在收到回复后使用unicast单播地址） -D : duplicate address detection mode (重复地址探测模式，用来检测有没有IP地址冲突，如果没有IP冲突则返回0) -U : Unsolicited ARP mode, update your neighbours (无理由的&#x2F;强制的 ARP模式去更新别的主机上的ARP CACHE列表中的本机的信息，不需要响应。) -A : ARP answer mode, update your neighbours (与-U参数类似，但是使用的是ARP REPLY包而非ARP REQUEST包) -V : print version and exit （打印版本并退出) -c count : how many packets to send (指定数量) -w timeout : how long to wait for a reply （等待时间） -I device : which ethernet device to use （指定网卡） -s source : source ip address （源IP） destination : ask for what ip address 参考文档 https://www.cnblogs.com/onlycat/p/11340872.html#/cnblog/works/article/11340872","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"socket套接字","slug":"linux-socket","date":"2020-10-09T11:45:13.000Z","updated":"2021-12-17T07:42:48.351Z","comments":true,"path":"2020/10/09/linux-socket/","link":"","permalink":"https://github.com/fafucoder/2020/10/09/linux-socket/","excerpt":"","text":"参考文档 https://www.debuginn.cn/5917.html https://cloud.tencent.com/developer/article/1162927 https://cloud.tencent.com/developer/article/1162934","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux awk, grep, sed命令","slug":"linux-awk","date":"2020-09-20T13:40:22.000Z","updated":"2021-12-17T07:42:48.340Z","comments":true,"path":"2020/09/20/linux-awk/","link":"","permalink":"https://github.com/fafucoder/2020/09/20/linux-awk/","excerpt":"","text":"概述awk、grep、sed 是 linux 操作文本的三大利器， 三者的功能都是处理文本，但侧重点各不相同。 grep适合单纯的查找或匹配文本，sed适合编辑匹配到的文本，awk适合格式化文本，对文本进行较复杂格式处理。 简单概述为： grep：数据查找定位 awk：数据切片 sed：数据修改 grepgrep(全局正则表达式打印)–命令用于查找文件里符合条件的字符串。 从文件的第一行开始，grep 将一行复制到 buffer 中，将其与搜索字符串进行比较，如果比较通过，则将该行打印到屏幕上。grep将重复这个过程，直到文件搜索所有行。 常见参数： -n –line-number #在显示符合样式的那一行之前，标示出该行的列数编号。 grep -n license package.json -v –revert-match #显示不包含匹配文本的所有行。 -c –count #计算符合样式的列数。 -l –file-with-matches #列出文件内容符合指定的样式的文件名称。 grep -l license * -i –ignore-case #忽略字符大小写的差别。 -x –line-regexp #只显示全列符合的列。 grep -x &quot;boo&quot; sampler.log -s –no-messages #不显示错误信息。 grep -ls license * -H –with-filename #在显示符合样式的那一行之前，表示该行所属的文件名称。 -C –context #显示匹配行和它前后各n行 grep -C 5 hello package.json -A –after #显示匹配行和它后n行 grep -A 5 hello package.json -B –before #显示匹配行和它前n行 grep -B 5 hello package.json awkawk 程序对输入文件的每一行进行操作。它可以有一个可选的 BEGIN{ } 部分在处理文件的任何内容之前执行的命令，然后主{ }部分运行在文件的每一行中，最后还有一个可选的END{ }部分操作将在后面执行文件读取完成: 123BEGIN &#123; …. initialization awk commands …&#125;&#123; …. awk commands for each line of the file…&#125;END &#123; …. finalization awk commands …&#125; 对于输入文件的每一行，它会查看是否有任何模式匹配指令，在这种情况下它仅在与该模式匹配的行上运行，否则它在所有行上运行。 这些 ‘pattern-matching’ 命令可以包含与 grep 一样的正则表达式。 类似数据库列的概念，但它是按照序号来指定的，比如我要第一个列是1， 第二列是2，依此类推。$0就是输出整个文本的内容。默认用空格作为分隔符，可以自己通过-F设置适合自己情况的分隔符。 例如：awk &#39;{print $1}&#39; demo.txt sedsed执行时，会从文件或者标准输入中读取一行，将其复制到缓冲区，对文本编辑完成之后，读取下一行直到所有的文本行都编辑完毕。 所以sed命令处理时只会改变缓冲区中文本的副本，如果想要直接编辑原文件，可以使用-i选项或者将结果重定向到新的文件中。 常见命令: -e ：直接在命令列模式上进行 sed 的动作编辑。 -i ：直接修改读取的文件内容，而不是输出到终端。 -f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作。 -n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 其他命令: 一般形式是 1sed -e &#39;&#x2F;pattern&#x2F; command&#39; sampler.log 其中 ‘pattern’ 是正则表达式，’command’ 可以是: a(append) ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d(delete) ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i(insert) ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p(print) ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s(search＆replace) ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 示例： 删除:12345678910111213141516171819202122# 删除指定行(第三行)sed -e &#39;3d&#39; demo.txt➜ indigo git:(master) ✗ sed &#39;2d&#39; demo.txt Twinkle, twinkle, little starUp above the world so highLike a diamond in the skyWhen the blazing sun is gone# 删除范围行(2-4行)sed -e &#39;2,4d&#39; demo.txt➜ indigo git:(master) ✗ sed &#39;2,4d&#39; demo.txtTwinkle, twinkle, little starWhen the blazing sun is gone# 到结尾行sed -e &#39;2,$d&#39; demo.txt➜ indigo git:(master) ✗ sed &#39;3,$d&#39; demo.txtTwinkle, twinkle, little starHow I wonder what you are 参考文档 https://cloud.tencent.com/developer/article/1465475 https://juejin.im/post/6845166891493785613","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux I/O多路复用技术","slug":"linux-io","date":"2020-09-14T12:22:13.000Z","updated":"2021-12-17T07:42:48.342Z","comments":true,"path":"2020/09/14/linux-io/","link":"","permalink":"https://github.com/fafucoder/2020/09/14/linux-io/","excerpt":"","text":"什么是文件描述符FD 阅读文件描述符之前，推荐先看 之前进程，线程，协程的文章 Linux 系统中，把一切都看做是文件，当进程打开现有文件或创建新文件时，内核向进程返回一个文件描述符，文件描述符就是内核为了高效管理已被打开的文件所创建的索引，用来指向被打开的文件，所有执行I/O操作的系统调用都会通过文件描述符。文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。 在linux中，用task_struct 表示一个进程： 123456789101112131415struct task_struct &#123; long state; // 虚拟内存结构体 struct mm_struct *mm; // 进程内存管理信息 pid_t pid; // 进程号 struct task_struct *parent; // 指向父进程的指针 struct list_head children; // 子进程列表 struct fs_struct* fs; // 存放文件系统信息的指针 struct files_struct *files; // 一个数组，包含该进程打开的文件指针&#125; 一个task_struct包含很多选项，其中包括两个选项: mm：指向的是进程的虚拟内存，也就是载入资源和可执行文件的地方； files：指针指向一个数组，这个数组里装着所有该进程打开的文件的指针。 文件描述符与文件的详细关系files是一个文件指针数组。一般来说，一个进程会从files[0]读取输入，将输出写入files[1]，将错误信息写入files[2]。 每个进程被创建时，files的前三位被填入默认值，分别指向标准输入流、标准输出流、标准错误流。我们常说的「文件描述符」就是指这个文件指针数组的索引 ，所以程序的文件描述符默认情况下 0 是输入，1 是输出，2 是错误。( 此处你应该查看这篇文章 ) 如果我们写的程序需要其他资源，比如打开一个文件进行读写，这也很简单，进行系统调用，让内核把文件打开，这个文件就会被放到files的第 4 个位置，对应文件描述符 3： 在操作系统中，为了维护文件描述符，建立了三个表(三个数据结构), 分别为进程级的文件描述符表, 系统级的文件描述符表, 文件系统的i-node表，不同表的记录内容如下所示： 下图展示的是文件描述符与打开的文件和i-node之间的关系: 在进程A中，文件描述符 fd 1和fd 30都指向了同一个打开的文件句柄（#23），这可能是通过调用dup()、dup2()、fcntl()或者对同一个文件多次调用了open()函数而形成的。 进程A中的文件描述符fd 2和进程B的文件描述符fd 2都指向了同一个打开的文件句柄（#73），这种情况有几种可能： 1): 进程A和进程B可能是父子进程关系(调用 fork() 后出现); 2): 进程A和进程B都调用open函数打开了同一个文件，此时描述符恰好一致 (低概率事件); 3): 进程A和进程B中某个进程通过UNIX域套接字将一个打开的文件描述符传递给另一个进程。 进程A的描述符fd 0和进程B的描述符fd 3分别指向不同的打开文件句柄，但这些句柄均指向i-node表的相同条目（#1936）换言之，指向同一个文件。发生这种情况是因为每个进程各自对同一个文件发起了打开请求。（同一个进程两次打开同一个文件，也会发生类似情况。） 文件描述符的限制在编写文件操作的或者网络通信的软件时，初学者一般可能会遇到“Too many open files”的问题。这主要是因为文件描述符是系统的一个重要资源，虽然说系统内存有多少就可以打开多少的文件描述符，但是在实际实现过程中内核是会做相应的处理的，一般最大打开文件数会是系统内存的10%(称之为系统级限制)，与此同时，内核为了不让某一个进程消耗掉所有的文件资源，其也会对单个进程最大打开文件数做默认值处理（称之为用户级限制），默认值一般是1024。 查看系统级别的最大打开文件数可以使用sysctl -a | grep fs.file-max命令查看；查看用户级别的最大打开文件数可以使用使用 ulimit -n命令查看； socket 概念socket这个词可以表示很多概念，在TCP/IP协议中“IP地址 + TCP或UDP端口号”唯一标识网络通讯中的一个进程，“IP + 端口号”就称为socket。在TCP协议中，建立连接的两个进程各自有一个socket来标识，那么两个socket组成的socket pair就唯一标识一个连接。 在Unix/Linux系统下，一个socket句柄，可以看做是一个文件，在socket上收发数据，相当于对一个文件进行读写，所以一个socket句柄，通常也用表示文件句柄的fd来表示。在linux中，一个socket fd 可能如下表示： 1234root@ubuntu:~# ll /proc/1583/fd total 0 lrwx------ 1 root root 64 Jul 19 12:37 7 -&gt; socket:[18892] lrwx------ 1 root root 64 Jul 19 12:37 8 -&gt; socket:[18893] 这里我们看到 fd 7、8 都是一个 socket fd，名字为 socket:[18892]、socket:[18893], 名字中的socket 标识这是一个 socket 类型的 fd， 数字[18892] 表示这个是一个 inode 号，能够唯一标识本机的一条网络连接。 套接字由 socket() 系统调用创建，但是套接字其实可分为两种类型，监听套接字和普通套接字。而监听套接字是由 listen() 把 socket fd 转化而成。listenfd称为监听套接字（listening socket）描述符，它的生命周期与服务器的生命周期一致且一般一个服务器保有一个listenfd。connfd是已连接套接字（connected socket）描述符，一个连接对应一个connfd，当连接关闭时connfd也会被关闭。值得一提的是connfd是在完成TCP三次握手后被创建。(总结起来就是listenfd是内核维护的，connfd是应用程序自己维护的) 进程堵塞 概念多路复用解决的问题参考文档 https://wumingx.com/performance/io-epoll.html https://xie.infoq.cn/article/5241a48ffdb62ec3ba0235733 百万 Go TCP 连接的思考: epoll方式减少资源占用 // 鸟窝大神 I/O多路复用 // 这老哥的文章不错哈，值得阅读 文件描述符（File Descriptor）简介 // 文件描述符的相关知识， 图画的不错~ Linux fd 系列 — socket fd 是什么？ // 这篇文章写的属实不错，学习了~","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"http1.x，http2.0, http3.0功能起底","slug":"linux-http2","date":"2020-09-14T11:52:55.000Z","updated":"2021-12-17T07:42:48.342Z","comments":true,"path":"2020/09/14/linux-http2/","link":"","permalink":"https://github.com/fafucoder/2020/09/14/linux-http2/","excerpt":"","text":"HTTP协议超文本传输协议（英文：HyperText Transfer Protocol，缩写：HTTP）是互联网上应用最为广泛的一种网络协议。设计HTTP最初的目的是为了提供一种发布和接收HTML页面的方法。通过HTTP或者HTTPS协议请求的资源由统一资源标识符（URI）来标识。 HTTP是一个应用层协议，由请求和响应构成，是一个标准的客户端服务器模型。HTTP是基于TCP/IP的无连接，无状态的协议(每一个HTTP报文不依赖于其前面的报文状态)。HTTP假定其下层协议提供可靠的传输。因此也就是其在TCP/IP协议族使用TCP作为其传输层。 HTTP协议的主要特点可概括如下： 简单：客户向服务器请求服务时，只需传送请求方法和路径。请求方法常用的有GET、HEAD、POST。每种方法规定了客户与服务器联系的不同类型。 灵活：HTTP允许传输任意类型的数据对象。正在传输的类型由Content-Type加以标记。 请求-响应模式：客户端每次向服务器发起一个请求时都建立一个连接， 服务器处理完客户的请求即断开连接。 无状态：HTTP协议是无状态协议。无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传。 HTTP工作过程HTTP由请求和响应构成，是一个标准的客户端服务器模型（B/S）。HTTP协议永远都是客户端发起请求，服务器回送响应 1、客户机（浏览器）主动向服务器（web server)发出连接请求(这一步骤可能需要DNS解析协议来获取服务器的IP地址)。 2、服务器接受连接请求并建立起连接。 (1,2步即我们所熟知的TCP三次握手) 3、客户机通过此连接向服务器发出GET等http命令，(“HTTP请求报文”)。 4、服务器接到命令并根据命令向客户机传送相应的数据，(“HTTP响应报文”)。 5、客户机接收从服务器送过来的数据。 6、服务器发送完数据后，主动关闭此次连接。 （”TCP四次分手“）。 概况起来就是 客户/服务器传输过程可分为四个基本步骤： 1) 浏览器与服务器建立连接； (TCP三次握手) 2) 浏览器向服务器HTTP请求报文； 3) 服务器响应浏览器请求； 4) 断开连接。（”TCP四次分手“） HTTP1.X 原理在http1.0的工作过程中，http传输数据时都要经历三次握手，四次挥手，增加了延时 在http1.1加入了keep-alive来保持tcp连接不断开，减少了部分延时 HTTP 1.x存在的问题 连接无法复用 连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对大量小文件请求影响较大。 队头堵塞HOLB 队头堵塞Head-Of-Line Blocking（HOLB）导致带宽无法被充分利用，以及后续健康请求被阻塞。HOLB是指一系列包（package）因为第一个包被阻塞；当页面中需要请求很多资源的时候，HOLB（队头阻塞）会导致在达到最大请求数量时，剩余的资源需要等待其他资源请求完成后才能发起请求。针对队头阻塞,人们尝试过以下办法来解决: HTTP 1.0：下个请求必须在前一个请求返回后才能发出，request-response对按序发生。显然，如果某个请求长时间没有返回，那么接下来的请求就全部阻塞了。 HTTP 1.1：尝试使用 pipeling 来解决，即浏览器可以一次性发出多个请求（同个域名，同一条 TCP 链接）。但 pipeling 要求返回是按序的，那么前一个请求如果很耗时（比如处理大图片），那么后面的请求即使服务器已经处理完，仍会等待前面的请求处理完才开始按序返回。所以，pipeling 只部分解决了 HOLB。 引入雪碧图、将小图内联等 请求头开销大 由于报文Header一般会携带”User Agent””Cookie””Accept””Server”等许多固定的头字段（如下图），多达几百字节甚至上千字节,但Body却经常只有几十字节. Header 内容大在一定程度上增加了传输的成本. 安全因素 HTTP 1.x在传输数据时，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份，这在一定程度上无法保证数据的安全性。 不支持服务端推送消息 因为 HTTP/1.x 并没有推送机制。所以通常两种做法， 一是轮询，客户端定时轮询，二是websocket 2.0 工作原理2015年，HTTP/2 发布。HTTP/2是现行HTTP协议（HTTP/1.x）的替代，但它不是重写，HTTP方法/状态码/语义都与HTTP/1.x一样。HTTP/2基于SPDY，专注于性能，最大的一个目标是在用户和网站间只用一个连接（connection）。 HTTP2解决的问题针对http1.x存在的问题，http2针对的解决方案如下： 二进制传输 HTTP/2 采用二进制格式传输数据，而非 HTTP 1.x 的文本格式，二进制协议解析起来更高效。 HTTP / 1 的请求和响应报文，都是由起始行，首部和实体正文（可选）组成，各部分之间以文本换行符分隔。HTTP/2 将请求和响应数据分割为更小的帧，并且它们采用二进制编码。 在了解 HTTP/2 之前，需要知道一些通用术语： Stream： 一个双向流，一条连接可以有多个 streams。 Message： 也就是逻辑上面的 request，response。 Frame:：数据传输的最小单位。每个 Frame 都属于一个特定的 stream 或者整个连接。一个 message 可能有多个 frame 组成。Frame 是 HTTP/2 里面最小的数据传输单位，一个 Frame 定义如下: Length：也就是 Frame 的长度，默认最大长度是 16KB，如果要发送更大的 Frame，需要显式的设置 max frame size。 Type：Frame 的类型，譬如有 DATA，HEADERS，PRIORITY 等。 Flag 和 R：保留位。 Stream Identifier：标识所属的 stream，如果为 0，则表示这个 frame 属于整条连接。 Frame Payload：根据不同 Type 有不同的格式。 Stream 有很多重要特性： 一条连接可以包含多个 streams，多个 streams 发送的数据互相不影响。 Stream 可以被 client 和 server 单方面或者共享使用。 Stream 可以被任意一段关闭。 Stream 会确定好发送 frame 的顺序，另一端会按照接受到的顺序来处理 Stream 用一个唯一 ID 来标识。如果是 client 创建的 stream，ID 就是奇数，如果是 server 创建的，ID 就是偶数。 二进制传输把TCP协议的部分特性挪到了应用层，把原来的”Header+Body”的消息”打散”为数个小片的二进制”帧”(Frame),用”HEADERS”帧存放头数据、”DATA”帧存放实体数据。HTP/2数据分帧后”Header+Body”的报文结构就完全消失了，协议看到的只是一个个的”碎片”。 HTTP/2 中，同域名下所有通信都在单个连接上完成，该连接可以承载任意数量的双向数据流。每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装。 Header压缩 在 HTTP/1 中，由于使用文本的形式传输 header，在 header 携带 cookie和user agent 的情况下，可能每次都需要重复传输几百到几千的字节。HTTP/2没有使用传统的压缩算法，而是开发了专门的”HPACK”算法，在客户端和服务器两端建立“字典”，用索引号表示重复的字符串，采用哈夫曼编码来压缩整数和字符串，可以达到50%~90%的高压缩率。 具体如下： HTTP/2 在客户端和服务器端使用“首部表”来跟踪和存储之前发送的键－值对，对于相同的数据，不再通过每次请求和响应发送； 首部表在 HTTP/2 的连接存续期内始终存在，由客户端和服务器共同渐进地更新; 每个新的首部键－值对要么被追加到当前表的末尾，要么替换表中之前的值 多路复用(请求和响应复用) 同个域名只需要占用一个 TCP 连接， 在一个 TCP 连接上，我们可以向对方不断发送帧，每帧的 stream identifier 的标明这一帧属于哪个流，然后在对方接收时，根据 stream identifier 拼接每个流的所有帧组成一整块数据。 上面协议解析中提到的stream id就是用作连接共享机制的.一个request对应一个stream并分配一个id，这样一个连接上可以有多个stream，每个stream的frame可以随机的混杂在一起，接收方可以根据stream id将frame再归属到各自不同的request里面。请求和响应之间互相不影响 多路复用（MultiPlexing），这个功能相当于是长连接的增强，每个request可以随机的混杂在一起，接收方可以根据request的id将request再归属到各自不同的服务端请求里面。 多路复用中，支持流的优先级（Stream dependencies），允许客户端告诉server哪些内容是更优先级的资源，可以优先传输。 服务器端推送 HTTP/2 新增的另一个强大的新功能是，服务器可以对一个客户端请求发送多个响应。 换句话说，除了对最初请求的响应外，服务器还可以向客户端推送额外资源，而无需客户端明确地请求。 服务端可以主动推送，客户端也有权利选择是否接收。如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送RST_STREAM帧来拒收。 主动推送遵守同源策略, 服务器不能随便将第三方资源推送给客户端，而必须是经过双方确认才行。 提高安全性 HTTP/2延续了HTTP/1的“明文”特点，可以像以前一样使用明文传输数据，不强制使用加密通信，不过格式还是二进制，只是不需要解密。 由于HTTPS已经是大势所趋，而且主流的浏览器Chrome、Firefox等都公开宣布只支持加密的HTTP/2，所以“事实上”的HTTP/2是加密的。也就是说，互联网上通常所能见到的HTTP/2都是使用”https”协议名，跑在TLS上面。HTTP/2协议定义了两个字符串标识符：“h2”表示加密的HTTP/2，“h2c”表示明文的HTTP/2。 HTTP2存在的问题 http1.x和http2都是基于tcp, tcp作为安全的可靠的传输协议，建立连接的延时还是有点高。主要是TCP 以及 TCP+TLS建立连接的延时。 无法解决tcp的队头堵塞问题，http2通过多路复用解决http层的对头堵塞，tcp的失败重传的问题还是无解的 HTTP3.0 原理因为TCP 存在的时间实在太长，已经充斥在各种设备中，并且这个协议是由操作系统实现的，更新起来不大现实。 基于这个原因，Google 就更起炉灶搞了一个基于 UDP 协议的 QUIC 协议，并且使用在了 HTTP/3 上，HTTP/3 之前名为 HTTP-over-QUIC，从这个名字中我们也可以发现，HTTP/3 最大的改造就是使用了 QUIC。QUIC（Quick UDP Internet Connections，快速 UDP 网络连接） 基于 UDP, 正是看中了 UDP 的速度与效率。同时 QUIC 也整合了 TCP、TLS 和 HTTP/2 的优点，并加以优化。 QUIC的次要目标包括减少连接和传输延迟，在每个方向进行带宽估计以避免拥塞。它还将拥塞控制算法移动到用户空间，而不是内核空间，此外使用前向纠错（FEC）进行扩展，以在出现错误时进一步提高性能。 HTTP3实现的功能 实现快速握手功能 HTTP/2 的连接需要 3 RTT，如果考虑会话复用，即把第一次握手算出来的对称密钥缓存起来，那么也需要 2 RTT，更进一步的，如果 TLS 升级到 1.3，那么 HTTP/2 连接需要 2 RTT，考虑会话复用则需要 1 RTT。而 HTTP/3 首次连接只需要 1 RTT，后面的连接更是只需 0 RTT。 使用QUIC协议的客户端和服务端要使用1RTT进行密钥交换，使用的交换算法是DH(Diffie-Hellman)迪菲-赫尔曼算法。 首次连接时客户端和服务端的密钥协商和数据传输过程，其中涉及了DH算法的基本过程： 客户端对于首次连接的服务端先发送client hello请求。 服务端生成一个素数p和一个整数g，同时生成一个随机数为私钥a，然后计算出公钥A = mod p，服务端将A，p，g三个元素打包称为config，后续发送给客户端。 客户端随机生成一个自己的私钥b，再从config中读取g和p，计算客户端公钥B = mod p。然后客户端根据 A、p、b 算出初始密钥 K。B 和 K 算好后，客户端会用 K 加密 HTTP 数据，连同 B 一起发送给服务端； 客户端使用自己的私钥b和服务端发来的config中读取的服务端公钥，生成后续数据加密用的密钥K = mod p。 客户端使用密钥K加密业务数据，并追加自己的公钥，都传递给服务端。 服务端根据自己的私钥a和客户端公钥B生成客户端加密用的密钥K = mod p。 为了保证数据安全，上述生成的密钥K只会生成使用1次，后续服务端会按照相同的规则生成一套全新的公钥和私钥，并使用这组公私钥生成新的密钥M。 服务端将新公钥和新密钥M加密的数据发给客户端，客户端根据新的服务端公钥和自己原来的私钥计算出本次的密钥M，进行解密。 之后的客户端和服务端数据交互都使用密钥M来完成，密钥K只使用1次。 客户端和服务端首次连接时服务端传递了config包，里面包含了服务端公钥和两个随机数，客户端会将config存储下来，后续再连接时可以直接使用，从而跳过这个1RTT，实现0RTT的业务数据交互。 客户端保存config是有时间期限的，在config失效之后仍然需要进行首次连接时的密钥交换。 集成TLS加密功能 向前安全 前向安全指的是密钥泄漏也不会让之前加密的数据被泄漏，影响的只有当前，对之前的数据无影响。 QUIC使用config的有效期，后续又生成了新密钥，使用其进行加解密，当时完成交互时则销毁，从而实现了前向安全。 向前纠错 前向纠错也叫前向纠错码Forward Error Correction 简称FEC 是增加数据通讯可信度的方法，在单向通讯信道中，一旦错误被发现，其接收器将无权再请求传输。 FEC 是利用数据进行传输冗余信息的方法，当传输中出现错误，将允许接收器再建数据。 QUIC每发送一组数据就对这组数据进行异或运算，并将结果作为一个FEC包发送出去，接收方收到这一组数据后根据数据包和FEC包即可进行校验和纠错。一段数据被切分为 10 个包后，依次对每个包进行异或运算，运算结果会作为 FEC 包与数据包一起被传输，如果不幸在传输过程中有一个数据包丢失，那么就可以根据剩余 9 个包以及 FEC 包推算出丢失的那个包的数据，这样就大大增加了协议的容错性。 多路复用，解决tcp中存在的队头堵塞问题 HTTP2.0协议的多路复用机制解决了HTTP层的队头阻塞问题，但是在TCP层仍然存在队头阻塞问题。 TCP协议在收到数据包之后，这部分数据可能是乱序到达的，但是TCP必须将所有数据收集排序整合后给上层使用，如果其中某个包丢失了，就必须等待重传，从而出现某个丢包数据阻塞整个连接的数据使用。 QUIC协议是基于UDP协议实现的，在一条链接上可以有多个流，流与流之间是互不影响的，当一个流出现丢包影响范围非常小，从而解决队头阻塞问题。 实现类似TCP的流量控制(…这部分内容多，看参考) 参考文档 五分钟读懂http3 HTTP/3 原理实战 图解为什么http3使用udp协议 科普：QUIC协议原理分析 解密http/2与http/3 的特性 //http2,http3 grpc之http2协议 //http2 http协议解析 //http","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubernetes update跟patch区别","slug":"kubernetes-update-patch","date":"2020-09-09T13:58:47.000Z","updated":"2021-12-17T07:42:48.340Z","comments":true,"path":"2020/09/09/kubernetes-update-patch/","link":"","permalink":"https://github.com/fafucoder/2020/09/09/kubernetes-update-patch/","excerpt":"","text":"概述​ 在kubernetes中，对资源进行更新的方式有两种一种是update操作，一种是patch操作。对于update请求,我们需要将整个修改后的对象提交给k8s, 而对于patch请求， 我们只需要将对象中某些字段的修改提交给k8s。 kubernetes update 机制​ 在kubernetes的所有资源对象中，都有一个全局唯一的版本号(metadata.resourceVersion), 每个资源对象从创建开始就会有一个版本号，而后每次被修改（不管是 update 还是 patch 修改），版本号都会发生变化。 ​ kubernetes官方文档 告诉我们，这个版本号是一个 K8s 的内部机制，用户不应该假设它是一个数字或者通过比较两个版本号大小来确定资源对象的新旧，唯一能做的就是通过比较版本号相等来确定对象是否是同一个版本（即是否发生了变化）。而 resourceVersion 一个重要的用处，就是来做 update 请求的版本控制。 ​ kubernetes 要求用户 update 请求中提交的对象必须带有 resourceVersion，也就是说我们提交 update 的数据必须先来源于 K8s 中已经存在的对象。因此，一次完整的 update 操作流程是： 首先，从 K8s 中拿到一个已经存在的对象（可以选择直接从 K8s 中查询；如果在客户端做了 list watch，推荐从本地 informer 中获取）； 然后，基于这个取出来的对象做一些修改，比如将 Deployment 中的 replicas 做增减，或是将 image 字段修改为一个新版本的镜像； 最后，将修改后的对象通过 update 请求提交给 K8s； 此时，kube-apiserver 会校验用户 update 请求提交对象中的 resourceVersion 一定要和当前 K8s 中这个对象最新的 resourceVersion 一致，才能接受本次 update。否则，K8s 会拒绝请求，并告诉用户发生了版本冲突（Conflict）。 上图展示了多个用户同时 update 某一个资源对象时会发生的事情。而如果如果发生了 Conflict 冲突，对于 User A 而言应该做的就是做一次重试，再次获取到最新版本的对象，修改后重新提交 update, 因此： 用户修改 YAML 后提交 update 失败，是因为 YAML 文件中没有包含 resourceVersion 字段。对于 update 请求而言，应该取出当前 K8s 中的对象做修改后提交； 如果两个用户同时对一个资源对象做 update，不管操作的是对象中同一个字段还是不同字段，都存在版本控制的机制确保两个用户的 update 请求不会发生覆盖。 kubernetes patch 机制相比于 update 的版本控制，K8s 的 patch 机制则显得更加简单。当用户对某个资源对象提交一个 patch 请求时，kube-apiserver 不会考虑版本问题，而是“无脑”地接受用户的请求（只要请求发送的 patch 内容合法），也就是将 patch 打到对象上、同时更新版本号。 不过，patch 的复杂点在于，目前 K8s 提供了 4 种 patch 策略：json patch、merge patch、strategic merge patch、apply patch(server-side apply)。通过 kubectl patch -h 命令我们也可以看到这个策略选项（默认采用 strategic) json patch在RFC6902协议的定义中，JSON Patch是执行在资源对象上的一系列操作，如下所示： 12345&#123; \"op\": \"add\", \"path\": \"/spec/containers/0/image\", \"value\": \"busybox:latest\"&#125; op: 表示对资源对象的操作，主要有以下六种操作。 add replace remove move copy test path: 表示被作资源对象的路径. 例如/spec/containers/0/image表示要操作的对象是“spec.containers[0].image” value: 表示预修改的值。 新增容器： 12kubectl patch deployment/foo --type='json' -p \\ '[&#123;\"op\":\"add\",\"path\":\"/spec/template/spec/containers/1\",\"value\":&#123;\"name\":\"nginx\",\"image\":\"nginx:alpine\"&#125;&#125;]' 修改已有的容器镜像: 12kubectl patch deployment/foo --type='json' -p \\ '[&#123;\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/image\",\"value\":\"app-image:v2\"&#125;]' 根据http patch原子性的定义，当某个op(操作)不成功,则整个patch都不成功 merge patchmerge patch 必须包含一个对资源对象的部分描述，json对象。该json对象被提交到服务端，并和服务端的当前对象进行合并，从而创建新的对象。完整的替换列表，也就是说，新的列表定义会替换原有的定义。 例如(设置label)： 1kubectl patch deployment/foo --type='merge' -p '&#123;\"metadata\":&#123;\"labels\":&#123;\"test-key\":\"foo\"&#125;&#125;&#125;' 使用merge patch也有如下限制： 如果value的值为null,表示要删除对应的键，因此我们无法将value的值设置为null, 如下，表示删除键f 123456&#123; \"a\":\"z\", \"c\": &#123; \"f\": null &#125;&#125; merge patch 无法单独更新一个列表(数组)中的某个元素，因此不管我们是要在 containers 里新增容器、还是修改已有容器的 image、env 等字段，都要用整个 containers 列表(数组)来提交 patch： 12kubectl patch deployment/foo --type='merge' -p \\ '&#123;\"spec\":&#123;\"template\":&#123;\"spec\":&#123;\"containers\":[&#123;\"name\":\"app\",\"image\":\"app-image:v2\"&#125;,&#123;\"name\":\"nginx\",\"image\":\"nginx:alpline\"&#125;]&#125;&#125;&#125;&#125;' strategic merge patch这种 patch 策略并没有一个通用的 RFC 标准，而是 K8s 独有的，不过相比前两种而言却更为强大的。 我们先从 K8s 源码看起，在 K8s 原生资源的数据结构定义中额外定义了一些的策略注解。比如以下这个截取了 podSpec 中针对 containers 列表的定义，参考 Github： 1234// ...// +patchMergeKey=name// +patchStrategy=mergeContainers []Container `json:\"containers\" patchStrategy:\"merge\" patchMergeKey:\"name\" protobuf:\"bytes,2,rep,name=containers\"` 可以看到其中有两个关键信息：patchStrategy:”merge” patchMergeKey:”name” 。这就代表了，containers 列表使用 strategic merge patch 策略更新时，会把下面每个元素中的 name 字段看作 key。 简单来说，在我们 patch 更新 containers 不再需要指定下标序号了，而是指定 name 来修改，K8s 会把 name 作为 key 来计算 merge。比如针对以下的 patch 操作： 12kubectl patch deployment/foo -p \\ '&#123;\"spec\":&#123;\"template\":&#123;\"spec\":&#123;\"containers\":[&#123;\"name\":\"nginx\",\"image\":\"nginx:mainline\"&#125;]&#125;&#125;&#125;&#125;' 如果 K8s 发现当前 containers 中已经有名字为 nginx 的容器，则只会把 image 更新上去；而如果当前 containers 中没有 nginx 容器，K8s 会把这个容器插入 containers 列表。 此外还要说明的是，目前 strategic 策略只能用于原生 K8s 资源以及 Aggregated API 方式的自定义资源，对于 CRD 定义的资源对象，是无法使用的。这很好理解，因为 kube-apiserver 无法得知 CRD 资源的结构和 merge 策略。如果用 kubectl patch 命令更新一个 CR，则默认会采用 merge patch 的策略来操作。 apply patchapply patch 分为client-side apply和服务器端的server-side apply。 client-side apply在使用默认参数执行 apply 时，触发的是 client-side apply。kubectl 逻辑如下： 首先解析用户提交的数据（YAML/JSON）为一个对象 A；然后调用 Get 接口从 K8s 中查询这个资源对象： 如果查询结果不存在，kubectl 将本次用户提交的数据记录到对象 A 的 annotation 中（key 为 kubectl.kubernetes.io/last-applied-configuration），最后将对象 A提交给 K8s 创建； 如果查询到 K8s 中已有这个资源，假设为对象 B： kubectl 尝试从对象 B 的 annotation 中取出 kubectl.kubernetes.io/last-applied-configuration 的值（对应了上一次 apply 提交的内容） kubectl 根据前一次 apply 的内容和本次 apply 的内容计算出 diff（默认为 strategic merge patch 格式，如果非原生资源则采用 merge patch） 将 diff 中添加本次的 kubectl.kubernetes.io/last-applied-configuration annotation，最后用 patch 请求提交给 K8s 做更新 与kubectl apply 相比，kubectl edit 逻辑上更简单一些。在用户执行kubectl edit 命令之后，kubectl 从 K8s 中查到当前的资源对象，并打开一个命令行编辑器（默认用 vi）为用户提供编辑界面。当用户修改完成、保存退出时，kubectl 并非直接把修改后的对象提交 update（避免 Conflict，如果用户修改的过程中资源对象又被更新），而是会把修改后的对象和初始拿到的对象计算 diff，最后将 diff 内容用 patch 请求提交给 K8s。 server-side apply [k8s v1.18新特性]server-side apply是k8s v1.18的新特性，虽然v1.16起就已经是Beta版了，但是它没有跟踪与未应用的对象相关联的字段的所有者。这意味着大多数对象没有存储managedFields元数据，这些对象的冲突无法解决。在Kubernetes 1.18中，所有新对象都将附加managedFields，并提供关于冲突的准确信息。 那什么是Server-side Apply呢？简单的说就是多个Controller控制一个资源多个Controller控制一个资源, 通过managedFields来记录哪个Field被哪个资源控制，例如 WorkloadController只能修改image相关的操作，而ScaleController 只能修改副本数。 相比于client-side apply 用 last-applied annotations的方式，服务器端(server-side) apply 新提供了一种声明式 API (叫 ManagedFields) 来明确指定谁管理哪些资源字段。当使用server-side apply时，尝试着去改变一个被其他人管理的字段， 会导致请求被拒绝（在没有设置强制执行时，参见冲突）。 冲突是一种特定的错误状态， 发生在执行 Apply 改变一个字段，而恰巧该字段被其他用户声明过主权时。 这可以防止一个应用者不小心覆盖掉其他用户设置的值。 冲突发生时，应用者有三种办法来解决它： 覆盖前值，成为唯一的管理器： 如果打算覆盖该值（或应用者是一个自动化部件，比如控制器）， 应用者应该设置查询参数 force 为 true，然后再发送一次请求。 这将强制操作成功，改变字段的值，从所有其他管理器的 managedFields 条目中删除指定字段。 不覆盖前值，放弃管理权： 如果应用者不再关注该字段的值， 可以从配置文件中删掉它，再重新发送请求。 这就保持了原值不变，并从 managedFields 的应用者条目中删除该字段。 不覆盖前值，成为共享的管理器： 如果应用者仍然关注字段值，并不想覆盖它， 他们可以在配置文件中把字段的值改为和服务器对象一样，再重新发送请求。 这样在不改变字段值的前提下， 就实现了字段管理被应用者和所有声明了管理权的其他的字段管理器共享。 参考文档 Kubernetes中的JSON patch // 掘金 理解 K8s 资源更新机制，从一个 OpenKruise 用户疑问开始 // kubernetes 中文社区 服务器端应用(Server-Side Apply) // kubernetes官方文档 k8s源码解析 - apply命令的实现 //csdn Kubernetes 1.18 Feature Server-side Apply Beta 2 //kubernetes 官方文档","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"golang类型别名和类型定义","slug":"golang-type","date":"2020-08-06T06:50:53.000Z","updated":"2021-12-17T07:42:48.327Z","comments":true,"path":"2020/08/06/golang-type/","link":"","permalink":"https://github.com/fafucoder/2020/08/06/golang-type/","excerpt":"","text":"类型定义根据基本类型声明一个新的数据类型。 类型定义格式类似如下： 12type hello inttype world func(name string) string 类型别名类型别名 是 Go 1.9 版本添加的新功能。主要应用于代码升级、工程重构、迁移中类型的兼容性问题。C/C++ 语言中，代码的重构升级可以使用宏快速定义新的代码。 类型别名格式类似如下: 12type hello &#x3D; inttype world &#x3D; int32 类型定义与类型别名类型别名规定：Type Alias只是Type 的别名，本质上Type Alias 与Type是同一个类型，即基本数据类型是一致的。这意味着这两种类型的数据可以互相赋值，而类型定义要和原始类型赋值的时候需要类型转换(Conversion T(x))。 1234567891011121314151617181920package mainimport &quot;fmt&quot;type world func(name string) stringtype hello &#x3D; intfunc main() &#123; var h hello var w world fmt.Printf(&quot;hello Type: %T, value： %d\\n&quot;, h, h) fmt.Printf(&quot;world Type: %T, value： %d\\n&quot;, w, w)&#125;result:hello Type: int, value： 0world Type: main.world, value： 0 参考文档 https://blog.csdn.net/AMimiDou_212/article/details/94873163 https://sanyuesha.com/2017/07/27/go-type/ https://colobu.com/2017/06/26/learn-go-type-aliases/","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"kubernetes-podSecurityPolicy","slug":"kubernetes-podSecurityPolicy","date":"2020-08-06T03:35:06.000Z","updated":"2021-12-17T07:42:48.337Z","comments":true,"path":"2020/08/06/kubernetes-podSecurityPolicy/","link":"","permalink":"https://github.com/fafucoder/2020/08/06/kubernetes-podSecurityPolicy/","excerpt":"","text":"参考文档 https://ieevee.com/tech/2019/02/18/psp.html https://www.qikqiak.com/post/setup-psp-in-k8s/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes metallb 原理解析","slug":"kubernetes-metallb","date":"2020-08-06T03:02:25.000Z","updated":"2021-12-17T07:42:48.337Z","comments":true,"path":"2020/08/06/kubernetes-metallb/","link":"","permalink":"https://github.com/fafucoder/2020/08/06/kubernetes-metallb/","excerpt":"","text":"概述k8s的LoadBalancer类型的Service依赖于外部的云提供的Load Balancer。为了从外部访问裸机 Kubernetes 群集，目前只能使用 NodePort 或 Ingress 的方法进行服务暴露。前者的缺点是每个暴露的服务需要占用所有节点的某个端口，后者的缺点是仅仅能支持 HTTP 协议。 MetalLB 是一个负载均衡器，专门解决裸金属 Kubernetes 集群中无法使用 LoadBalancer 类型服务的痛点。 工作原理MetalLB 会在 Kubernetes 内运行，监控服务对象的变化，一旦监测到有新的 LoadBalancer 服务运行，并且没有可申请的负载均衡器之后，就会完成地址分配和外部声明两部分的工作。使用 MetalLB 时，MetalLB 会自己为用户的 LoadBalancer 类型 Service 分配 IP 地址，当然该 IP 地址不是凭空产生的，需要用户在配置中提供一个 IP 地址池，Metallb 将会在其中选取地址分配给服务。 MetalLB 将 IP 分配给某个服务后，它需要对外宣告此 IP 地址，并让外部主机可以路由到此 IP。MetalLB 支持两种声明模式：Layer 2（ ARP / NDP ）模式或者 BGP 模式。 Layer2 模式 Layer2 模式下，每个 Service 会有集群中的一个 Node 来负责。服务的入口流量全部经由单个节点，然后该节点的 Kube-Proxy 会把流量再转发给服务的 Pods。也就是说，该模式下 MetalLB 并没有真正提供负载均衡器。尽管如此，MetalLB 提供了故障转移功能，如果持有 IP 的节点出现故障，则默认 10 秒后即发生故障转移，IP 会被分配给其它健康的节点。 Layer2 模式的优缺点： Layer2 模式更为通用，不需要用户有额外的设备； Layer2 模式下存在单点问题，服务的所有入口流量经由单点，其网络带宽可能成为瓶颈； 由于 Layer 2 模式需要 ARP/NDP 客户端配合，当故障转移发生时，MetalLB 会发送 ARP 包来宣告 MAC 地址和 IP 映射关系的变化，地址分配略为繁琐。 BGP模式 当在第三层工作时，集群中所有机器都和你控制的最接近的路由器建立 BGP 会话，此会话让路由器能学习到如何转发针对 K8S 服务 IP 的数据包。 通过使用 BGP，可以实现真正的跨多节点负载均衡（需要路由器支持 multipath），还可以基于 BGP 的策略机制实现细粒度的流量控制。 具体的负载均衡行为和路由器有关，可保证的共同行为是：每个连接（TCP 或 UDP 会话）的数据包总是路由到同一个节点上。 BGP 模式的优缺点： 不能优雅处理故障转移，当持有服务的节点宕掉后，所有活动连接的客户端将收到 Connection reset by peer ； BGP 路由器对数据包的源 IP、目的 IP、协议类型进行简单的哈希，并依据哈希值决定发给哪个 K8S 节点。问题是 K8S 节点集是不稳定的，一旦（参与 BGP）的节点宕掉，很大部分的活动连接都会因为 rehash 而坏掉。 BGP 模式问题的缓和措施： 将服务绑定到一部分固定的节点上，降低 rehash 的概率。 在流量低的时段改变服务的部署。 客户端添加透明重试逻辑，当发现连接 TCP 层错误时自动重试。 运行流程 在metallb中，controller跟speaker通过list-watch service, configmap跟api-service交互，当创建一个loadbalancer的service的时候，controller获取到svc，检查configMap是否有config配置，接着为svc分配一个地址，最后更新svc的status(更新svc.Status.LoadBalancer.Ingress), 此时svc已经获取到了一个external ip地址。 speaker是一个daemonset，在speaker中，所以speaker会初始化memberlist, 然后把所有speaker所在node加入到memberlist中，目的是实现节点快速的故障转移(如果使用node list-watch机制，sync其实有个时间过程，而使用memberlist能更快速的故障转移)。 当controller更新完状态后，speaker监听到资源变化，获取到loadbalancerIP， 选择合适的节点进行通告(选择backends所在node作为通告节点，首先会获取到endpoints-&gt;接着endpoints的NodeName-&gt;判断memberlist中是否有此nodeName-&gt;nodeName作为通告节点), 如果loadbalanceIP是ipv4则选择arp进行通告，否则就选择ndp进行通告。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576func (a *Announce) SetBalancer(name string, ip net.IP) &#123; a.Lock() defer a.Unlock() &#x2F;&#x2F; Kubernetes may inform us that we should advertise this address multiple &#x2F;&#x2F; times, so just no-op any subsequent requests. if _, ok :&#x3D; a.ips[name]; ok &#123; return &#125; a.ips[name] &#x3D; ip a.ipRefcnt[ip.String()]++ if a.ipRefcnt[ip.String()] &gt; 1 &#123; &#x2F;&#x2F; Multiple services are using this IP, so there&#39;s nothing &#x2F;&#x2F; else to do right now. return &#125; for _, client :&#x3D; range a.ndps &#123; if err :&#x3D; client.Watch(ip); err !&#x3D; nil &#123; a.logger.Log(&quot;op&quot;, &quot;watchMulticastGroup&quot;, &quot;error&quot;, err, &quot;ip&quot;, ip, &quot;msg&quot;, &quot;failed to watch NDP multicast group for IP, NDP responder will not respond to requests for this address&quot;) &#125; &#125; go a.spam(name)&#125;func (a *Announce) spam(name string) &#123; start :&#x3D; time.Now() for time.Since(start) &lt; 5*time.Second &#123; if err :&#x3D; a.gratuitous(name); err !&#x3D; nil &#123; a.logger.Log(&quot;op&quot;, &quot;gratuitousAnnounce&quot;, &quot;error&quot;, err, &quot;service&quot;, name, &quot;msg&quot;, &quot;failed to make gratuitous IP announcement&quot;) &#125; time.Sleep(1100 * time.Millisecond) &#125;&#125;func (a *Announce) gratuitous(name string) error &#123; a.Lock() defer a.Unlock() ip, ok :&#x3D; a.ips[name] if !ok &#123; &#x2F;&#x2F; No IP means we&#39;ve lost control of the IP, someone else is &#x2F;&#x2F; doing announcements. return nil &#125; if ip.To4() !&#x3D; nil &#123; for _, client :&#x3D; range a.arps &#123; if err :&#x3D; client.Gratuitous(ip); err !&#x3D; nil &#123; return err &#125; &#125; &#125; else &#123; for _, client :&#x3D; range a.ndps &#123; if err :&#x3D; client.Gratuitous(ip); err !&#x3D; nil &#123; return err &#125; &#125; &#125; return nil&#125;func (a *arpResponder) Gratuitous(ip net.IP) error &#123; for _, op :&#x3D; range []arp.Operation&#123;arp.OperationRequest, arp.OperationReply&#125; &#123; pkt, err :&#x3D; arp.NewPacket(op, a.hardwareAddr, ip, ethernet.Broadcast, ip) if err !&#x3D; nil &#123; return fmt.Errorf(&quot;assembling %q gratuitous packet for %q: %s&quot;, op, ip, err) &#125; if err &#x3D; a.conn.WriteTo(pkt, ethernet.Broadcast); err !&#x3D; nil &#123; return fmt.Errorf(&quot;writing %q gratuitous packet for %q: %s&quot;, op, ip, err) &#125; stats.SentGratuitous(ip.String()) &#125; return nil&#125; speaker通告过程如下，首先在speaker初始化中，会获取到主机的所有网卡，然后过滤掉没有地址，状态不是UP，ARP未开启的网卡，然后获取这些网卡的第一个IP地址(一个网卡可能有多个地址), 使用rawSocket监听网卡的数据包，构建应答包。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465func newARPResponder(logger log.Logger, ifi *net.Interface, ann announceFunc) (*arpResponder, error) &#123; client, err :&#x3D; arp.Dial(ifi) if err !&#x3D; nil &#123; return nil, fmt.Errorf(&quot;creating ARP responder for %q: %s&quot;, ifi.Name, err) &#125; ret :&#x3D; &amp;arpResponder&#123; logger: logger, intf: ifi.Name, hardwareAddr: ifi.HardwareAddr, conn: client, closed: make(chan struct&#123;&#125;), announce: ann, &#125; go ret.run() return ret, nil&#125;func (a *arpResponder) run() &#123; for a.processRequest() !&#x3D; dropReasonClosed &#123; &#125;&#125;func (a *arpResponder) processRequest() dropReason &#123; pkt, eth, err :&#x3D; a.conn.Read() if err !&#x3D; nil &#123; &#x2F;&#x2F; ARP listener doesn&#39;t cleanly return EOF when closed, so we &#x2F;&#x2F; need to hook into the call to arpResponder.Close() &#x2F;&#x2F; independently. select &#123; case &lt;-a.closed: return dropReasonClosed default: &#125; if err &#x3D;&#x3D; io.EOF &#123; return dropReasonClosed &#125; return dropReasonError &#125; &#x2F;&#x2F; Ignore ARP replies. if pkt.Operation !&#x3D; arp.OperationRequest &#123; return dropReasonARPReply &#125; &#x2F;&#x2F; Ignore ARP requests which are not broadcast or bound directly for this machine. if !bytes.Equal(eth.Destination, ethernet.Broadcast) &amp;&amp; !bytes.Equal(eth.Destination, a.hardwareAddr) &#123; return dropReasonEthernetDestination &#125; &#x2F;&#x2F; Ignore ARP requests that the announcer tells us to ignore. if reason :&#x3D; a.announce(pkt.TargetIP); reason !&#x3D; dropReasonNone &#123; return reason &#125; stats.GotRequest(pkt.TargetIP.String()) a.logger.Log(&quot;interface&quot;, a.intf, &quot;ip&quot;, pkt.TargetIP, &quot;senderIP&quot;, pkt.SenderIP, &quot;senderMAC&quot;, pkt.SenderHardwareAddr, &quot;responseMAC&quot;, a.hardwareAddr, &quot;msg&quot;, &quot;got ARP request for service IP, sending response&quot;) if err :&#x3D; a.conn.Reply(pkt, a.hardwareAddr, pkt.TargetIP); err !&#x3D; nil &#123; a.logger.Log(&quot;op&quot;, &quot;arpReply&quot;, &quot;interface&quot;, a.intf, &quot;ip&quot;, pkt.TargetIP, &quot;senderIP&quot;, pkt.SenderIP, &quot;senderMAC&quot;, pkt.SenderHardwareAddr, &quot;responseMAC&quot;, a.hardwareAddr, &quot;error&quot;, err, &quot;msg&quot;, &quot;failed to send ARP reply&quot;) &#125; else &#123; stats.SentResponse(pkt.TargetIP.String()) &#125; return dropReasonNone&#125; 参考文档 https://zhuanlan.zhihu.com/p/103717169 https://blog.fleeto.us/post/intro-metallb/ https://github.com/metallb/metallb https://cloud.tencent.com/developer/article/1631910 https://www.jianshu.com/p/5e39d1637184","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"ssh命令知多少","slug":"linux-ssh","date":"2020-08-04T12:49:26.000Z","updated":"2021-12-17T07:42:48.351Z","comments":true,"path":"2020/08/04/linux-ssh/","link":"","permalink":"https://github.com/fafucoder/2020/08/04/linux-ssh/","excerpt":"","text":"基础知识正向代理所谓正向代理就是无法直接访问目标服务，只能借助中间机器来访问目标服务。如下图所示：hostA无法直接访问目标服务(例如在内网环境，无法直接访问外部服务)，但是能够访问hostB且hostB能够访问目标服务，因此hostA去访问hostB，然后由hostB去请求目标服务。 反向代理反向代理跟正向代理相反，正向代理解决的是怎么出去问题，反向代理解决的是怎么让外部访问你的服务问题，例如hostA有个服务需要对外暴露，但是外部网络无法直接访问,此时hostB能访问到hostA，外部网络能访问hostB，因此外部网络通过访问hostB,hostB再把数据代理到HostA ssh 代理功能ssh 命令除了登陆外还有三种代理功能： 正向代理（-L）：相当于 iptable 的 port forwarding 反向代理（-R）：相当于 frp 或者 ngrok socks5 代理（-D）：相当于 ss/ssr ssh 正向代理1ssh -L [客户端IP或省略]:[客户端端口]:[服务器侧能访问的IP]:[服务器侧能访问的IP的端口] [登陆服务器的用户名@服务器IP] -p [服务器ssh服务端口（默认22）] 客户端IP可以省略，省略的话就是127.0.0.1。服务器IP都可以用域名来代替。 举例说明： 你的IP是192.168.1.2，你可以ssh到某台服务器8.8.8.8，8.8.8.8可以访问8.8.4.4，你内网里还有一台机器可以访问你。 如果你想让内网里的另外一台电脑访问8.8.4.4的80端口的http服务，那么可以执行： 1ssh -L 192.168.1.2:80:8.8.4.4:80 test@8.8.8.8 这时内网里的另外一台机器可以通过浏览器中输入http://192.168.1.2:80查看8.8.4.4的网页 ssh 反向代理1ssh -R [服务器IP或省略]:[服务器端口]:[客户端侧能访问的IP]:[客户端侧能访问的IP的端口] [登陆服务器的用户名@服务器IP] -p [服务器ssh服务端口（默认22）] 服务器IP如果省略，则默认为127.0.0.1，只有服务器自身可以访问。指定服务器外网IP的话，任何人都可以通过[服务器IP:端口]来访问服务。 举例说明： 你的IP是192.168.1.2，你可以ssh到外网某台服务器8.8.8.8，你内网里有一台机器192.168.1.3。 如果你想让外网所有的能访问8.8.8.8的IP都能访问192.168.1.3的http服务，那么可以执行： 1ssh -R 8.8.8.8:80:192.168.1.3:80 test@8.8.8.8 这时外网任何一台可以访问8.8.8.8的机器都可以通过80端口访问到内网192.168.1.3机器的80端口。 参考文档 https://cloud.tencent.com/developer/article/1114279 https://blog.csdn.net/sch0120/article/details/73770386 https://kanda.me/2019/07/01/ssh-over-http-or-socks/ ssh命令的三种代理功能","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubernetes kube-proxy实现原理","slug":"kubernetes-kube-proxy","date":"2020-07-27T03:17:59.000Z","updated":"2021-12-17T07:42:48.336Z","comments":true,"path":"2020/07/27/kubernetes-kube-proxy/","link":"","permalink":"https://github.com/fafucoder/2020/07/27/kubernetes-kube-proxy/","excerpt":"","text":"概念service 是一组pod的服务抽象，相当于一组pod的LoadBanlance, 负责将请求分发给对应的pod，service会为这个LB提供一个IP，一般称为cluster IP。ClusterIP是个假的IP，这个IP在整个集群中根本不存在，无法通过IP协议栈无法路由，底层underlay设备也无法感知这个IP的存在，因此ClusterIP只能是单主机(Host Only）作用域可见，这个IP在其他节点以及集群外均无法访问。 kube-proxy的作用主要是负责service的实现，具体来说，就是实现集群内的客户端pod访问service，或者是集群外的主机通过NodePort等方式访问service。 kube-proxy存在于各个node节点上，部署方式的daemonset, 默认使用iptables模式。 kubernetes中各组件如下： kube proxy模式kube-proxy 目前有3中常见的proxyMode, 分别是userspace, iptables, ipvs。userspace mode是v1.0及以前版本的默认模式。从v1.1版本开始，增加了iptables mode，在 v1.3版本中正式替代了 userspace 模式成为默认模式（需要 iptables 的版本&gt;= 1.4.11。IPVS 是 LVS 的负载均衡模块，同样基于 netfilter，但比 iptables 性能更好，具备更好的可扩展性。 userspace mode基于用户态的 proxy，service 的请求会先从用户空间进入内核 iptables，然后再回到用户空间，由 kube-proxy 完成后端 endpoints 的选择和代理工作，这种方式流量从用户空间进出内核带来的性能损耗比较大。 userspace模式下，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；对每个 Service，它都为其在本地节点开放一个端口，作为其服务代理端口；发往该端口的请求会采用一定的策略转发给与该服务对应的后端 Pod 实体。kube-proxy 同时会在本地节点设置 iptables 规则，配置一个 Virtual IP，把发往 Virtual IP 的请求重定向到与该 Virtual IP 对应的服务代理端口上。 iptables modeiptables 的方式是完全通过内核的 iptables 实现 service 的代理和 LB。 iptables模式相比 userspace 模式，克服了请求在用户态-内核态反复传递的问题，性能上有所提升，但使用 iptables NAT 来完成转发，存在不可忽视的性能损耗 iptables 模式与 userspace 相同，kube-proxy 持续监听 Service 以及 Endpoints 对象的变化；但它并不在本地节点开启反向代理服务，而是把反向代理全部交给 iptables 来实现；即 iptables 直接将对 VIP 的请求转发给后端 Pod，通过 iptables 设置转发策略。 ipvs modeipvs模式是基于 NAT 实现的，通过ipvs的NAT模式，对访问k8s service的请求进行虚IP到POD IP的转发。当创建一个 service 后，kubernetes 会在每个节点上创建一个网卡，同时帮你将 Service IP(VIP) 绑定上。 与iptables、userspace 模式一样，kube-proxy依然监听Service以及Endpoints对象的变化, 不过它并不创建反向代理, 也不创建大量的 iptables 规则, 而是通过netlink 创建ipvs规则，并使用k8s Service与Endpoints信息，对所在节点的ipvs规则进行定期同步; netlink 与 iptables 底层都是基于 netfilter 钩子，但是 netlink 由于采用了 hash table 而且直接工作在内核态，在性能上比 iptables 更优 设置kube-proxy模式由于kube-proxy是daemonset, 因此通过编辑daemonset发现通过mount configMap把配置传递进去，因此只要修改configmap中的数据就行. 修改kube-proxy configMap中的mode设置kube-proxy模式 1234567kind: KubeProxyConfigurationmetricsBindAddress: &quot;&quot;mode: &quot;&quot;nodePortAddresses: nulloomScoreAdj: nullportRange: &quot;&quot;udpIdleTimeout: 0s iptables 模式原理解析iptables 模式下的规则如下所示： ClusterIp 模式 非本机节点pod访问: PREROUTING --&gt; KUBE-SERVICE --&gt; KUBE-SVC-XXX --&gt; KUBE-SEP-XXX 本机节点访问pod: OUTPUT --&gt; KUBE-SERVICE --&gt; KUBE-SVC-XXX --&gt; KUBE-SEP-XXX 访问流程： 对于进入 PREROUTING 链的都转到 KUBE-SERVICES 链进行处理； 在 KUBE-SERVICES 链，对于访问 clusterIP 为 10.110.243.155 的转发到 KUBE-SVC-5SB6FTEHND4GTL2W； 访问 KUBE-SVC-5SB6FTEHND4GTL2W 的使用随机数负载均衡，并转发到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 上； KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 对应 endpoint 中的 pod 192.168.137.147 和 192.168.98.213，设置 mark 标记，进行 DNAT 并转发到具体的 pod 上，如果某个 service 的 endpoints 中没有 pod，那么针对此 service 的请求将会被 drop 掉； 1234567891011121314151617&#x2F;&#x2F; 1.对于进入 PREROUTING 链的都转到 KUBE-SERVICES 链进行处理；-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES&#x2F;&#x2F; 2.在 KUBE-SERVICES 链，对于访问 clusterIP 为 10.110.243.155 的转发到 KUBE-SVC-5SB6FTEHND4GTL2W；-A KUBE-SERVICES -d 10.110.243.155&#x2F;32 -p tcp -m comment --comment &quot;pks-system&#x2F;tenant-service: cluster IP&quot; -m tcp --dport 7000 -j KUBE-SVC-5SB6FTEHND4GTL2W&#x2F;&#x2F; 3.访问 KUBE-SVC-5SB6FTEHND4GTL2W 的使用随机数负载均衡，并转发到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 上；-A KUBE-SVC-5SB6FTEHND4GTL2W -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CI5ZO3FTK7KBNRMG-A KUBE-SVC-5SB6FTEHND4GTL2W -j KUBE-SEP-OVNLTDWFHTHII4SC&#x2F;&#x2F; 4.KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-OVNLTDWFHTHII4SC 对应 endpoint 中的 pod 192.168.137.147 和 192.168.98.213，设置 mark 标记，进行 DNAT 并转发到具体的 pod 上，如果某个 service 的 endpoints 中没有 pod，那么针对此 service 的请求将会被 drop 掉；-A KUBE-SEP-CI5ZO3FTK7KBNRMG -s 192.168.137.147&#x2F;32 -j KUBE-MARK-MASQ-A KUBE-SEP-CI5ZO3FTK7KBNRMG -p tcp -m tcp -j DNAT --to-destination 192.168.137.147:7000-A KUBE-SEP-OVNLTDWFHTHII4SC -s 192.168.98.213&#x2F;32 -j KUBE-MARK-MASQ-A KUBE-SEP-OVNLTDWFHTHII4SC -p tcp -m tcp -j DNAT --to-destination 192.168.98.213:7000 NodePort 模式在 nodePort 方式下，会用到 KUBE-NODEPORTS 规则链，通过 iptables -t nat -L -n 可以看到 KUBE-NODEPORTS 位于 KUBE-SERVICE 链的最后一个，iptables 在处理报文时会优先处理目的 IP 为clusterIP 的报文，在前面的 KUBE-SVC-XXX 都匹配失败之后再去使用 nodePort 方式进行匹配。 本机模式： OUTPUT --&gt; KUBE-SERVICE --&gt; KUBE-NODEPORTS --&gt; KUBE-SVC-XXX --&gt; KUBE-SEP-XXX 非本机模式: PREROUTING --&gt; KUBE-SERVICE --&gt; KUBE-NODEPORTS --&gt; KUBE-SVC-XXX --&gt; KUBE-SEP-XXX 访问流程： 该服务的 nodePort 端口为 30070，其 iptables 访问规则和使用 clusterIP 方式访问有点类似，不过 nodePort 方式会比 clusterIP 的方式多走一条链 KUBE-NODEPORTS，其会在 KUBE-NODEPORTS 链设置 mark 标记并转发到 KUBE-SVC-5SB6FTEHND4GTL2W，nodeport 与 clusterIP 访问方式最后都是转发到了 KUBE-SVC-xxx 链。 经过 PREROUTING 转到 KUBE-SERVICES 经过 KUBE-SERVICES 转到 KUBE-NODEPORTS 经过 KUBE-NODEPORTS 转到 KUBE-SVC-5SB6FTEHND4GTL2W 经过KUBE-SVC-5SB6FTEHND4GTL2W 转到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV 经过 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV 分别转到 192.168.137.147:7000 和 192.168.89.11:7000 1234567891011121314151617181920&#x2F;&#x2F; 1. 经过 PREROUTING 转到 KUBE-SERVICES-A PREROUTING -m comment --comment &quot;kubernetes service portals&quot; -j KUBE-SERVICES&#x2F;&#x2F; 2. 经过 KUBE-SERVICES 转到 KUBE-NODEPORTS-A KUBE-SERVICES xxx-A KUBE-SERVICES -m comment --comment &quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS&#x2F;&#x2F; 3. 经过 KUBE-NODEPORTS 转到 KUBE-SVC-5SB6FTEHND4GTL2W-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;pks-system&#x2F;tenant-service:&quot; -m tcp --dport 30070 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;pks-system&#x2F;tenant-service:&quot; -m tcp --dport 30070 -j KUBE-SVC-5SB6FTEHND4GTL2W&#x2F;&#x2F; 4. 经过KUBE-SVC-5SB6FTEHND4GTL2W 转到 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV-A KUBE-SVC-5SB6FTEHND4GTL2W -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-CI5ZO3FTK7KBNRMG-A KUBE-SVC-5SB6FTEHND4GTL2W -j KUBE-SEP-VR562QDKF524UNPV&#x2F;&#x2F; 5. 经过 KUBE-SEP-CI5ZO3FTK7KBNRMG 和 KUBE-SEP-VR562QDKF524UNPV 分别转到 192.168.137.147:7000 和 192.168.89.11:7000-A KUBE-SEP-CI5ZO3FTK7KBNRMG -s 192.168.137.147&#x2F;32 -j KUBE-MARK-MASQ-A KUBE-SEP-CI5ZO3FTK7KBNRMG -p tcp -m tcp -j DNAT --to-destination 192.168.137.147:7000-A KUBE-SEP-VR562QDKF524UNPV -s 192.168.89.11&#x2F;32 -j KUBE-MARK-MASQ-A KUBE-SEP-VR562QDKF524UNPV -p tcp -m tcp -j DNAT --to-destination 192.168.89.11:7000 ipvs模式解析ipvs的实现模式跟iptables的实现模式类似，底层都是基于netfilter, 但是ipvs使用hash表进行规则查找(时间复杂度为O(1))，而iptables使用链表进行查找(平均时间复杂度O(n/2),最好的情况是O(1),最坏的情况是O(n))。其次，由于 IPVS 的 DNAT 钩子挂在 INPUT 链上，因此为了让内核识别 VIP 是本机的 IP，ipvs模式下k8s 会将service cluster ip 绑定到虚拟网卡kube-ipvs0上 ipvs跟iptables的主要区别如下： iptables 使用链表，ipvs 使用哈希表； iptables 只支持随机、轮询两种负载均衡算法而 ipvs 支持的多达 8 种； ipvs 还支持 realserver 运行状况检查、连接重试、端口映射、会话保持等功能。 参考文档 https://www.cnblogs.com/fuyuteng/p/11598768.html https://zhuanlan.zhihu.com/p/94418251?from_voters_page=true https://juejin.im/post/6844904098605563912 https://xuxinkun.github.io/2016/07/22/kubernetes-proxy/ https://knarfeh.com/2018/07/28/Kubernetes%20%E6%BA%90%E7%A0%81%E7%AC%94%E8%AE%B0%EF%BC%88kube-proxy%EF%BC%89 https://linuxops.dev/post/kubernetes%E7%9A%84kube-proxy%E7%9A%84%E8%BD%AC%E5%8F%91%E8%A7%84%E5%88%99%E5%88%86%E6%9E%90/ https://www.cnblogs.com/charlieroro/p/9588019.html https://www.cnblogs.com/luozhiyun/p/13782077.html // 深入k8s：kube-proxy ipvs及其源码分析 https://blog.csdn.net/u011563903/article/details/87904873 // K8S kube-proxy ipvs 原理分析 https://mp.weixin.qq.com/s/X6EL8GwWoi9_DyvhHL6Mlw // 米开朗基杨博客","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes PLEG不健康的问题","slug":"kubernetes-pleg-error","date":"2020-07-22T06:16:28.000Z","updated":"2021-12-17T07:42:48.337Z","comments":true,"path":"2020/07/22/kubernetes-pleg-error/","link":"","permalink":"https://github.com/fafucoder/2020/07/22/kubernetes-pleg-error/","excerpt":"","text":"node no ready的可能问题 网络插件有问题 kubelet有问题（PLEG的问题) 系统资源问题（cpu或者内存满了) 参考文档 https://segmentfault.com/a/1190000018240634?utm_source=tag-newest https://segmentfault.com/a/1190000022163789 https://fuckcloudnative.io/posts/understanding-the-pleg-is-not-healthy/ https://blog.csdn.net/Tulipyx/article/details/113869714","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes scheduler","slug":"kubernetes-schedule","date":"2020-07-12T11:35:09.000Z","updated":"2021-12-17T07:42:48.339Z","comments":true,"path":"2020/07/12/kubernetes-schedule/","link":"","permalink":"https://github.com/fafucoder/2020/07/12/kubernetes-schedule/","excerpt":"","text":"参考文档 https://zhuanlan.zhihu.com/p/27754017","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes service 原理解析","slug":"kubernetes-services","date":"2020-07-10T07:00:34.000Z","updated":"2021-12-17T07:42:48.339Z","comments":true,"path":"2020/07/10/kubernetes-services/","link":"","permalink":"https://github.com/fafucoder/2020/07/10/kubernetes-services/","excerpt":"","text":"概念service 是一组具有相同 label pod 集合的抽象，集群内外的各个服务可以通过 service 进行互相通信，当创建一个 service 对象时也会对应创建一个 endpoint 对象，endpoint 是用来做容器发现的，service 只是将多个 pod 进行关联，实际的路由转发都是由 kubernetes 中的 kube-proxy 组件来实现。 endpoints controller 是负责生成和维护所有 endpoints 对象的控制器，监听 service 和对应 pod 的变化，更新对应 service 的 endpoints 对象。当用户创建 service 后 endpoints controller 会监听 pod 的状态，当 pod 处于 running 且准备就绪时，endpoints controller 会将 pod ip 记录到 endpoints 对象中，因此，service 的容器发现是通过 endpoints 来实现的。而 kube-proxy 会监听 service 和 endpoints 的更新并调用其代理模块在主机上刷新路由转发规则。 service 类型service 默认有四种 ClusterIP、NodePort、LoadBalancer、ExternelName 类型， 此外还有 Ingress ClusterIpClusterIP 类型的 service 是 kubernetes 集群默认的服务暴露方式，它只能用于集群内部通信，可以被各 pod 访问，其访问方式为： 1pod ---&gt; ClusterIP:ServicePort --&gt; (iptables)DNAT --&gt; PodIP:containePort nodePortNodePort 类型的 service 会在集群内部署了 kube-proxy 的节点打开一个指定的端口，之后所有的流量直接发送到这个端口，然后会被转发到 service 后端真实的服务进行访问。Nodeport 构建在 ClusterIP 上，其访问链路如下所示： client ---&gt; NodeIP:NodePort ---&gt; ClusterIP:ServicePort ---&gt; (iptables)DNAT ---&gt; PodIP:containePort LoadBalanceLoadBalancer 类型的 service 通常和云厂商的 LB 结合一起使用，用于将集群内部的服务暴露到外网，云厂商的 LoadBalancer 会给用户分配一个 IP，之后通过该 IP 的流量会转发到你的 service 上。 ExternelName通过 CNAME 将 service 与 externalName 的值(比如：foo.bar.example.com)映射起来，这种方式用的比较少。 IngressIngress 其实不是 service 的一个类型，但是它可以作用于多个 service，被称为 service 的 service，作为集群内部服务的入口，Ingress 作用在七层，可以根据不同的 url，将请求转发到不同的 service 上。 service服务发现service 的 endpoints 解决了容器发现问题，但不提前知道 service 的 Cluster IP，怎么发现 service 服务呢？ service 当前支持两种类型的服务发现机制，一种是通过环境变量，另一种是通过 DNS。 环境变量当一个 pod 创建完成之后，kubelet 会在该 pod 中注册该集群已经创建的所有 service 相关的环境变量，但是需要注意的是，在 service 创建之前的所有 pod 是不会注册该环境变量的，所以在平时使用时，建议通过 DNS 的方式进行 service 之间的服务发现。 DNS可以在集群中部署 CoreDNS 服务(旧版本的 kubernetes 群使用的是 kubeDNS)， 来达到集群内部的 pod 通过DNS 的方式进行集群内部各个服务之间的通讯。 当前 kubernetes 集群默认使用 CoreDNS 作为默认的 DNS 服务，主要原因是 CoreDNS 是基于 Plugin 的方式进行扩展的，简单，灵活，并且不完全被Kubernetes所捆绑。 参考文档 https://segmentfault.com/a/1190000019376912?utm_source=tag-newest https://draveness.me/kubernetes-service/ https://kubernetes.io/zh/docs/tutorials/services/source-ip/ https://cloud.tencent.com/developer/article/1553954","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"linux网卡收发包流程","slug":"linux-nic","date":"2020-07-09T06:55:33.000Z","updated":"2021-12-17T07:42:48.348Z","comments":true,"path":"2020/07/09/linux-nic/","link":"","permalink":"https://github.com/fafucoder/2020/07/09/linux-nic/","excerpt":"","text":"网卡数据包接收流程 网卡收到数据包。 将数据包从网卡硬件缓存转移到服务器内存中。 通过中断请求，通知内核处理。 经过 TCP/IP 协议逐层处理。 应用程序通过 read() 从 socket buffer 读取数据。 网卡收到的数据包转移到主机内存（NIC 与驱动交互）NIC(网卡)在接收到数据包之后，首先需要将数据同步到内核中，这中间的桥梁是 rx ring buffer。它是由 NIC 和驱动程序共享的一片区域，事实上，rx ring buffer存储的并不是实际的 packet 数据，而是一个描述符，这个描述符指向了它真正的存储地址，具体流程如下： 驱动在内存中分配一片缓冲区用来接收数据包，叫做 sk_buffer； 将上述缓冲区的地址和大小（即接收描述符），加入到 rx ring buffer。描述符中的缓冲区地址是 DMA 使用的物理地址； 驱动通知网卡有一个新的描述符； 网卡从 rx ring buffer 中取出描述符，从而获知缓冲区的地址和大小； 网卡收到新的数据包； 网卡将新数据包通过 DMA 直接写到 sk_buffer 中。 当驱动处理速度跟不上网卡收包速度时，驱动来不及分配缓冲区，NIC 接收到的数据包无法及时写到 sk_buffer，就会产生堆积，当 NIC 内部缓冲区写满后，就会丢弃部分数据，引起丢包。这部分丢包为 rx_fifo_errors，在 /proc/net/dev 中体现为 fifo 字段增长，在 ifconfig 中体现为 overruns 指标增长。 系统内核处理（驱动与 Linux 内核交互）这个时候，数据包已经被转移到了 sk_buffer 中。前文提到，这是驱动程序在内存中分配的一片缓冲区，并且是通过 DMA 写入的，这种方式不依赖 CPU 直接将数据写到了内存中，意味着对内核来说，其实并不知道已经有新数据到了内存中。那么如何让内核知道有新数据进来了呢？答案就是中断，通过中断告诉内核有新数据进来了，并需要进行后续处理。 提到中断，就涉及到硬中断和软中断，首先需要简单了解一下它们的区别： 硬中断：由硬件自己生成，具有随机性，硬中断被 CPU 接收后，触发执行中断处理程序。中断处理程序只会处理关键性的、短时间内可以处理完的工作，剩余耗时较长工作，会放到中断之后，由软中断来完成。硬中断也被称为上半部分。 软中断：由硬中断对应的中断处理程序生成，往往是预先在代码里实现好的，不具有随机性。（除此之外，也有应用程序触发的软中断，与本文讨论的网卡收包无关。）也被称为下半部分。 当 NIC 把数据包通过 DMA 复制到内核缓冲区 sk_buffer 后，NIC 立即发起一个硬件中断。CPU 接收后，首先进入上半部分，网卡中断对应的中断处理程序是网卡驱动程序的一部分，之后由它发起软中断，进入下半部分，开始消费 sk_buffer 中的数据，交给内核协议栈处理。 通过中断，能够快速及时地响应网卡数据请求，但如果数据量大，那么会产生大量中断请求，CPU 大部分时间都忙于处理中断，效率很低。为了解决这个问题，现在的内核及驱动都采用一种叫 NAPI（new API）的方式进行数据处理，其原理可以简单理解为 中断 + 轮询，在数据量大时，一次中断后通过轮询接收一定数量包再返回，避免产生多次中断。 Ring BufferRing Buffer 相关的收消息过程大致如下： NIC (network interface card) 在系统启动过程中会向系统注册自己的各种信息，系统会分配 Ring Buffer 队列也会分配一块专门的内核内存区域给 NIC 用于存放传输上来的数据包。struct sk_buff 是专门存放各种网络传输数据包的内存接口，在收到数据存放到 NIC 专用内核内存区域后，[http://elixir.free-electrons.com/linux/v4.4/source/include/linux/skbuff.h#L706](sk_buff 内有个 data 指针会指向这块内存) Ring Buffer 队列内存放的是一个个 Packet Descriptor ，其有两种状态： ready 和 used 。初始时 Descriptor 是空的，指向一个空的 sk_buff，处在 ready 状态。当有数据时，DMA 负责从 NIC 取数据，并在 Ring Buffer 上按顺序找到下一个 ready 的 Descriptor，将数据存入该 Descriptor 指向的 sk_buff 中，并标记槽为 used。因为是按顺序找 ready 的槽，所以 Ring Buffer 是个 FIFO 的队列。 当 DMA 读完数据之后，NIC 会触发一个 IRQ(中断请求) 让 CPU 去处理收到的数据。因为每次触发 IRQ(中断请求) 后 CPU 都要花费时间去处理 Interrupt Handler，如果 NIC 每收到一个 Packet 都触发一个 IRQ 会导致 CPU 花费大量的时间在处理 Interrupt Handler，处理完后又只能从 Ring Buffer 中拿出一个 Packet，虽然 Interrupt Handler 执行时间很短，但这么做也非常低效，并会给 CPU 带去很多负担。所以目前都是采用一个叫做 New API(NAPI) 的机制，去对 IRQ 做合并以减少 IRQ 次数。 NAPINAPI 主要是让 NIC 的 driver 能注册一个 poll 函数，之后 NAPI 的 subsystem 能通过 poll 函数去从 Ring Buffer 中批量拉取收到的数据。主要事件及其顺序如下： NIC driver 初始化时向 Kernel 注册 poll 函数，用于后续从 Ring Buffer 拉取收到的数据 driver 注册开启 NAPI，这个机制默认是关闭的，只有支持 NAPI 的 driver 才会去开启 收到数据后 NIC 通过 DMA 将数据存到内存 NIC 触发一个 IRQ，并触发 CPU 开始执行 driver 注册的 Interrupt Handler driver 的 Interrupt Handler 通过 napi_schedule 函数触发 softirq (NET_RX_SOFTIRQ) 来唤醒 NAPI subsystem，NET_RX_SOFTIRQ 的 handler 是 net_rx_action 会在另一个线程中被执行，在其中会调用 driver 注册的 poll 函数获取收到的 Packet driver 会禁用当前 NIC 的 IRQ，从而能在 poll 完所有数据之前不会再有新的 IRQ 当所有事情做完之后，NAPI subsystem 会被禁用，并且会重新启用 NIC 的 IRQ 回到第三步 从上面的描述可以看出来还缺一些东西，Ring Buffer 上的数据被 poll 走之后是怎么交付上层网络栈继续处理的呢？以及被消耗掉的 sk_buff 是怎么被重新分配重新放入 Ring Buffer 的呢？ 这两个工作都在 poll 中完成，上面说过 poll 是个 driver 实现的函数，所以每个 driver 实现可能都不相同。但 poll 的工作基本是一致的就是： 从 Ring Buffer 中将收到的 sk_buff 读取出来 对 sk_buff 做一些基本检查，可能会涉及到将几个 sk_buff 合并因为可能同一个 Frame 被分散放在多个 sk_buff 中 将 sk_buff 交付上层网络栈处理 清理 sk_buff，清理 Ring Buffer 上的 Descriptor 将其指向新分配的 sk_buff 并将状态设置为 ready 更新一些统计数据，比如收到了多少 packet，一共多少字节等 软中断处理 遍历 ptype_base 链表，找出 Protocol Layer 中能处理当前数据包的 packet_type 来接着处理数据。所有能处理链路层数据包的协议都会注册到 ptype_base 中。 基本就是在做各种检查以及为 transport 层做一些数据准备。最后如果各种检查都能过，就执行 NF_HOOK。如果有检查不过需要丢弃数据包就会返回 NET_RX_DROP 并在之后会对丢数据包这个事情进行计数。 NF_HOOK 比较神，它实际是 HOOK 到一个叫做 Netfilter 的东西，在这里你可以根据各种规则对数据包做过滤以及对数据包做一些修改。如果 HOOK 执行后返回 1 表示 Netfilter 允许继续处理该数据包，就会进入 ip_rcv_finish，HOOK 没有返回 1 则会返回 Netfilter 的结果，数据包不会继续被处理。 ip_rcv_finish 负责为 sk_buff 从 IP Route System 中找到路由目标，如果是路由到本机则在下一个处理这个 sk_buff 的协议内(比如上层的 TCP/UDP 协议)还需要从 sk_buff 中找到对应的 socket。 参考文档 https://mp.weixin.qq.com/s/0TGH6zZP_psKyZl4JA-3Qw https://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/ https://plantegg.github.io/2019/05/24/%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%B5%81%E8%BD%AC/ https://elixir.bootlin.com/linux/v4.4/source/drivers/net/ethernet/intel/igb/igb_main.c#L6361","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux make编译脚本","slug":"linux-make","date":"2020-07-09T06:53:52.000Z","updated":"2021-12-17T07:42:48.343Z","comments":true,"path":"2020/07/09/linux-make/","link":"","permalink":"https://github.com/fafucoder/2020/07/09/linux-make/","excerpt":"","text":"参考文档 http://www.ruanyifeng.com/blog/2015/02/make.html https://seisman.github.io/how-to-write-makefile/overview.html","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubernetes raft一致性算法","slug":"kubernetes-raft","date":"2020-07-08T09:36:30.000Z","updated":"2021-12-17T07:42:48.338Z","comments":true,"path":"2020/07/08/kubernetes-raft/","link":"","permalink":"https://github.com/fafucoder/2020/07/08/kubernetes-raft/","excerpt":"","text":"参考文档 https://cizixs.com/2017/12/04/raft-consensus-algorithm https://mp.weixin.qq.com/s/2fr1gMcOUcwktP1lz_EGcw https://www.cnblogs.com/mindwind/p/5231986.html https://segmentfault.com/a/1190000016641336 https://zhuanlan.zhihu.com/p/27415397 https://github.com/happyer/distributed-computing","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes dns解析","slug":"kubernetes-dns","date":"2020-07-08T08:44:29.000Z","updated":"2021-12-26T03:23:04.749Z","comments":true,"path":"2020/07/08/kubernetes-dns/","link":"","permalink":"https://github.com/fafucoder/2020/07/08/kubernetes-dns/","excerpt":"","text":"DNS概念 DNS协议的作用是将域名解析成ip地址， 域名只是ip地址的代号 为了将域名解析成ip地址，需要建立一个域名到ip地址之间的映射关系，提供将域名动态解析成ip地址的服务器就叫做DNS服务器， 由于当前故障的原因，DNS无法设计成集中式，因此出现了分层解析架构，分为根服务器，二级域名服务器，三级域名服务器等。 根域名服务器（Root Domain）：用 · （句点）表示，根域名服务器全球的数量是固定的，为 13 个（当然这里的“个”是“组”的意思） 顶级域名服务器（Top-Level Domain, TLD）：指代某个国家/地区或组织使用的类型名称，如 com、cn、edu 等。 次级域名服务器（Second-Level Domain, SLD）：个人或组织在 Internet 上注册的名称，如 qq.com、gitHub.com 等 三次域名服务器或权威域名服务器（如果有）：这层严格来说是次级域名的子域，是二层域名派生的域名，通俗说就是网站名，如 index.baidu.com linux网络中，/etc/hosts用于记录主机hostname跟ip之间的映射关系，/etc/resolve.conf用于记录外网DNS服务器地址 DNS查询过程 以查询www.baidu.com为例子: 本地终端发出域名解析请求，到本地DNS服务器 本地DNS服务器查询DNS缓存是否有对应的Ip, 检查hosts文件中是否与对应的ip地址, 不存在就想外部DNS服务器发送请求(Dnsmasq提供DNS缓存和DHCP服务、Tftp服务功能。当接受到一个DNS请求时，Dnsmasq首先会查找/etc/hosts这个文件，然后查找/etc/resolv.conf中定义的外部DNS) 外部DNS服务器同样先查看缓存，不存在再向众所周知的全球 13 台根服务器发出请求 根 DNS 服务器收到请求后会判断该域名由谁授权管理，返回管理这个域名的顶级 DNS 服务器的 IP，给到DNS服务器 DNS 服务器根据返回的 IP 继续请求顶级 DNS 服务器 顶级 DNS 服务器收到请求后，如果自己无法解析，也会判断对应域名由下一级的哪个 DNS 服务器授权管理，并将该次级 DNS 服务器的 IP 返回 DNS 服务器继续根据返回的 IP 请求次级 DNS 服务器 次级 DNS 服务器如果解析出对应的域名，就将该域名对应的 IP 返回给 DNS 服务器 DNS 服务器缓存一份域名与 IP 的映射关系 DNS 域名记录设置在使用云厂商的域名后进行DNS域名配置时，首先需要选择对应的记录类型，DNS有多种记录类型，包括： A 地址记录， 将域名指向一个 IP 地址(例如：120.55.23.197)； AAAA 地址记录， 将域名指向一个 IPv6 地址； CNAME 别名记录，将域名指向另一个域名，再由另一个域名提供 IP 地址； TXT 域名对应的文本信息， 对域名进行标识和说明，绝大多数的 TXT 记录是用来做 SPF 记录(反垃圾邮件); NS 名字服务器记录, 将子域名交给其他 DNS 服务商解析； SRV TCP服务器信息记录， 用来标识某台服务器使用了某个服务，常见于微软系统的目录管理; MX 邮件服务器记录， 设置邮箱，让邮箱能收到邮件; AFSDB Andrew文件系统数据库服务器记录； ATMA ATM地址记录； HINFO 硬件配置记录，包括CPU、操作系统信息; ISDN 域名对应的ISDN号码; MB 存放指定邮箱的服务器; MG 邮件组记录; MINFO 邮件组和邮箱的信息记录; MR 改名的邮箱记录; PTR 反向记录; RP 负责人记录; RT 路由穿透记录; X25 域名对应的X.25地址记录; 注： 主机记录就是域名前缀，例如www.baidu.com中的www就是域名前缀，比较特殊的主机记录有： @：直接解析主域名，例如@.baidu.com == https://baidu.com ***** : 泛解析，匹配其他所有域名, 例如 *.baidu.com == https://xxxx.baidu.com DNS 查询相关的命令1、nslookup 命令nslookup 是域名查询命令， 用户查询域名对应的ip地址， 命令格式如下： nslookup damain [dns-server] 如果未指定dns服务器时， 采用系统默认的dns服务器 123456789dawn@node-2:~$ nslookup baidu.comServer: 127.0.0.53Address: 127.0.0.53#53Non-authoritative answer: #未认证回答Name: baidu.comAddress: 39.156.69.79Name: baidu.comAddress: 220.181.38.148 nslookup 默认查询的A类型的记录， 可以设置查询指定类型的记录，通过qt=类型查询指定类型的记录，例如：nslookup -qt=mx baidu.com 2、whois 命令whois用于查询域名的注册情况。 12345678910111213141516171819202122232425[root@whois~]# whois baidu.com Domain Name: BAIDU.COM Registry Domain ID: 11181110_DOMAIN_COM-VRSN Registrar WHOIS Server: whois.markmonitor.com Registrar URL: http:&#x2F;&#x2F;www.markmonitor.com Updated Date: 2019-05-09T04:30:46Z Creation Date: 1999-10-11T11:05:17Z Registry Expiry Date: 2026-10-11T11:05:17Z Registrar: MarkMonitor Inc. Registrar IANA ID: 292 Registrar Abuse Contact Email: abusecomplaints@markmonitor.com Registrar Abuse Contact Phone: +1.2083895740 Domain Status: clientDeleteProhibited https:&#x2F;&#x2F;icann.org&#x2F;epp#clientDeleteProhibited Domain Status: clientTransferProhibited https:&#x2F;&#x2F;icann.org&#x2F;epp#clientTransferProhibited Domain Status: clientUpdateProhibited https:&#x2F;&#x2F;icann.org&#x2F;epp#clientUpdateProhibited Domain Status: serverDeleteProhibited https:&#x2F;&#x2F;icann.org&#x2F;epp#serverDeleteProhibited Domain Status: serverTransferProhibited https:&#x2F;&#x2F;icann.org&#x2F;epp#serverTransferProhibited Domain Status: serverUpdateProhibited https:&#x2F;&#x2F;icann.org&#x2F;epp#serverUpdateProhibited Name Server: NS1.BAIDU.COM Name Server: NS2.BAIDU.COM Name Server: NS3.BAIDU.COM Name Server: NS4.BAIDU.COM Name Server: NS7.BAIDU.COM DNSSEC: unsigned URL of the ICANN Whois Inaccuracy Complaint Form: https:&#x2F;&#x2F;www.icann.org&#x2F;wicf&#x2F; 3、 dig 命令Linux下解析域名除了使用nslookup之外，还可以使用dig(全称 domain information groper)命令来解析域名，相对于nslookup, dig命令的输出更为丰富，它会打印出&gt;DNS name server的回应。dig命令的格式如下： 12dig [@server] [-b address] [-c class] [-f filename] [-k filename] [ -n ][-p port#] [-t type] [-x addr] [-y name:key] [name] [type] [class] [queryopt...]dig [global-queryopt...] [query...] 选项： 12345678@server：指定进行域名解析的域名服务器；-b [address]：当主机具有多个IP地址，指定使用本机的哪个IP地址向域名服务器发送域名查询请求；-f [filename]：指定dig以批处理的方式运行，指定的文件中保存着需要批处理查询的DNS任务信息；-P [port]：指定域名服务器所使用端口号；-t [type]：指定要查询的DNS数据类型，缺省查询类型为A记录；-x [address]：执行逆向域名查询(将地址映射到名称)；-4：使用IPv4；-6：使用IPv6； 查询选项： 123456789101112131415161718192021222324+[no]tcp: 查询域名服务器时使用 [不使用] TCP。缺省行为是使用 UDP，除非是 AXFR 或 IXFR 请求，才使用 TCP 连接。 +[no]vc: 查询名称服务器时使用 [不使用] TCP。+[no]tcp 的备用语法提供了向下兼容。vc 代表虚电路。 +[no]ignore: 忽略 UDP 响应的中断，而不是用 TCP 重试。缺省情况运行 TCP 重试。 +domain&#x3D;somename: 设定包含单个域 somename 的搜索列表，好像被 &#x2F;etc&#x2F;resolv.conf 中的域伪指令指定，并且启用搜索列表处理，好像给定了 +search 选项。 +[no]adflag: 在查询中设置 [不设置] AD（真实数据）位。目前 AD 位只在响应中有标准含义，而查询中没有，但是出于完整性考虑在查询中这种性能可以设置。 +[no]cdflag: 在查询中设置 [不设置] CD（检查禁用）位。它请求服务器不运行响应信息的 DNSSEC 合法性。 +[no]recursive: 切换查询中的 RD（要求递归）位设置。在缺省情况下设置该位，也就是说 dig 正常情形下发送递归查询。当使用查询选项 +nssearch 或 +trace 时，递归自动禁用。 +[no]nssearch: 这个选项被设置时，dig 试图寻找包含待搜名称的网段的权威域名服务器，并显示网段中每台域名服务器的 SOA 记录。 +[no]trace: 切换为待查询名称从根名称服务器开始的代理路径跟踪。缺省情况不使用跟踪。一旦启用跟踪，dig 使用迭代查询解析待查询名称。它将按照从根服务器的参照，显示来自每台使用解析查询的服务器的应答。 +[no]cmd: 设定在输出中显示指出 dig 版本及其所用的查询选项的初始注释。缺省情况下显示注释。 +[no]short: 提供简要答复。缺省值是以冗长格式显示答复信息。 +[no]comments: 切换输出中的注释行显示。缺省值是显示注释。 +[no]stats: 查询选项设定显示统计信息：查询进行时，应答的大小等等。缺省显示查询统计信息。 +[no]qr: 显示 [不显示] 发送的查询请求。缺省不显示。 +[no]question: 当返回应答时，显示 [不显示] 查询请求的问题部分。缺省作为注释显示问题部分。 +[no]answer: 显示 [不显示] 应答的回答部分。缺省显示。 +[no]authority: 显示 [不显示] 应答的权限部分。缺省显示。 +[no]additional: 显示 [不显示] 应答的附加部分。缺省显示。 +[no]all: 设置或清除所有显示标志。 +time&#x3D;T: 为查询设置超时时间为T秒。缺省是5秒。如果将T设置为小于1的数，则以1秒作为查询超时时间。 +tries&#x3D;A: 设置向服务器发送UDP查询请求的重试次数为A，代替缺省的3次。如果把A小于或等于0，则采用1为重试次数。 +ndots&#x3D;D: 出于完全考虑，设置必须出现在名称 D 的点数。缺省值是使用在 &#x2F;etc&#x2F;resolv.conf 中的 ndots 语句定义的，或者是 1，如果没有 ndots 语句的话。带更少点数的名称被解释为相对名称，并通过搜索列表中的域或文件 &#x2F;etc&#x2F;resolv.conf 中的域伪指令进行搜索。 +bufsize&#x3D;B: 设置使用 EDNS0 的 UDP 消息缓冲区大小为 B 字节。缓冲区的最大值和最小值分别为 65535 和 0。超出这个范围的值自动舍入到最近的有效值。 +[no]multiline: 以详细的多行格式显示类似 SOA 的记录，并附带可读注释。缺省值是每单个行上显示一条记录，以便于计算机解析 dig 的输出。 查询例子： 12345678910111213141516171819202122232425262728293031323334[root@localhost ~]# dig www.baidu.com; &lt;&lt;&gt;&gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.8 &lt;&lt;&gt;&gt; www.baidu.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 35946;; flags: qr rd ra; QUERY: 1, ANSWER: 3, AUTHORITY: 5, ADDITIONAL: 5;; QUESTION SECTION:;www.baidu.com. IN A;; ANSWER SECTION:www.baidu.com. 837 IN CNAME www.a.shifen.com.www.a.shifen.com. 600 IN A 14.215.177.39www.a.shifen.com. 600 IN A 14.215.177.38;; AUTHORITY SECTION:a.shifen.com. 151 IN NS ns4.a.shifen.com.a.shifen.com. 151 IN NS ns3.a.shifen.com.a.shifen.com. 151 IN NS ns2.a.shifen.com.a.shifen.com. 151 IN NS ns5.a.shifen.com.a.shifen.com. 151 IN NS ns1.a.shifen.com.;; ADDITIONAL SECTION:ns1.a.shifen.com. 143 IN A 110.242.68.42ns2.a.shifen.com. 305 IN A 220.181.33.32ns3.a.shifen.com. 305 IN A 112.80.255.253ns4.a.shifen.com. 273 IN A 14.215.177.229ns5.a.shifen.com. 443 IN A 180.76.76.95;; Query time: 4 msec;; SERVER: 218.85.152.99#53(218.85.152.99);; WHEN: Sat Dec 25 00:37:02 EST 2021;; MSG SIZE rcvd: 260 DNS In Kubernetes在 k8s 中，一个 Pod 如果要访问同 Namespace 下的 Service（比如 user-svc），那么只需要curl user-svc。如果 Pod 和 Service 不在同一域名下，那么就需要在 Service Name 之后添加上 Service 所在的 Namespace（比如 beta），curl user-svc.beta。那么 k8s 是如何知道这些域名是内部域名并为他们做解析的呢？答案是dns resolv.confresolv.conf 是 DNS 域名解析的配置文件。每行都会以一个关键字开头，然后跟配置参数。这里主要使用到的关键词有3个。 nameserver #定义 DNS 服务器的 IP 地址 search #定义域名的搜索列表，当查询的域名中包含的 . 的数量少于 options.ndots 的值时，会依次匹配列表中的每个值 options #定义域名查找时的配置信息 那么我们进入一个 Pod 查看它的 resolv.conf 1231. nameserver 100.64.0.102. search default.svc.cluster.local svc.cluster.local cluster.local3. options ndots:5 上述配置文件 resolv.conf 是 dnsPolicy: ClusterFirst 情况下，k8s 为 Pod 自动生成的，这里的 nameserver 所对应的地址正是 DNS Service 的Cluster IP（该值在启动 kubelet 的时候，通过 clusterDNS 指定）。所以，从集群内请求的所有的域名解析都需要经过 DNS Service 进行解析，不管是 k8s 内部域名还是外部域名。 可以看到这里的 search 域默认包含了 namespace.svc.cluster.local、svc.cluster.local 和 cluster.local 三种。当我们在 Pod 中访问 a Service时（ curl a ），会选择nameserver 100.64.0.10 进行解析，然后依次带入 search 域进行 DNS 查找，直到找到为止。 12# curl aa.default.svc.cluster.local 显然因为 Pod 和 a Service 在同一 Namespace 下，所以第一次 lookup 就能找到。 如果 Pod 要访问不同 Namespace（例如： beta ）下的 Service b （ curl b.beta ），会经过两次 DNS 查找，分别是 123# curl b.beta1. b.beta.default.svc.cluster.local（未找到）2. b.beta.svc.cluster.local（找到） 正是因为 search 的顺序性，所以访问同一 Namespace 下的 Service， curl a 是要比 curl a.default 的效率更高的，因为后者多经过了一次 DNS 解析。 那么当Pod中访问外部域名时仍然需要走search域吗这个答案，不能说肯定也不能说否定，看情况，可以说，大部分情况要走 search 域。 在 /etc/resolv.conf 文件中，我们可以看到 options 中有个配置项 ndots:5 。 ndots:5，表示：如果需要 lookup 的 Domain 中包含少于5个 . ，那么将使用非绝对域名，如果需要查询的 DNS 中包含大于或等于5个 . ，那么就会使用绝对域名。如果是绝对域名则不会走 search 域，如果是非绝对域名，就会按照 search 域中进行逐一匹配查询，如果 search 走完了都没有找到，那么就会使用 原域名.（domgoer.com.） 的方式作为绝对域名进行查找。在 /etc/resolv.conf 文件中，我们可以看到 options 中有个配置项 ndots:5 。 因此,可以找到两种优化的方法 直接使用绝对域名 这是最简单直接的优化方式，可以直接在要访问的域名后面加上 . 如：domgoer.com. ，这样就会避免走 search 域进行匹配。 配置ndots 还记得之前说过 /etc/resolv.conf 中的参数都可以通过k8s中的 dnsConfig 字段进行配置。这就允许你根据你自己的需求配置域名解析的规则。 例如 当域名中包含两个 . 或以上时，就能使用绝对域名直接进行域名解析。 12345678910111213apiVersion: v1kind: Podmetadata:namespace: defaultname: dns-examplespec:containers:- name: test image: nginxdnsConfig: options: - name: ndots value: 2 Kubernetes DNS配置Pod中dns配置主要有dnsConfig跟dnsPolicy, 其中dnsPolicy主要dns网络策略， dnsConfig主要配置dns的resolv.conf配置，详情可以参考官方api dnsConfigdnsConfig通过配置nameserver、search 和 options来修改容器的pod的resolv.conf配置 12345678910dnsConfig: nameservers: - 1.2.3.4 searches: - ns1.svc.cluster.local - my.dns.search.suffix options: - name: ndots value: &quot;2&quot; - name: edns0 dnsPolicy在k8s中，有4中DNS策略，分别是 ClusterFirstWithHostNet、ClusterFirst、Default、和 None，这些策略可以通过dnsPolicy这个字段来定义 如果在初始化 Pod、Deployment 或者 RC 等资源时没有定义，则会默认使用 ClusterFirst 策略 ClusterFirstWithHostNet 当一个 Pod 以 HOST 模式（和宿主机共享网络）启动时，这个 POD 中的所有容器都会使用宿主机的/etc/resolv.conf 配置进行 DNS 查询，但是如果你还想继续使用 Kubernetes 的 DNS 服务，就需要将 dnsPolicy 设置为 ClusterFirstWithHostNet。 ClusterFirst 使用这是方式表示 Pod 内的 DNS 优先会使用 k8s 集群内的DNS服务，也就是会使用 kubedns 或者 coredns 进行域名解析。如果解析不成功，才会使用宿主机的 DNS 配置进行解析。 Default 这种方式，会让 kubelet 来绝定 Pod 内的 DNS 使用哪种 DNS 策略。kubelet 的默认方式，其实就是使用宿主机的 /etc/resolv.conf 来进行解析。你可以通过设置 kubelet 的启动参数，–resolv-conf=/etc/resolv.conf 来决定 DNS 解析文件的地址 None 这种方式顾名思义，不会使用集群和宿主机的 DNS 策略。而是和 dnsConfig 配合一起使用，来自定义 DNS 配置，否则在提交修改时报错。 参考文档 https://blog.domgoer.io/2019/08/07/kube-dns-and-core-dns https://cizixs.com/2017/04/11/kubernetes-intro-kube-dns/ https://juejin.im/entry/5b84a90f51882542e60663cc https://kubernetes.io/zh/docs/concepts/services-networking/dns-pod-service/ https://mp.weixin.qq.com/s/V5qOl5Ure3Pj5aZmTFp-Fg https://sanyuesha.com/2017/11/08/how-dns-resolve-in-linux/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"go逃逸分析","slug":"golang-escape-analysis","date":"2020-07-07T15:47:40.000Z","updated":"2021-12-17T07:42:48.326Z","comments":true,"path":"2020/07/07/golang-escape-analysis/","link":"","permalink":"https://github.com/fafucoder/2020/07/07/golang-escape-analysis/","excerpt":"","text":"堆与栈的概念即使是现代操作系统中，内存依然是计算机中很宝贵的资源，为了充分利用和管理系统内存资源，Linux采用虚拟内存管理技术，利用虚拟内存技术让每个进程都有4GB 互不干涉的虚拟地址空间。 进程初始化分配和操作的都是基于这个「虚拟地址」，只有当进程需要实际访问内存资源的时候才会建立虚拟地址和物理地址的映射，调入物理内存页。 4GB 的进程虚拟地址空间被分成两部分：「用户空间」和「内核空间」 不管是用户空间还是内核空间，使用的地址都是虚拟地址，当需进程要实际访问内存的时候，会由内核的「请求分页机制」产生「缺页异常」调入物理内存页 Linux 内核会将物理内存分为3个管理区分别是: DMA区域, 包含0MB~16MB之间的内存页框，直接映射到内核的地址空间（DMA好熟悉的感觉-^-） 普通内存区域, 包含16MB~896MB之间的内存页框，常规页框，直接映射到内核的地址空间。 高端内存区域。包含896MB以上的内存页框，不进行直接映射，可以通过永久映射和临时映射进行这部分内存页框的访问。 进程（执行的程序）占用的用户空间按照「 访问属性一致的地址空间存放在一起 」的原则，划分成 5个不同的内存区域，分别是 代码段， 数据段， BSS段, 堆 heap, 栈 stack 因此整个用户空间跟内核空间的映射关系如下: 堆与栈的区别 申请方式和回收方式不同 栈（操作系统）：由操作系统自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。堆（操作系统）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收，分配方式倒是类似于链表。 速度不同 栈：由系统自动分配，速度较快。但程序员是无法控制的。 堆：是由new分配的内存，一般速度比较慢，而且容易产生内存碎片不过用起来最方便。 申请大小的限制 栈顶的地址和栈的最大容量是系统预先规定好的， 栈获得的空间较小。 堆的大小受限于计算机系统中有效的虚拟内存， 堆获得的空间比较灵活，也比较大。 go逃逸分析在go中， 一个变量是在堆上分配，还是在栈上分配，是经过编译器的逃逸分析之后得出的结论。 在编译原理中，分析指针动态范围的方法称之为逃逸分析。通俗来讲，当一个对象的指针被多个方法或线程引用时，我们称这个指针发生了逃逸。 编译器会分析代码的特征和代码生命周期，Go中的变量只有在编译器可以证明在函数返回后不会再被引用的，才分配到栈上，其他情况下都是分配到堆上。 123如果函数外部没有引用，则优先放到栈中；如果函数外部存在引用，则必定放到堆中； 结语堆上动态分配内存比栈上静态分配内存，开销大很多。 变量分配在栈上需要能在编译期确定它的作用域，否则会分配到堆上。 Go编译器会在编译期对考察变量的作用域，并作一系列检查，如果它的作用域在运行期间对编译器一直是可知的，那么就会分配到栈上。 简单来说，编译器会根据变量是否被外部引用来决定是否逃逸。对于Go程序员来说，编译器的这些逃逸分析规则不需要掌握，我们只需通过go build-gcflags’-m’命令来观察变量逃逸情况就行了。 不要盲目使用变量的指针作为函数参数，虽然它会减少复制操作。但其实当参数为变量自身的时候，复制是在栈上完成的操作，开销远比变量逃逸后动态地在堆上分配内存少的多。 最后，尽量写出少一些逃逸的代码，提升程序的运行效率。 参考文档 https://mp.weixin.qq.com/s/ashgWyb-w4fT47xX60yNFA https://www.jianshu.com/p/52b5a1879aa1 http://www.cleey.com/blog/single/id/776.html https://mp.weixin.qq.com/s/o2Ugtn-ubQehxnLdgMQYPA","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"golang build方法详解","slug":"golang-build","date":"2020-07-07T04:24:32.000Z","updated":"2021-12-17T07:42:48.325Z","comments":true,"path":"2020/07/07/golang-build/","link":"","permalink":"https://github.com/fafucoder/2020/07/07/golang-build/","excerpt":"","text":"build 参数使用方法usage: go build [-o output] [-i] [build flags] [packages] go build的使用比较简洁，所有的参数都可以忽略，直到只有go build，这个时候意味着使用当前目录进行编译, 因此: go build = go build . = go build main.go go build 主要参数是 -o output 指定编译输出的名称，代替默认的包名。 此外还有一些基础参数，包括install, run, test都能用的参数包括: 123456789101112131415161718192021222324-gcflags &#39;arg list&#39;: 垃圾回收参数-ldflags &#39;-s -w&#39;: 压缩编译后的体积 -s: 去掉符号表 -w: 去掉调试信息，不能gdb调试了-p n 开多少核cpu来并行编译，默认为本机CPU核数.-work 打印临时工作目录的名称，并在退出时不删除它-race 同时检测数据竞争状态，只支持 linux&#x2F;amd64, freebsd&#x2F;amd64, darwin&#x2F;amd64 和 windows&#x2F;amd64.-tags &#39;tag list&#39; 构建出带tag的版本.-linkshared(-buildmode&#x3D;shared) 链接到以前使用创建的共享库-buildmode mode 编译模式(go help buildmode) 其中通过利用 -ldflags 参数，可以向链接器传递指令。向链接器传一个 -X 指令可以设置程序中字符串变量的值。利用这个方法能够实现例如在编译时设置程序的版本信息， 例如： 12345678910111213141516package main import &quot;fmt&quot;import &quot;flag&quot; var _version_ &#x3D; &quot;v0.1&quot; func main() &#123; var version bool flag.BoolVar(&amp;version, &quot;v&quot;, false, &quot;-v&quot;) flag.Parse() if version &#123; fmt.Printf(&quot;Version: %s&quot;, _version_) &#125;&#125; 通过设置 go build -ldflags &#39;-X main._version_=&quot;v0.2&quot;&#39;编译后输出的版本信息就是v0.2, 通过-X参数可以很方便的设置一些频繁改动的变量。需要注意的是-X只能给string类型变量赋值 如果要设置多个变量可以: go build -ldflags &quot;-X importpath.name=value -X importpath_2.name_2=value_2&quot; 如果要赋值的变量包含空格，需要用引号将 -X 后面的变量和值都扩起来：go build -ldflags &quot;-X &#39;importpath.name=a string contains space&#39; -X &#39;importpath_2.name_2=v 通过利用-gcflags参数， 可以做一些逃逸分析等操作，用-gcflags给go编译器传入参数，也就是传给go tool compile的参数，因此可以用go tool compile --help查看所有gcflags可用的参数， 例如： 12345go build -gcflags -gcflags&#x3D;&#39;log&#x3D;-N -l&#39; main.go-N参数代表禁止优化, -l参数代表禁止内联,go在编译目标程序的时候会嵌入运行时(runtime)的二进制,禁止优化和内联可以让运行时(runtime)中的函数变得更容易调试(runtime还是需要好好研读的) Go 交叉编译go build 提供了跨平台编译，默认情况下，都是根据我们当前的机器生成的可执行文件，比如你的是Linux 64位，就会生成Linux 64位下的可执行文件。 可以通过go env 来查看当前的环境变量 12345678910111213141516171819202122232425262728293031323334➜ ~ go envGO111MODULE&#x3D;&quot;on&quot;GOARCH&#x3D;&quot;amd64&quot;GOBIN&#x3D;&quot;&#x2F;Users&#x2F;dawn&#x2F;go&#x2F;bin&quot;GOCACHE&#x3D;&quot;&#x2F;Users&#x2F;dawn&#x2F;Library&#x2F;Caches&#x2F;go-build&quot;GOENV&#x3D;&quot;&#x2F;Users&#x2F;dawn&#x2F;Library&#x2F;Application Support&#x2F;go&#x2F;env&quot;GOEXE&#x3D;&quot;&quot;GOFLAGS&#x3D;&quot;&quot;GOHOSTARCH&#x3D;&quot;amd64&quot;GOHOSTOS&#x3D;&quot;darwin&quot;GOINSECURE&#x3D;&quot;&quot;GONOPROXY&#x3D;&quot;&quot;GONOSUMDB&#x3D;&quot;&quot;GOOS&#x3D;&quot;darwin&quot;GOPATH&#x3D;&quot;&#x2F;Users&#x2F;dawn&#x2F;go&quot;GOPRIVATE&#x3D;&quot;&quot;GOPROXY&#x3D;&quot;https:&#x2F;&#x2F;goproxy.cn,direct&quot;GOROOT&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;Cellar&#x2F;go&#x2F;1.14.2_1&#x2F;libexec&quot;GOSUMDB&#x3D;&quot;sum.golang.google.cn&quot;GOTMPDIR&#x3D;&quot;&quot;GOTOOLDIR&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;Cellar&#x2F;go&#x2F;1.14.2_1&#x2F;libexec&#x2F;pkg&#x2F;tool&#x2F;darwin_amd64&quot;GCCGO&#x3D;&quot;gccgo&quot;AR&#x3D;&quot;ar&quot;CC&#x3D;&quot;clang&quot;CXX&#x3D;&quot;clang++&quot;CGO_ENABLED&#x3D;&quot;1&quot;GOMOD&#x3D;&quot;&#x2F;dev&#x2F;null&quot;CGO_CFLAGS&#x3D;&quot;-g -O2&quot;CGO_CPPFLAGS&#x3D;&quot;&quot;CGO_CXXFLAGS&#x3D;&quot;-g -O2&quot;CGO_FFLAGS&#x3D;&quot;-g -O2&quot;CGO_LDFLAGS&#x3D;&quot;-g -O2&quot;PKG_CONFIG&#x3D;&quot;pkg-config&quot;GOGCCFLAGS&#x3D;&quot;-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length&#x3D;0 -fdebug-prefix-map&#x3D;&#x2F;var&#x2F;folders&#x2F;4g&#x2F;tx2_phhn0gx5j81d1y286bf80000gn&#x2F;T&#x2F;go-build124067622&#x3D;&#x2F;tmp&#x2F;go-build -gno-record-gcc-switches -fno-common&quot; 在上面的环境变量中，与跨平台编译有关的变量有GOOS, GOARCH, GOOS指的是目标操作系统, GOARCH指的是目标处理器的架构。 GOOS可选的值包括: android，darwin（MACOS 10.11和上述和IOS), dragonfly，freebsd，illumos，js， linux，netbsd，，openbsd 和。 plan9solariswindows GOARCH可选的值包括 amd64（64位x86，最成熟的端口）， 386（32位x86），arm（32位ARM），arm64（64位ARM）， ppc64le（PowerPC 64位，little-endian），ppc64（PowerPC 64位（big-endian）， mips64le（MIPS 64位，little-endian），mips64（MIPS 64位，big-endian）， mipsle（MIPS 32位，little-endian），mips（MIPS 32位，big-endian） ）， s390x（IBM System z 64位，big-endian）和 wasm（WebAssembly 32位)。 关于他们的对应关系可以参考官方文档 在交叉编译中通过指定不同的GOOS跟GOARCH，就可以编译成不同平台的可执行文件 例如可以在linux系统下编译成window跟mac的可执行文件 12CGO_ENABLED&#x3D;0 GOOS&#x3D;darwin GOARCH&#x3D;amd64 go build main.goCGO_ENABLED&#x3D;0 GOOS&#x3D;windows GOARCH&#x3D;amd64 go build main.go 可以在mac系统中编译成linux跟windows的可执行文件 12CGO_ENABLED&#x3D;0 GOOS&#x3D;windows GOARCH&#x3D;amd64 go build main.goCGO_ENABLED&#x3D;0 GOOS&#x3D;linux GOARCH&#x3D;amd64 go build main.go 参考文档 https://cloud.tencent.com/developer/article/1410259 https://www.cnblogs.com/Dong-Ge/articles/11276862.html https://blog.csdn.net/u013870094/article/details/78500478 https://blog.csdn.net/zl1zl2zl3/article/details/83374131","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"kubernetes-cilium","slug":"kubernetes-cilium","date":"2020-07-06T06:53:20.000Z","updated":"2021-12-17T07:42:48.333Z","comments":true,"path":"2020/07/06/kubernetes-cilium/","link":"","permalink":"https://github.com/fafucoder/2020/07/06/kubernetes-cilium/","excerpt":"","text":"cilium1.8 安装默认安装1234567891011121314helm install cilium cilium&#x2F;cilium \\ --namespace kube-system \\ --set global.nodeinit.enabled&#x3D;true \\ --set global.kubeProxyReplacement&#x3D;partial \\ --set global.hostServices.enabled&#x3D;false \\ --set global.externalIPs.enabled&#x3D;true \\ --set global.nodePort.enabled&#x3D;true \\ --set global.hostPort.enabled&#x3D;true \\ --set global.pullPolicy&#x3D;IfNotPresent \\ --set config.ipam&#x3D;kubernetes \\ --set global.hubble.enabled&#x3D;true \\ --set global.hubble.listenAddress&#x3D;&quot;:10000&quot; \\ --set global.hubble.relay.enabled&#x3D;true \\ --set global.hubble.ui.enabled&#x3D;true cni-chaining安装方式123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: v1data: cni-config: |- &#123; &quot;name&quot;: &quot;generic-veth&quot;, &quot;cniVersion&quot;: &quot;0.3.1&quot;, &quot;plugins&quot;: [ &#123; &quot;type&quot;: &quot;kube-ovn&quot;, &quot;log_level&quot;: &quot;info&quot;, &quot;datastore_type&quot;: &quot;kubernetes&quot;, &quot;mtu&quot;: 1400, &quot;ipam&quot;: &#123; &quot;type&quot;: &quot;kube-ovn&quot;, &quot;server_socket&quot;: &quot;&#x2F;run&#x2F;openvswitch&#x2F;kube-ovn-daemon.sock&quot; &#125;, &quot;policy&quot;: &#123; &quot;type&quot;: &quot;k8s&quot; &#125;, &quot;kubernetes&quot;: &#123; &quot;kubeconfig&quot;: &quot;&#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;01-kube-ovn.conflist&quot; &#125; &#125;, &#123; &quot;type&quot;: &quot;cilium-cni&quot; &#125; ] &#125;kind: ConfigMapmetadata: name: cni-configuration namespace: kube-systemhelm install cilium cilium&#x2F;cilium --version 1.8.1 \\ --namespace&#x3D;kube-system \\ --set global.cni.chainingMode&#x3D;generic-veth \\ --set global.cni.customConf&#x3D;true \\ --set global.cni.configMap&#x3D;cni-configuration \\ --set global.tunnel&#x3D;disabled \\ --set global.masquerade&#x3D;false --set global.hubble.enabled&#x3D;true \\ --set global.hubble.listenAddress&#x3D;&quot;:10000&quot; \\ --set global.hubble.relay.enabled&#x3D;false \\ --set global.hubble.ui.enabled&#x3D;true 安装prometheus123456789101112131415161718helm install cilium cilium&#x2F;cilium --version 1.8.1 \\ --namespace&#x3D;kube-system \\ --set global.cni.chainingMode&#x3D;generic-veth \\ --set global.cni.customConf&#x3D;true \\ --set global.cni.configMap&#x3D;cni-configuration \\ --set global.tunnel&#x3D;disabled \\ --set global.masquerade&#x3D;false \\ --set global.hubble.enabled&#x3D;true \\ --set global.hubble.listenAddress&#x3D;&quot;:4244&quot; \\ --set global.hubble.relay.enabled&#x3D;true \\ --set global.hubble.ui.enabled&#x3D;true \\ --set global.kubeProxyReplacement&#x3D;strict \\ --set global.k8sServiceHost&#x3D;172.24.37.57 \\ --set global.k8sServicePort&#x3D;6443 \\ --set global.prometheus.enabled&#x3D;true \\ --set global.operatorPrometheus.enabled&#x3D;true \\ --set global.hubble.enabled&#x3D;true \\ --set global.hubble.metrics.enabled&#x3D;&quot;&#123;dns,drop,tcp,flow,port-distribution,icmp,http&#125;&quot; 参考文档安装文档相关 https://blog.csdn.net/F8qG7f9YD02Pe/article/details/79815702 https://cilium.io/blog/2020/05/04/guest-blog-kubernetes-cilium/ https://www.jianshu.com/p/090c3d32c2be tc相关 https://cloud.tencent.com/developer/article/1409664 https://tonydeng.github.io/sdn-handbook/linux/tc.html bpf相关 https://blog.csdn.net/pwl999/article/details/82884882 xdp相关 https://cloud.tencent.com/developer/article/1484793 系列博客 https://davidlovezoe.club/wordpress/ cilium 网卡收发包路径 https://mp.weixin.qq.com/s/Lus1ghqtYS_LxSHBO6XCCQ // 深入理解网卡的收发包路径","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"linux虚拟网络设备","slug":"linux-veth-pair","date":"2020-06-14T11:56:41.000Z","updated":"2021-12-17T07:42:48.352Z","comments":true,"path":"2020/06/14/linux-veth-pair/","link":"","permalink":"https://github.com/fafucoder/2020/06/14/linux-veth-pair/","excerpt":"","text":"veth-pair概念Linux 虚拟网络的背后都是由一个个的虚拟设备构成的。虚拟化技术没出现之前，计算机网络系统都只包含物理的网卡设备，通过网卡适配器，线缆介质，连接外部网络，构成庞大的 Internet。 veth-pair 是成对出现的一种虚拟网络设备，一端连接着协议栈，一端连接着彼此，数据从一端出，从另一端进。它的这个特性常常用来连接不同的虚拟网络组件，构建大规模的虚拟网络拓扑，比如连接 Linux Bridge、OVS、LXC 容器等。 创建veth-pair通过iproute2包中的ip link跟ip netns可以创建一个完整的veth-pair 123456789101112131415161718192021# 创建veth pairsudo ip link add veth0 type veth peer name veth1# 设置命名空间sudo ip netns add ns1sudo ip netns add ns2# 把veth pair移到命名空间下面sudo ip link set veth0 netns ns1sudo ip link set veth1 netns ns2# 查看命名空间网卡设备sudo ip netns exec ns1 ip link show# veth-pair设置ip地址sudo ip netns exec ns1 ip address add 192.168.1.1&#x2F;24 dev veth0sudo ip netns exec ns2 ip address add 192.168.1.2&#x2F;24 dev veth1# 设置网卡为up状态sudo ip netns exec ns1 ip link set veth0 upsudo ip netns exec ns2 ip link set veth1 up 通过bridge连接veth-pairLinux Bridge 相当于一台交换机，可以中转两个 namespace 的流量 12345678910111213141516171819202122232425262728293031# 创建bridgesudo ip link add br0 type bridgesudo ip link set br0 up# 创建veth-pairsudo ip link add veth0 type veth peer name br0-veth0sudo ip link add veth1 type veth peer name br0-veth1# 把peer name 移到bridge中sudo ip link set br0-veth0 master br0sudo ip link set br0-veth1 master br0# 设置peer name 为upsudo ip link set br0-veth0 upsudo ip link set br0-veth1 up# 创建namespacesudo ip netns add ns1sudo ip netns add ns2# 把peer name移到 namespace中sudo ip link set veth0 netns ns1sudo ip link set veth1 netns ns2# veth-pair设置ip地址sudo ip netns exec ns1 ip address add 192.168.1.1&#x2F;24 dev veth0sudo ip netns exec ns2 ip address add 192.168.1.2&#x2F;24 dev veth1# 设置网卡为up状态sudo ip netns exec ns1 ip link set veth0 upsudo ip netns exec ns2 ip link set veth1 up 经过以上流程，发现还是无法ping通， 原因待查 ping原理 参考文档 https://ctimbai.github.io/tags/veth-pair","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux网络相关命令十全大补丸","slug":"linux-network-command","date":"2020-06-06T11:19:10.000Z","updated":"2021-12-17T07:42:48.344Z","comments":true,"path":"2020/06/06/linux-network-command/","link":"","permalink":"https://github.com/fafucoder/2020/06/06/linux-network-command/","excerpt":"","text":"网卡相关网卡相关的命令包括查看网卡状态，更改网卡ip等，主要命令包括ip， ifconfig等。 ifconfig 和ipethtool等，ifconfig属于net-tool包里面的一个命令 ， ip 是iproute2包 包含的命令，ip命令更为强大 ifconfig命令显示的信息比较好看，个人更喜欢使用ifconfig查看网卡信息等， iproute2跟net-tool的对照表如下所示: ip 跟 ifconfig命令对照表如下: 操作 ip ifconfig 备注 显示所有的网卡 ip a/ip link show ifconfig -a 启用或停用网卡 ip link set eth1 down/up ifconfig eth1 down/up 显示网卡详细信息 ip link show eth1 ifconfig eth1 设置网卡混杂模式 ip link set eth1 promisc on/off ifconfig eth1 promisc/-promisc 设置网卡MTU ip link set eth1 mtu 1400 ifconfig eth1 mtu 1400 设置网卡所属命名空间 ip link set eth1 netns pid/name 关于网络命名空间，ip提供了强大的netns命令， ip netns 可查看命令详情 配置网卡ipv4地址 ip addr add 10.0.0.1/24 dev eth1 ifconfig eth1 10.0.0.1/24 ip添加命令dev一定要加， ip add可以添加多个地址(ip addr add 10.0.0.1/24 broadcast 10.0.0.255 dev eth1, ) ip addr add 10.0.0.2/24 broadcast 10.0.0.255 dev eth1，ifconfig则不行 修改网卡ipv4地址 ip addr change 10.0.0.2/24 dev eth1 ifconfig eth1 10.0.0.2/24 ifconfig 添加跟修改命令一致 移除网卡ipv4地址 ip addr del 10.0.0.1/24 dev eth1 ifconfig eth1 0 ip 命令可以添加多个地址，所以删除需要对应的地址 配置网卡ipv6地址 ip -6 addr add 2002:0db5:0:f102::1/64 dev eth1 ifconfig eth1 inet6 add 2002:0db5:0:f102::1/64 修改网卡ipv4地址 ip -6 addr change 2002:0db5:0:f102::2/64 dev eth1 ifconfig eth1 inet6 add 2002:0db5:0:f102::2/64 移除网卡ipv6地址 ip -6 addr del 2002:0db5:0:f102::1/64 dev eth1 ifconfig eth1 inet6 del 2002:0db5:0:f102::1/64 对比以上表格命令可以得出，要修改网卡地址使用的是ip addr, 要设置网卡的配置信息使用的是ip link 网卡的命令空间操作使用 ip netns命令 ethtoolethtool命令主要是查看网卡详细信息,命令比较简单ethtool -params eth例如 123456789101112131415161718192021222324252627root@node-2:&#x2F;home&#x2F;dawn# ethtool enp0s9Settings for enp0s9: Supported ports: [ TP ] Supported link modes: 10baseT&#x2F;Half 10baseT&#x2F;Full 100baseT&#x2F;Half 100baseT&#x2F;Full 1000baseT&#x2F;Full &#x2F;&#x2F;支持模式 Supported pause frame use: No Supports auto-negotiation: Yes &#x2F;&#x2F;支持自动协商 Supported FEC modes: Not reported Advertised link modes: 10baseT&#x2F;Half 10baseT&#x2F;Full 100baseT&#x2F;Half 100baseT&#x2F;Full 1000baseT&#x2F;Full Advertised pause frame use: No Advertised auto-negotiation: Yes Advertised FEC modes: Not reported Speed: 1000Mb&#x2F;s &#x2F;&#x2F;速率 Duplex: Full &#x2F;&#x2F;工作模式(全双工，半双工...) Port: Twisted Pair PHYAD: 0 Transceiver: internal Auto-negotiation: on &#x2F;&#x2F;自动协商打开 MDI-X: off (auto) Supports Wake-on: umbg Wake-on: d Current message level: 0x00000007 (7) drv probe link Link detected: yes ethtool常见命令如下表： 参数 说明 示例 -a 查看网卡中接收模块RX、发送模块TX和Autonegotiate模块的状态：启动on 或 停用off。 ethtool -a enp0s9 -A 修改网卡中 接收模块RX、发送模块TX和Autonegotiate模块的状态：启动on 或 停用off。 ethtool -A enp0s9 rx off -i 显示网卡驱动的信息，如驱动的名称、版本等。 ethtool -i enp0s9 -s 修改网卡的部分配置，包括网卡速度、单工/全双工模式、mac地址等 ethtool -s enp0s9 advertise N -S 显示NIC- and driver-specific 的统计参数，如网卡接收/发送的字节数、接收/发送的广播包个数等。 ethtool -S enp0s9 路由相关路由相关的命令包括防火墙，路由查看等， iptables之前已有记录，这里不做记录。 routeroute属于net-tool包里的命令，ip route 是iproute2里面的命令。 路由格式如下： 123456789root@node-2:&#x2F;home&#x2F;dawn# routeKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Ifacedefault 192.168.3.1 0.0.0.0 UG 100 0 0 enp0s3172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0192.168.3.0 0.0.0.0 255.255.255.0 U 0 0 0 enp0s3192.168.3.1 0.0.0.0 255.255.255.255 UH 100 0 0 enp0s3192.168.56.0 0.0.0.0 255.255.255.0 U 0 0 0 enp0s8192.168.57.0 0.0.0.0 255.255.255.0 U 0 0 0 enp0s9 各字段表示如下： 字段 说明 Destination 目标网段或者主机 Gateway 网关地址，”*” 表示目标是本主机所属的网络，不需要路由 Genmask 网络掩码 Flags 标记。一些可能的标记如下： U — 路由是活动的 H — 目标是一个主机 G — 路由指向网关 R — 恢复动态路由产生的表项 D — 由路由的后台程序动态地安装 M — 由路由的后台程序修改 ! — 拒绝路由 Metric 路由距离，到达指定网络所需的中转数（linux 内核中没有使用） Ref 路由项引用次数（linux 内核中没有使用） Use 此路由项被路由软件查找的次数 Iface 该路由表项对应的输出接口 route 命令操作: route [add|del] [-net|-host] target [netmask Nm] [gw Gw] [[dev] If] route常见的操作包括添加路由表，删除路由表等， 操作如下表: 操作 net-tool iproute2 备注 显示所有路由 route ip route 添加网络路由 route add -net 192.168.55.0 netmask 255.255.255.0 eth3 / route add -net 192.168.1.0/24 eth1 添加默认路由 route add default gw 192.168.1.1 只能是gw 地址 添加主机路由 route add -host 192.168.1.2 dev eth0 添加网络路由指定默认网关 route add -net 10.20.30.48 netmask 255.255.255.248 gw 10.20.30.41 添加默认路由指定默认网关 route add -host 10.20.30.148 gw 10.20.30.40 删除网络路由 route del -net 10.20.30.40 netmask 255.255.255.248 eth0 删除默认路由 route del default gw 192.168.1.1 删除主机路由 route del -host 192.168.1.2 dev eth0 删除网络路由指定默认网关 route del -net 10.20.30.48 netmask 255.255.255.248 gw 10.20.30.41 删除主机路由指定默认网关 route del -host 10.20.30.148 gw 10.20.30.40 内核模块相关内核模块相关的命令包括查看相关模块是否加载，加载相关模块等 lsmodlsmod 列出内核已载入模块的状态,例如 123456root@node-2:&#x2F;home&#x2F;dawn# lsmod | grep vfiovfio_pci 45056 0vfio_virqfd 16384 1 vfio_pciirqbypass 16384 1 vfio_pcivfio_iommu_type1 24576 0vfio 28672 2 vfio_iommu_type1,vfio_pci linux 内核模块加载比较多， 一般配置grep 来检测指定模块是否加载 modprobemodprobe一般加载、去除linux内核模块， 例如modprobe vfio, 主要命令有 modprobe vfio: 添加vfio模块 modprobe -r vfio: 移除vfio模块 modinfomodinfo 用于查看内核模块信息, 例如 1234567891011121314151617181920root@node-2:&#x2F;home&#x2F;dawn# modinfo vfiofilename: &#x2F;lib&#x2F;modules&#x2F;4.15.0-101-generic&#x2F;kernel&#x2F;drivers&#x2F;vfio&#x2F;vfio.kosoftdep: post: vfio_iommu_type1 vfio_iommu_spapr_tcealias: devname:vfio&#x2F;vfioalias: char-major-10-196description: VFIO - User Level meta-driverauthor: Alex Williamson &lt;alex.williamson@redhat.com&gt;license: GPL v2version: 0.3srcversion: 53C6B01E26FFBE3618E4286depends: retpoline: Yintree: Yname: vfiovermagic: 4.15.0-101-generic SMP mod_unload signat: PKCS#7signer: sig_key: sig_hashalgo: md4parm: enable_unsafe_noiommu_mode:Enable UNSAFE, no-IOMMU mode. This mode provides no device isolation, no DMA translation, no host kernel protection, cannot be used for device assignment to virtual machines, requires RAWIO permissions, and will taint the kernel. If you do not know what this is for, step away. (default: false) (bool) lspcilspci 用于在系统中显示有关pci总线的信息以及连接到它们的设备。 默认情况下，它显示了一个简单的设备列表。使用下面描述的选项可以请求更详细的输出或其他程序用于解析的输出。 如果要报告PCI设备驱动程序或lspci本身中的bug，请使用选项“lspci-vvx”或更好的“lspci-vvxxx”的输出(不过，可能会有警告)。 例如查看物理网卡信息： 1234567root@node-2:&#x2F;home&#x2F;dawn# lspci | grep Eth00:03.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 02)00:08.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 02)00:09.0 Ethernet controller: Intel Corporation 82540EM Gigabit Ethernet Controller (rev 02)root@node-2:&#x2F;home&#x2F;dawn# lspci -n -s 00:09.000:09.0 0200: 8086:100e (rev 02) lspci 常见参数如下表: 参数 描述 如何使用 备注 -n 显示pci设备的厂商和设备代码 lspci -n -s 00:09.0 一般配置-s参数使用显示pci设备的设备驱动代码 -nn 更加详细显示pci设备的厂商和设备代码 lspci -nn -s 00:09.0 -m 以向后兼容并且机器可读的方式转储设备信息 lspci -m -mm 以机器可读的方式转储设备信息，以便脚本解析 lspci -mm -t 以树形结构显示pci设备的层次关系，包含所有总线、桥梁、设备和它们之间的连接 lspci -t -v 显示所有设备的详细信息 lscpi -v -vv 以更加详细的方式显示设备信息 lspci -vv 连通测试连通测试主要测试目的主机(服务器)是否可达，常见的有telnet，traceroute，tcpdump等 telnettelnet 可用于查看远方服务器端口是否开放，目的主机是否可达等,例如： telnet 192.168.56.100 22 traceroutetraceroute 用于跟踪到达目的主机过程中所经过的路径,例如: traceroute 192.168.56.100 tcpdumptcpdump 用于监听网卡收包情况, tcpdump 常见命令如下表： 参数 描述 说明 -i 指定监听网络接口 tcpdump -i eth0 (抓取eth0包) -p 不将网络接口设置成混杂模式 -c 指定监听数据包数量，当收到指定的包的数目后，tcpdump就会停止 -n 不把网络地址转换成名字 tcpdump -ni em2 -vv -e -e 在输出行打印出数据链路层的头部信息 tcpdump -ni em2 -vv -e -d 将匹配信息包的代码以人们能够理解的汇编格式给出 -vv 输出详细的报文信息 tcpdump -ni em2 -vv -e tcpdump expressiontcpdump expression 用于tcpdump 数据包过滤，如果不过滤的话数据包很多，不好排查，因此需要用expression 进行数据包过滤. tcpdump利用正则表达式作为过滤报文的条件，如果数据包满足表达式的条件，则会被捕获。如果没有给出任何条件，则网络上所有的数据包将会被截获。表达式中常用关键字如下： (1) 指定参数类型的关键字，主要包括host，net，port等，如果没有指定类型，缺省的类型是host；例如：#tcpdump host 222.24.20.86 截获ip为222.24.20.86的主机收发的所有数据包 (2) 指定数据报文传输方向的关键字，主要包括src , dst ,dst or src, dst and src，缺省为src or dst；例如：#tcpdump src net 222.24.20.1 截取源网络地址为 222.24.20.1 的所有数据包 (3) 指定协议的关键字，主要包括fddi，ip，arp，rarp，tcp，udp等类型，默认监听所有协议的数据包；例如：#tucpdump arp 截获所有arp协议的数据包 (4) 其他重要的关键字还有，gateway，broadcast，less，greater，三种逻辑运算（取非运算是 ‘not ‘,’! ‘； 与运算是 ‘and ‘,’&amp;&amp; ‘；或运算 是’or ‘,’|| ‘）等。这些关键字的巧妙组合，能灵活构造过滤条件，从而满足用户需要。例如：#tcpdump host ubuntu and src port (80 or 8080) 截取主机ubuntu上源端口为80或8080的所有数据包。 expression 的整体结构 expr relop expr relop表示关系操作符，可以为&gt;, &lt; ,&gt;=,&lt;=, =, !=之一。 参考文档 https://blog.csdn.net/u011956172/article/details/54617516 //net-tool跟iproute2对照表 https://www.cnblogs.com/baiduboy/p/7278715.html //route指令详解 https://www.cnblogs.com/jacklikedogs/p/4659249.html //modprobe, lsmod命令使用 https://yq.aliyun.com/articles/653209 lspci命令使用","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubectl常用命令","slug":"kubernetes-kubectl","date":"2020-06-06T06:43:28.000Z","updated":"2021-12-17T07:42:48.336Z","comments":true,"path":"2020/06/06/kubernetes-kubectl/","link":"","permalink":"https://github.com/fafucoder/2020/06/06/kubernetes-kubectl/","excerpt":"","text":"参考文档 https://www.cnblogs.com/weifeng1463/p/10359581.html //drain https://blog.csdn.net/miss1181248983/article/details/88181434 https://zdyxry.github.io/2020/02/15/K8s-drain-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/#more","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes垃圾收集与finalizer","slug":"kubernetes-finalizers","date":"2020-06-04T04:26:14.000Z","updated":"2021-12-17T07:42:48.335Z","comments":true,"path":"2020/06/04/kubernetes-finalizers/","link":"","permalink":"https://github.com/fafucoder/2020/06/04/kubernetes-finalizers/","excerpt":"","text":"声明式API声明式表示的是：告诉k8s你要的是什么，而不是告诉他怎么做的命令。 在 K8s 里面，声明式的体现就是 kubectl apply 命令，在对象创建和后续更新中一直使用相同的 apply 命令，告诉 K8s 对象的终态即可，底层是通过执行了一个对原有 API 对象的 PATCH 操作来实现的，可以一次性处理多个写操作，具备 Merge 能力 diff 出最终的 PATCH，而命令式一次只能处理一个写请求。 finalizer 简介在一般情况下，如果资源被删除之后，我们虽然能够被触发删除事件，但是这个时候从 Cache 里面无法读取任何被删除对象的信息，这样一来，导致很多垃圾清理工作因为信息不足无法进行，K8s 的 Finalizer 字段用于处理这种情况。在 K8s 中，只要对象 ObjectMeta 里面的 Finalizers 不为空，对该对象的 delete 操作就会转变为 update 操作，具体说就是 update deletionTimestamp 字段，其意义就是告诉 K8s 的 GC“在deletionTimestamp 这个时刻之后，只要 Finalizers 为空，就立马删除掉该对象”。 所以一般的使用姿势就是在创建对象时把 Finalizers 设置好（任意 string），然后处理 DeletionTimestamp 不为空的 update 操作（实际是 delete），根据 Finalizers 的值执行完所有的 pre-delete hook（此时可以在 Cache 里面读取到被删除对象的任何信息）之后将 Finalizers 置为空即可。 参考文档 https://zhuanlan.zhihu.com/p/84051705 https://zdyxry.github.io/2019/09/13/Kubernetes-%E5%AE%9E%E6%88%98-Operator-Finalizers/?hmsr=codercto.com&amp;utm_medium=codercto.com&amp;utm_source=codercto.com https://kubernetes.io/zh/docs/concepts/workloads/controllers/garbage-collection/ https://draveness.me/kubernetes-garbage-collector/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"docker网络冲突问题","slug":"docker-network-conflict","date":"2020-05-17T08:59:16.000Z","updated":"2021-12-17T07:42:48.324Z","comments":true,"path":"2020/05/17/docker-network-conflict/","link":"","permalink":"https://github.com/fafucoder/2020/05/17/docker-network-conflict/","excerpt":"","text":"问题描述docker0网桥的默认网段为172.17.0.0/16, 某些虚拟机的内网地址为172.17.0.0/16,结果导致网段冲突，不同虚拟机无法互通 123dawn@node-1:~$ ip route list172.17.0.0&#x2F;16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown... 解决办法, 修改网桥ip地址段 方法一(临时性): 去除docker0的路由 sudo ip route del 172.17.0.0/16 dev docker0 方法二(永久): 修改网桥ip地址段: 在daemon.json中，添加”bip”:”192.168.100.1/24”配置，重启docker 再发现问题由于有的虚拟机会配置&quot;live-restore&quot;: true, 所以在daemon.json中修改docker0的默认网段，重启docker发现还是没有生效。遇到这个问题只能重启虚拟机了。 1Restart the Docker daemon. On Linux, you can avoid a restart (and avoid any downtime for your containers) by reloading the Docker daemon. If you use systemd, then use the command systemctl reload docker. Otherwise, send a SIGHUP signal to the dockerd process. 发送SIGHUP信号量是没法生效的，只能重启才可以生效 参考文档 https://docs.docker.com/config/containers/live-restore/ //官方文档 https://www.dazhuanlan.com/2019/12/24/5e01dd6bb1206 //如何优雅的重启dockerd","categories":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/tags/docker/"}]},{"title":"kubernetes 亲和性和反亲和性调度","slug":"kubernetes-affinity","date":"2020-05-17T07:23:09.000Z","updated":"2021-12-17T07:42:48.333Z","comments":true,"path":"2020/05/17/kubernetes-affinity/","link":"","permalink":"https://github.com/fafucoder/2020/05/17/kubernetes-affinity/","excerpt":"","text":"概述kubernetes 提供将pod限制在指定的node上运行，或者指定更倾向于在某些特定的node上运行，有几种方式实现这个功能： NodeName: 最简单的节点选择方式，直接指定节点，跳过调度器。 NodeSelector: 早期的简单控制方式，直接通过键—值对将 Pod 调度到具有特定 label 的 Node 上。 NodeAffinity: NodeSelector 的升级版，支持更丰富的配置规则，使用更灵活。(NodeSelector 将被淘汰.) PodAffinity: 根据已在节点上运行的 Pod 标签来约束 Pod 可以调度到哪些节点，而不是根据 node label。 NodeNamenodeName 是 PodSpec 的一个字段，用于直接指定调度节点，并运行该 pod。调度器在工作时，实际选择的是 nodeName 为空的 pod 并进行调度然后再回填该 nodeName，所以直接指定 nodeName 实际是直接跳过了调度器。换句话说，指定 nodeName 的方式是优于其他节点选择方法。 123456789apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx nodeName: kube-01 NodeSelectornodeSelector 也是 PodSpec 中的一个字段，指定键—值对的映射。如果想要将 pod 运行到对应的 node 上，需要先给这些 node 打上 label，然后在 podSpec.NodeSelector 指定对应 node labels 即可。 12345678910apiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx nodeSelector: type: gpu Affinityaffinity格式如下： 12345678910affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: requiredDuringSchedulingIgnoredDuringExecution: podAffinity: preferredDuringSchedulingIgnoredDuringExecution: requiredDuringSchedulingIgnoredDuringExecution: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: requiredDuringSchedulingIgnoredDuringExecution: Affinity 当前支持两种调度模式: requiredDuringSchedulingIgnoredDuringExecution: 一定要满足的条件，如果没有找到满足条件的节点，则 Pod 创建失败。所有也称为hard 模式。 preferredDuringSchedulingIgnoredDuringExecution: 优先选择满足条件的节点，如果没有找到满足条件的节点，则在其他节点中择优创建 Pod。所有也称为 soft 模式。 其中IgnoredDuringExecution的意义就跟 nodeSelector 的实现一样，即使 node label 发生变更，也不会影响之前已经部署且又不满足 affinity rules 的 pods，这些 pods 还会继续在该 node 上运行。换句话说，亲和性选择节点仅在调度 Pod 时起作用。 nodeAffinity 当前支持的匹配符号包括：In、NotIn、Exists、DoesNotExists、Gt、Lt 。 NodeAffinity1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: with-node-affinityspec: affinity: nodeAffinity: # 必须选择 node label key 为 kubernetes.io&#x2F;e2e-az-name, # value 为 e2e-az1 或 e2e-az2. requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io&#x2F;e2e-az-name operator: In values: - e2e-az1 - e2e-az2 # 过滤掉上面的必选项后，再优先选择 node label key 为 another-node-label-key # value 为 another-node-label-value. preferredDuringSchedulingIgnoredDuringExecution: # 如果满足节点亲和，积分加权重(优选算法，会对 nodes 打分) # weight: 0 - 100 - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: k8s.gcr.io&#x2F;pause:2.0 NodeAffinity 的结构体: 12345678910111213type NodeAffinity struct &#123; RequiredDuringSchedulingIgnoredDuringExecution *NodeSelector PreferredDuringSchedulingIgnoredDuringExecution []PreferredSchedulingTerm&#125;type NodeSelector struct &#123; NodeSelectorTerms []NodeSelectorTerm&#125;type NodeSelectorTerm struct &#123; MatchExpressions []NodeSelectorRequirement MatchFields []NodeSelectorRequirement&#125; 配置相关的注意点： 如果 nodeSelector 和 nodeAffinity 两者都指定，那 node 需要两个条件都满足，pod 才能调度。 如果指定了多个 NodeSelectorTerms，那 node 只要满足其中一个条件，pod 就可以进行调度。 如果指定了多个 MatchExpressions，那必须要满足所有条件，才能将 pod 调度到该 node。 PodAffinity跟NodeAffinity的调度方式类似 参考文档 https://segmentfault.com/a/1190000018446833 //Kubernetes 亲和性调度","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes taint(污点)跟toleration(容忍)","slug":"kubernetes-taint","date":"2020-05-17T02:35:32.000Z","updated":"2021-12-17T07:42:48.339Z","comments":true,"path":"2020/05/17/kubernetes-taint/","link":"","permalink":"https://github.com/fafucoder/2020/05/17/kubernetes-taint/","excerpt":"","text":"taint命令添加taint123456kubectl taint node [node] key&#x3D;value:[ NoSchedule | PreferNoSchedule | NoExecute ]例如:➜ ~ kubectl taint node central key1&#x3D;value1:NoSchedule ➜ ~ kubectl taint node central key2&#x3D;value2:NoExecute ➜ ~ kubectl taint node central key3&#x3D;value3:PreferNoSchedule 其中： Noschedule: 一定不能被调度 PreferNoSchedule: 尽量不要被调度 NoExecute: 不仅不会调度，还会驱逐Node上已有的pod 查看taint12345kubectl describe node node 或者 kubectl get node nodeName -o go-template&#x3D;&#123;&#123;.spec.taints&#125;&#125;例如➜ ~ kubectl get node central -o go-template&#x3D;&#123;&#123;.spec.taints&#125;&#125;[map[effect:PreferNoSchedule key:key3 value:value3] map[effect:NoExecute key:key2 value:value2] map[effect:NoSchedule key:key1 value:value1]] 删除taint12345kubectl taint node nodename key:[NoSchedule | PreferNoSchedule | NoExecute]- 或者 kubectl taint node nodename key- &#x2F;&#x2F;如果key是唯一的可以不指定调度方式例如➜ ~ kubectl taint node central key2:NoExecute-➜ ~ kubectl taint node central key1- kubernetes master节点kubernetes 的master节点默认添加了node-role.kubernetes.io/master=true的NoSchedule taints, 因此所有的pod都不会调度到master节点上，如果要去除这个taint可以执行如下命令, 完成后master就可以按正常的node节点被调度了 1kubectl taint node nodename node-role.kubernetes.io&#x2F;master- 如果需要为master添加回taint可以执行以下命令： 1kubectl taint nodes nodeName node-role.kubernetes.io&#x2F;master&#x3D;:NoSchedule taint test pod 为node 设置taint 1kubectl taint node central key1&#x3D;value1:NoSchedule 创建taint pod 123456789101112apiVersion: v1kind: Podmetadata: name: taint-test labels: key1: value1spec: containers: - name: nginx image: nginx ports: - containerPort: 80 结果 123➜ yamls kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStaint-test 0&#x2F;1 Pending 0 4m16s &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt; taint跟toleration含义NodeAffinity节点亲和性，是Pod上定义的一种属性，使Pod能够按我们的要求调度到某个Node上，而Taints则恰恰相反，它可以让Node拒绝运行Pod，甚至驱逐Pod。 Taints(污点)是Node的一个属性，设置了Taints(污点)后，因为有了污点，所以Kubernetes是不会将Pod调度到这个Node上的，于是Kubernetes就给Pod设置了个属性Tolerations(容忍)，只要Pod能够容忍Node上的污点，那么Kubernetes就会忽略Node上的污点，就能够(不是必须)把Pod调度过去。因此 Taints(污点)通常与Tolerations(容忍)配合使用。 kubectl taint node nodename key=value:NoSchedule 表示此节点已被key=value污染，Pod调度不允许（PodToleratesNodeTaints策略）或尽量不（TaintTolerationPriority策略）调度到此节点，除非是能够容忍（Tolerations）key=value污点的Pod。 taint字段1234567891011121314151617181920212223242526272829303132type Taint struct &#123; Key string Value string Effect TaintEffect &#x2F;&#x2F; add taint 的时间点 &#x2F;&#x2F; 只有 Effect &#x3D; NoExecute, 该值才会被 nodeController 写入 TimeAdded *metav1.Time&#125;type Toleration struct &#123; Key string Operator TolerationOperator Value string Effect TaintEffect &#x2F;&#x2F; 容忍时间 TolerationSeconds *int64&#125;type TaintEffect stringconst ( TaintEffectNoSchedule TaintEffect &#x3D; &quot;NoSchedule&quot; TaintEffectPreferNoSchedule TaintEffect &#x3D; &quot;PreferNoSchedule&quot; TaintEffectNoExecute TaintEffect &#x3D; &quot;NoExecute&quot;)type TolerationOperator stringconst ( TolerationOpExists TolerationOperator &#x3D; &quot;Exists&quot; TolerationOpEqual TolerationOperator &#x3D; &quot;Equal&quot;) taints注意事项 如果至少有一个 effect == NoSchedule 的 taint 没有被 pod toleration，那么 pod 不会被调度到该节点上。 如果所有 effect == NoSchedule 的 taints 都被 pod toleration，但是至少有一个 effect == PreferNoSchedule 没有被 pod toleration，那么 k8s 将努力尝试不把 pod 调度到该节点上。 如果至少有一个 effect == NoExecute 的 taint 没有被 pod toleration，那么不仅这个 pod 不会被调度到该节点，甚至这个节点上已经运行但是也没有设置容忍该污点的 pods，都将被驱逐 tolerationstoleration的格式如下: 123456tolerations:- effect: key: operator: tolerationSeconds: value 其中operator包含以下两类: Exists: 这个配置下，不需要指定 value。 Equal: 需要配置 value 值。(operator 的默认值) 有几个特殊情况: key 为空并且 operator 等于 Exists，表示匹配了所有的 keys，values 和 effects。换句话说就是容忍了所有的 taints。 12tolerations:- operator: &quot;Exists&quot; effect 为空，则表示匹配所有的 effects（NoSchedule、PreferNoSchedule、NoExecute） 123tolerations:- key: &quot;key&quot; operator: &quot;Exists&quot; TolerationSeconds与 effect 为 NoExecute 配套使用。用来指定在 node 添加了 effect = NoExecute 的 taint 后，能容忍该 taint 的 pods 可停留在 node 上的时间。 123456tolerations: - key: &quot;key1&quot; operator: &quot;Equal&quot; value: &quot;value1&quot; effect: &quot;NoExecute&quot; tolerationSeconds: 3600 tolerations test pod 创建toletation pod 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: toleration-test labels: key1: value1spec: containers: - name: nginx image: nginx ports: - containerPort: 80 tolerations: - key: &quot;key1&quot; value: &quot;value1&quot; operator: &quot;Equal&quot; effect: &quot;NoSchedule&quot; 结果： 1234➜ yamls kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEStaint-test 0&#x2F;1 Pending 0 21m &lt;none&gt; &lt;none&gt; &lt;none&gt; &lt;none&gt;toleration-test 1&#x2F;1 Running 0 111s 10.42.0.9 central &lt;none&gt; &lt;none&gt; 参考文档 https://segmentfault.com/a/1190000018446858?utm_source=tag-newest //深入理解taint https://kubernetes.io/zh/docs/concepts/configuration/taint-and-toleration //官方理解 https://www.jianshu.com/p/09356acd6991 //kubernetes taints 配置","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"linux nsenter命令","slug":"linux-nsenter","date":"2020-05-09T14:24:35.000Z","updated":"2021-12-17T07:42:48.349Z","comments":true,"path":"2020/05/09/linux-nsenter/","link":"","permalink":"https://github.com/fafucoder/2020/05/09/linux-nsenter/","excerpt":"","text":"问题概述要显示某个pod获取容器的IP地址(或者执行ping操作)， docker进入到容器里面，使用的命令包括docker exec或者docker attach。kubernetes进入到pod里面，使用的命令包括 kubectl exec。 进入到容器里面在ifconfig 或者ip a, 但是有的pod却没有这些命令，显示如下, 一般的做法就是起一个带来这些网络命令的pod(比如busybox)。 12345root@dawn:~# kubectl exec -it nginx-6db489d4b7-srfp7 /bin/sh# ifconfig/bin/sh: 1: ifconfig: not found# ping/bin/sh: 2: ping: not found 进入到容器网络的namespace等同于进入到容器中，而且还能使用宿主机的网络工具，例如ip或者ifconfig命令，所以我们只需要进入到容器网络就可以使用这些命令了。nsenter就是一个进入到namespace的命令，通过使用nsenter命令就不需要专门跑一个带有网络命令的pod了。 nsenter用途nsenter命令是一个可以在指定进程的命令空间下运行指定程序的命令。它位于util-linux包中。nsenter不仅可以进入到network命名空间，也可以进入mnt, uts, ipc, pid, user命令空间，以及指定根目录和工作目录。 使用方法nsenter的命令如下 123456789101112131415161718192021222324252627Usage: nsenter [options] [&lt;program&gt; [&lt;argument&gt;...]]Run a program with namespaces of other processes.Options: -a, --all enter all namespaces -t, --target &lt;pid&gt; target process to get namespaces from -m, --mount[=&lt;file&gt;] enter mount namespace -u, --uts[=&lt;file&gt;] enter UTS namespace (hostname etc) -i, --ipc[=&lt;file&gt;] enter System V IPC namespace -n, --net[=&lt;file&gt;] enter network namespace -p, --pid[=&lt;file&gt;] enter pid namespace -C, --cgroup[=&lt;file&gt;] enter cgroup namespace -U, --user[=&lt;file&gt;] enter user namespace -S, --setuid &lt;uid&gt; set uid in entered namespace -G, --setgid &lt;gid&gt; set gid in entered namespace --preserve-credentials do not touch uids or gids -r, --root[=&lt;dir&gt;] set the root directory -w, --wd[=&lt;dir&gt;] set the working directory -F, --no-fork do not fork before exec'ing &lt;program&gt; -Z, --follow-context set SELinux context according to --target PID -h, --help display this help -V, --version display versionFor more details see nsenter(1). 如何使用比如有个pod叫做 nginx-6db489d4b7-srfp7, 那我们要怎么进入呢？ 获取pod的ContainerID 12dawn@dawn:~$ kubectl get pod nginx-6db489d4b7-srfp7 -oyaml | grep containerID - containerID: docker://e60c6eb3f0caedb1f48cd80c2ab0450f54cc344fad2182f26e1f2426c8b597c4 获取容器的pid 123456789dawn@dawn:~$ docker inspect -f &#123;&#123;.State.Pid&#125;&#125; e60c6eb3f0c24784或者dawn@dawn:~$ docker inspect e60c6eb3f0c | grep Pid \"Pid\": 24784, \"PidMode\": \"\", \"PidsLimit\": null, 进入容器网络空间, 进入后可以使用宿主机的网络命令(ping, ifconfig等) 1234567891011121314151617dawn@dawn:~$ sudo nsenter -n -t 24784root@dawn:~# ifconfigeth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1400 inet 10.16.0.5 netmask 255.255.0.0 broadcast 10.16.255.255 ether 00:00:00:f8:aa:c8 txqueuelen 0 (Ethernet) RX packets 30 bytes 1728 (1.7 KB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 100 (100.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt; mtu 65536 inet 127.0.0.1 netmask 255.0.0.0 loop txqueuelen 1000 (Local Loopback) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 参考资料 https://staight.github.io/2019/09/23/nsenter%E5%91%BD%E4%BB%A4%E7%AE%80%E4%BB%8B/","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux namespace技术","slug":"linux-namespace","date":"2020-05-09T14:20:30.000Z","updated":"2021-12-17T07:42:48.343Z","comments":true,"path":"2020/05/09/linux-namespace/","link":"","permalink":"https://github.com/fafucoder/2020/05/09/linux-namespace/","excerpt":"","text":"概述​ Namespace是对全局系统资源的一种封装隔离，使得处于不同namespace的进程拥有独立的全局系统资源，改变一个namespace中的系统资源只会影响当前namespace里的进程，对其他namespace中的进程没有影响。 linux内核支持的namespace如下所示： 12345678名称 宏定义 隔离内容 Cgroup CLONE_NEWCGROUP Cgroup root directory (since Linux 4.6)IPC CLONE_NEWIPC System V IPC, POSIX message queues (since Linux 2.6.19)Network CLONE_NEWNET Network devices, stacks, ports, etc. (since Linux 2.6.24)Mount CLONE_NEWNS Mount points (since Linux 2.4.19)PID CLONE_NEWPID Process IDs (since Linux 2.6.24)User CLONE_NEWUSER User and group IDs (started in Linux 2.6.23 and completed in Linux 3.8)UTS CLONE_NEWUTS Hostname and NIS domain name (since Linux 2.6.19) Mount（mnt） 隔离挂载点 Process ID (pid) 隔离进程 ID Network (net) 隔离网络设备、网络栈、端口等等 Interprocess Communication (ipc) 隔离信号量、消息队列和共享内存 UTS Namespace(uts) 隔离主机名和域名 User Namespace (user) 隔离用户和用户组 注意: 由于Cgroup namespace在4.6的内核中才实现，并且和cgroup v2关系密切，现在普及程度还不高，比如docker现在就还没有用它，所以在namespace这个系列中不会介绍Cgroup namespace。 各命名空间的详细信息 参考这里 namespace系统调用接口在namespace man上提供的系统调用包括clone, setns, unshare, ioctl clone: 创建一个新的进程并把他放到新的namespace中 1int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg); setns： 将当前进程加入到已有的namespace中, (ip netns命令调用的就是setns系统调用) 1int setns(int fd, int nstype); unshare: 使当前进程退出指定类型的namespace，并加入到新创建的namespace（unshare命令） 1int unshare(int flags); clone和unshare的区别 unshare是使当前进程加入新的namespace clone是创建一个新的子进程，然后让子进程加入新的namespace，而当前进程保持不变 查看进程所属的namespace系统中的每个进程都有/proc/[pid]/ns/这样一个目录，里面包含了这个进程所属namespace的信息 123456789dev@ubuntu:~$ ls -l &#x2F;proc&#x2F;$$&#x2F;ns total 0lrwxrwxrwx 1 dev dev 0 7月 7 17:24 cgroup -&gt; cgroup:[4026531835]lrwxrwxrwx 1 dev dev 0 7月 7 17:24 ipc -&gt; ipc:[4026531839] lrwxrwxrwx 1 dev dev 0 7月 7 17:24 mnt -&gt; mnt:[4026531840] lrwxrwxrwx 1 dev dev 0 7月 7 17:24 net -&gt; net:[4026531957]lrwxrwxrwx 1 dev dev 0 7月 7 17:24 pid -&gt; pid:[4026531836]lrwxrwxrwx 1 dev dev 0 7月 7 17:24 user -&gt; user:[4026531837]lrwxrwxrwx 1 dev dev 0 7月 7 17:24 uts -&gt; uts:[4026531838] uts namespaceuts(UNIX Time-Sharing System) namespace可隔离hostname和NIS Domain name资源，使得一个宿主机可拥有多个主机名或Domain Name。换句话说，可让不同namespace中的进程看到不同的主机名。 123456789101112131415161718# 设置当前root namespace的主机名为hostname[root@master ~]# hostname hello[root@master ~]# hostnamehello# -u或--uts表示创建一个uts namespace 进入了新的namespace中的shell[root@master ~]# unshare -u /bin/sh# 其主机名初始时也是hello, 其拷贝自上级namespace资源sh-4.2# hostnamehellosh-4.2# hostname worldsh-4.2# hostnameworldsh-4.2# exitexit[root@master ~]# hostnamehello 通过pstree 查看进程树 sshd(28352)---bash(28375)---bash(4146)-+-grep(4434) 原理: Bash进程(28375)运行在当前namespace中，它将fork一个新进程来运行unshare程序，unshare程序加载完成后，将创建一个新的uts namespace，unshare进程自身将加入到这个uts namespace中，unshare进程内部再exec加载/bin/bash，于是unshare进程被替换为/bin/bash进程，/bin/bash进程也将运行在uts namespace中 ps: 当namespace中的/bin/sh进程退出，如果namespace中没有任何进程，该namespace将自动销毁。注意，在默认情况下，namespace中必须要有至少一个进程，否则将被自动被销毁。但也有一些手段可以让namespace持久化，即使已经没有任何进程在其中运行。(例如创建一个文件) mount namespacemount namespace可隔离出一个具有独立挂载点信息的运行环境，内核知道如何去维护每个namespace的挂载点列表。即每个namespace之间的挂载点列表是独立的，各自挂载互不影响。(用户通常使用mount命令来挂载普通文件系统，但实际上mount能挂载的东西非常多，甚至连现在功能完善的Linux系统，其内核的正常运行也都依赖于挂载功能，比如挂载根文件系统/。其实所有的挂载功能和挂载信息都由内核负责提供和维护，mount命令只是发起了mount()系统调用去请求内核。) 内核将每个进程的挂载点信息保存在/proc/&lt;pid&gt;/{mountinfo,mounts,mountstats}三个文件中: 1234[root@node1 ~]# ls -l /proc/$$/mount*-r--r--r--. 1 root root 0 Aug 16 08:47 /proc/5844/mountinfo-r--r--r--. 1 root root 0 Aug 16 08:47 /proc/5844/mounts-r--------. 1 root root 0 Aug 16 08:47 /proc/5844/mountstats 具有独立的挂载点信息，意味着每个mnt namespace可具有独立的目录层次，这在容器中起了很大作用：容器可以挂载只属于自己的文件系统。 当创建mount namespace时，内核将拷贝一份当前namespace的挂载点信息列表到新的mnt namespace中，此后两个mnt namespace就没有了任何关系.(也不是完全没关系，有个shared subtrees选项可设置挂载共享方式，默认是私有的，也就是没关系) 1234567891011121314151617181920212223242526272829#! /bin/bash# 创建目录mkdir -p iso/iso1/dir1 &amp;&amp; mkdir -p iso/iso2/dir2# 生成iso文件cd iso &amp;&amp; mkisofs -o 1.iso iso1 &amp;&amp; mkisofs -o 2.iso iso2# 挂载iso1mkdir /mnt/&#123;iso1,iso2&#125; &amp;&amp; mount 1.iso /mnt/iso1 &amp;&amp; mount | grep iso1# 创建mount+uts namespaceunshare -m -u /bin/bash# 在namespace中挂载iso2mount 2.iso2 /mnt/iso2/# 查看挂载信息echo \"查看挂载信息\\n\\n\"mount | grep 'iso[12]'# 卸载挂载, 此时看到没有iso1的挂载信息umount /mnt/iso1/echo \"查看挂载信息\\n\\n\"mount | grep 'iso[12]'# 新起一个terminal, 查看挂载信息没有iso2的挂载信息，但是还是有iso1的挂载信息mount | grep 'ios[12]' pid namespacepid namespace表示隔离一个具有独立PID的运行环境。在每一个pid namespace中，进程的pid都从1开始，且和其他pid namespace中的PID互不影响。这意味着，不同pid namespace中可以有相同的PID值。 因为pid namespace中的PID是独立的，每一个PID namespace都允许一些特殊的操作：允许pid namespace挂起、迁移以及恢复，就像虚拟机一样。 在介绍pid namespace之前，先创建其他类型的namespace然后查看进程关系: 12345678# 在root namespace中查看当前进程的进程IDecho $$# 创建一个uts namespaceunshare -u /bin/bash# 在uts namespace中查看当前进程pstree -p | grep grep 运行截图如下所示， unshare进程会在创建新的namespace后会被改namespace中的第一个进程给替换掉。 创建新的pid namespace方式： 1234# unshare --pid --fork [--mount-proc] &lt;CMD&gt;# --pid(-p): 表示创建pid namespace# --mount-proc: 表示创建pid namespace时重新挂载procfs# --fork(-f): 表示创建pid namespace时，不是直接替换unshare进程，而是fork unshare进程，并使用CMD替换fork出来的子进程 使用–fork的操作的结果是(如下图): unshare进程被保留，且保留在原来的pid namespace中，而不是加入到新的pid namespace中。 pid namespace 嵌套pid namespace可以存在嵌套关系。所有的子孙pid namespace中的进程信息都会保存在父级以及祖先级namespace中，只不过在不同嵌套层级中，同一个进程对应的PID不同。 123456789101112131415# ns1 namespace[root@node1 ~]# unshare -pmfu --mount-proc /bin/bash[root@node1 ~]# hostname ns1[root@node1 ~]# exec bash# ns2 namespace[root@ns1 ~]# unshare -pmfu --mount-proc /bin/bash[root@ns1 ~]# hostname ns2[root@ns1 ~]# exec bash[root@ns2 ~]# pstree -p | grep grepbash(1)-+-grep(27)# root namespace [root@node1 ~]# pstree -p 19288bash(19288)───unshare(14472)───bash(14473)───unshare(24783)───bash(24786) pid namespace和procfs/proc目录是内核对外暴露的可供用户查看或修改的内核中所记录的信息，包括内核自身的部分信息以及每个进程的信息。比如对于pid=N的进程来说，它的信息保存在/proc/目录下。在操作系统启动的过程中，会挂载procfs到/proc目录，它存在于root namespace中。 但是，创建新的pid namespace时不会自动重新挂载procfs，而是直接拷贝父级namespace的挂载点信息。这使得在新的pid namespace中仍然保留了父级namespace的/proc目录，也就是在新创建的这个pid namespace中仍然保留了父级的进程信息。 12345678910111213# 注意没--mount-proc[root@node1 ~]# unshare -pmuf /bin/bash[root@node1 ~]# hostname ns1[root@node1 ~]# exec bash# 查看进程树， 发现还是宿主机的[root@ns1 ~]# pstreesystemd─┬─NetworkManager─┬─3*[dhclient] │ └─3*[&#123;NetworkManager&#125;] ├─agetty ├─auditd───&#123;auditd&#125; ├─chronyd .... 之所以有上述问题，其原因是在pid namespace中保留了root namespace中的/proc目录，而不是属于pid namespace自己的/proc。 但用户创建pid namespace时希望的是有完全独立的进程运行环境。这时，需要在pid namespace中重新挂载procfs，或者在创建pid namespace时指定–mount-proc选项。 12345678# 注意这儿添加了mount-proc参数[root@node1 ~]# unshare -pmuf --mount-proc /bin/bash[root@node1 ~]# hostname ns1[root@node1 ~]# exec bash# 进程树中展示了bash 为init进程[root@ns1 ~]# pstree -pbash(1)───pstree(24) pid namespace 信号量问题pid=1的进程是每一个pid namespace的核心进程(init进程)，它不仅负责收养其所在pid namespace中的孤儿进程，还影响整个pid namespace。 当pid namespace中pid=1的进程退出或终止，内核默认会发送SIGKILL信号给该pid namespace中的所有进程以便杀掉它们(如果该pid namespace中有子孙namespace，也会直接被杀)。 在创建pid namespace时可以通过–kill-child选项指定pid=1的进程终止后内核要发送给pid namespace中进程的信号，其默认信号便是SIGKILL。 1unshare -p -f -m -u --mount-proc --kill-child=SIGHUP /bin/bash user namespacenetwork namespacenetwork namespace用来隔离网络环境，在network namespace中，网络设备、端口、套接字、网络协议栈、路由表、防火墙规则等都是独立的。 因为network namespace中具有独立的网络协议栈，因此每个network namespace中都有一个lo接口，但lo接口默认未启动，需要手动启动起来。 让某个network namespace和root network namespace或其他network namespace之间保持通信是一个非常常见的需求，这一般通过veth虚拟设备实现。veth类型的虚拟设备由一对虚拟的eth网卡设备组成，像管道一样，一端写入的数据总会从另一端流出，从一端读取的数据一定来自另一端。 用户可以将veth的其中一端放在某个network namespace中，另一端保留在root network namespace中。这样就可以让用户创建的network namespace和宿主机通信。 (实验部分略，之前有相关的实验了) unshare 命令 操作namespace的命令有unshare, lsns nsenter unshare命名就是unshare系统调用的实现，下面将通过unshare命令演示namespace的隔离技术 lsns 命令lsns命令列举出当前已经创建的namespace 参考文档 https://medium.com/@teddyking/linux-namespaces-850489d3ccf //namespace 概念 https://github.com/teddyking/ns-process //namespace 源代码 https://coolshell.cn/articles/17010.html //酷壳 DOCKER基础技术 https://segmentfault.com/a/1190000006908272 //Namespace概述 https://www.junmajinlong.com/virtual/namespace/ns_overview/ //各个namespace详解","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"helm 包管理","slug":"kubernetes-helm","date":"2020-04-13T04:41:47.000Z","updated":"2021-12-17T07:42:48.335Z","comments":true,"path":"2020/04/13/kubernetes-helm/","link":"","permalink":"https://github.com/fafucoder/2020/04/13/kubernetes-helm/","excerpt":"","text":"Helm 基础概念Helm Chart: Chart 代表Helm包，它包含在 Kubernetes 集群内部运行应用程序，工具或服务所需的所有资源定义。可以把Helm Chart比作Apt dpkg或者yum rpm在linux中的等价位。 Helm Hub: 类似于docker hub 用于存放公共chart的地方 Helm Repository: 是用来存放和共享 charts 的地方(放置在本地) Helm Release: Release 是运行在 Kubernetes 集群中的 chart 的实例。一个 chart 通常可以在同一个集群中安装多次。每一次安装都会创建一个新的 release。以 MySQL chart为例，如果你想在你的集群中运行两个数据库，你可以安装该chart两次。每一个数据库都会拥有它自己的 release 和 release name。 Helm 命令(详细参数查阅官方文档)helm search hub: 从 Artifact Hub 中查找并列出 helm charts。 Artifact Hub中存放了大量不同的仓库。 helm search repo: 从你添加（使用 helm repo add）到本地 helm 客户端中的仓库中进行查找。该命令基于本地数据进行搜索，无需连接互联网。 helm repo list: 列出所有本地已经添加的repo helm repo add: 从repo hub中添加一个chart到本地repo helm repo remove: 从本地repo中移除chart helm repo update: 更新本地repo charts helm list: 列出所有的release chart helm uninstall: 删除一个release helm status: 查看release状态(可以通过helm status 查看helm的输出信息，比如默认用户名密码可能通过helm status 来查看) helm install: 安装一个chart helm upgrade: 升级chart helm show values: 列出chart 的自定义变量value helm get values: 查看release 的values(也就是说可以查看哪些自定义的值作了修改) helm history: 查看 release 的历史版本 helm pull(fetch): 从包仓库(hub or repo)中检索包，并在本地下载它。 helm template: 本地渲染模板(也就是说把默认的信息转化成yaml) 参考文档 https://blog.csdn.net/qq_27234433/article/details/114338623 //helm文档 https://helm.sh/zh/docs/helm/helm_repo/ //helm 官方文档","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"iperf","slug":"linux-iperf","date":"2020-04-12T06:23:08.000Z","updated":"2021-12-17T07:42:48.342Z","comments":true,"path":"2020/04/12/linux-iperf/","link":"","permalink":"https://github.com/fafucoder/2020/04/12/linux-iperf/","excerpt":"","text":"参考文档 https://aijishu.com/a/1060000000081891 https://github.com/Pharb/kubernetes-iperf3/blob/master/README.md https://blog.51cto.com/tryingstuff/1954531 https://hazx.hmacg.cn/server/windows-iso-add-driver.html","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"linux 用户态和内核态","slug":"linux-workspace","date":"2020-04-08T03:13:04.000Z","updated":"2021-12-17T07:42:48.352Z","comments":true,"path":"2020/04/08/linux-workspace/","link":"","permalink":"https://github.com/fafucoder/2020/04/08/linux-workspace/","excerpt":"","text":"Unix/Linux的体系架构 Linux操作系统的体系架构分为用户态和内核态（或者用户空间和内核）。内核从本质上看是一种软件——控制计算机的硬件资源，并提供上层应用程序运行的环境。用户态即上层应用程序的活动空间，应用程序的执行必须依托于内核提供的资源，包括CPU资源、存储资源、I/O资源等。为了使上层应用能够访问到这些资源，内核必须为上层应用提供访问的接口：即系统调用 系统调用是操作系统的最小功能单位，这些系统调用根据不同的应用场景可以进行扩展和裁剪，现在各种版本的Unix实现都提供了不同数量的系统调用，如Linux的不同版本提供了240-260个系统调用，FreeBSD大约提供了320个（reference：UNIX环境高级编程） Shell是一个特殊的应用程序，俗称命令行，本质上是一个命令解释器，它下通系统调用，上通各种应用，通常充当着一种“胶水”的角色，来连接各个小功能程序，让不同程序能够以一个清晰的接口协同工作，从而增强各个程序的功能。同时，Shell是可编程的，它可以执行符合Shell语法的文本，这样的文本称为Shell脚本，通常短短的几行Shell脚本就可以实现一个非常大的功能，原因就是这些Shell语句通常都对系统调用做了一层封装。为了方便用户和系统交互，一般，一个Shell对应一个终端，终端是一个硬件设备，呈现给用户的是一个图形化窗口。我们可以通过这个窗口输入或者输出文本。这个文本直接传递给shell进行分析解释，然后执行。 运行级别 linux的运行级别如上所示， Linux使用了Ring3级别运行用户态，Ring0作为 内核态，没有使用Ring1和Ring2。Ring3状态不能访问Ring0的地址空间，包括代码和数据。 用户态通过系统调用切换成内核态，cpu通过跳转到系统调用对应的内核代码位置执行， 完成后再由内核态切回用户态 系统内核 向下控制硬件资源，向内管理操作系统资源：包括进程的调度和管理、内存的管理、文件系统的管理、设备驱动程序的管理以及网络资源的管理，向上则向应用程序提供系统调用的接口。 运行态切换用户态的应用程序可以通过三种方式来访问内核态的资源 系统调用 异常事件 外围设备的中断 系统调用因为操作系统的资源是有限的，如果访问资源的操作过多，必然会消耗过多的资源，而且如果不对这些操作加以区分，很可能造成资源访问的冲突。所以，为了减少有限资源的访问和使用冲突，Unix/Linux的设计哲学之一就是：对不同的操作赋予不同的执行等级，就是所谓特权的概念。简单说就是有多大能力做多大的事，与系统相关的一些特别关键的操作必须由最高特权的程序来完成。Intel的X86架构的CPU提供了0到3四个特权级，数字越小，特权越高，Linux操作系统中主要采用了0和3两个特权级，分别对应的就是内核态和用户态。运行于用户态的进程可以执行的操作和访问的资源都会受到极大的限制，而运行在内核态的进程则可以执行任何操作并且在资源的使用上没有限制。很多程序开始时运行于用户态，但在执行的过程中，一些操作需要在内核权限下才能执行，这就涉及到一个从用户态切换到内核态的过程。比如C函数库中的内存分配函数malloc()，它具体是使用sbrk()系统调用来分配内存，当malloc调用sbrk()的时候就涉及一次从用户态到内核态的切换，类似的函数还有printf()，调用的是wirte()系统调用来输出字符串，等等。 异常事件当CPU正在执行运行在用户态的程序时，突然发生某些预先不可知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件，典型的如缺页异常 外围设备中断当外围设备完成用户的请求操作后，会像CPU发出中断信号，此时，CPU就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换。 系统调用的本质其实也是中断，相对于外围设备的硬中断，这种中断称为软中断，这是操作系统为用户特别开放的一种中断，如Linux int 80h中断。所以，从触发方式和效果上来看，这三种切换方式是完全一样的，都相当于是执行了一个中断响应的过程。但是从触发的对象来看，系统调用是进程主动请求切换的，而异常和硬中断则是被动的。 常用系统调用方式procfs(/proc)procfs 是 进程文件系统 的缩写，它本质上是一个伪文件系统，为什么说是 伪 文件系统呢？因为它不占用外部存储空间，只是占用少量的内存，通常是挂载在 /proc 目录下。 我们在该目录下看到的一个文件，实际上是一个内核变量。内核就是通过这个目录，以文件的形式展现自己的内部信息，相当于 /proc 目录为用户态和内核态之间的交互搭建了一个桥梁，用户态读写 /proc 下的文件，就是读写内核相关的配置参数。 比如常见的 /proc/cpuinfo、/proc/meminfo 和 /proc/net 就分别提供了 CPU、内存、网络的相关参数。除此之外，还有很多的参数 1234567891011root@ubuntu:~# ls &#x2F;proc&#x2F;1 1143 1345 1447 2 2292 29 331 393 44 63 70 76 acpi diskstats irq locks sched_debug sysvipc zoneinfo10 1145 1357 148 20 23 290 332 396 442 64 7019 77 asound dma kallsyms mdstat schedstat thread-self1042 1149 1361 149 2084 2425 291 34 398 45 65 7029 8 buddyinfo driver kcore meminfo scsi timer_list1044 1150 1363 15 2087 25 3 3455 413 46 66 7079 83 bus execdomains keys misc self timer_stats1046 1151 1371 16 2090 256 30 35 418 47 6600 7080 884 cgroups fb key-users modules slabinfo tty1048 1153 1372 17 21 26 302 36 419 5 67 71 9 cmdline filesystems kmsg mounts softirqs uptime11 1190 1390 18 22 27 31 37 420 518 6749 72 96 consoles fs kpagecgroup mtrr stat version1126 12 143 182 2214 28 32 373 421 524 68 73 97 cpuinfo interrupts kpagecount net swaps version_signature1137 1252 1434 184 2215 280 327 38 422 525 69 74 98 crypto iomem kpageflags pagetypeinfo sys vmallocinfo1141 13 144 190 2262 281 33 39 425 5940 7 75 985 devices ioports loadavg partitions sysrq-trigger vmstat 可以看到，这里面有很多的数字表示的文件，这些其实是当前系统运行的进程文件，数字表示进程号（PID），每个文件包含该进程所有的配置信息，包括进程状态、文件描述符、内存映射等等，我们可以看下： 123456root@ubuntu:~# ls &#x2F;proc&#x2F;1&#x2F;attr&#x2F; cmdline environ io mem ns&#x2F; pagemap schedstat stat timersautogroup comm exe limits mountinfo numa_maps personality sessionid statm uid_mapauxv coredump_filter fd&#x2F; loginuid mounts oom_adj projid_map setgroups status wchancgroup cpuset fdinfo&#x2F; map_files&#x2F; mountstats oom_score root&#x2F; smaps syscall clear_refs cwd&#x2F; gid_map maps net&#x2F; oom_score_adj sched stack task&#x2F; 综上，内核通过一个个的文件来暴露自己的系统配置信息，这些文件，有些是只读的，有些是可写的，有些是动态变化的，比如进程文件，当应用程序读取某个 /proc/ 文件时，内核才会去注册这个文件，然后再调用一组内核函数来处理，将相应的内核参数拷贝到用户态空间，这样用户读这个文件就可以获取到内核的信息。一个大概的图示如下所示：procfs sysctl(/proc/sys)我们熟悉的 sysctl 是一个 Linux 命令，man sysctl 可以看到它的功能和用法。它主要是被用来修改内核的运行时参数，换句话说，它可以在内核运行过程中，动态修改内核参数。 它本质上还是用到了文件的读写操作，来完成用户态和内核态的通信。它使用的是 /proc 的一个子目录 /proc/sys。和 procfs 的区别在于： procfs 主要是输出只读数据，而 sysctl 输出的大部分信息是可写的。 例如，我们比较常见的是通过 cat /proc/sys/net/ipv4/ip_forward 来获取内核网络层是否允许转发 IP 数据包，通过 echo 1 &gt; /proc/sys/net/ipv4/ip_forward 或者 sysctl -w net.ipv4.ip_forward=1 来设置内核网络层允许转发 IP 数据包。 同样的操作，Linux 也提供了文件 /etc/sysctl.conf 来让你进行批量修改。 sysfs(/sys)sysfs 是 Linux 2.6 才引入的一种虚拟文件系统，它的做法也是通过文件 /sys 来完成用户态和内核的通信。和 procfs 不同的是，sysfs 是将一些原本在 procfs 中的，关于设备和驱动的部分，独立出来，以 “设备树” 的形式呈现给用户。 sysfs 不仅可以从内核空间读取设备和驱动程序的信息，也可以对设备和驱动进行配置。 我们看下 /sys 下有什么： 12# ls &#x2F;sysblock bus class dev devices firmware fs hypervisor kernel module power netlink 套接口netlink 是 Linux 用户态与内核态通信最常用的一种方式。Linux kernel 2.6.14 版本才开始支持。它本质上是一种 socket，常规 socket 使用的标准 API，在它身上同样适用。 netlink 这种灵活的方式，使得它可以用于内核与多种用户进程之间的消息传递系统，比如路由子系统，防火墙（Netfilter），ipsec 安全策略等等。 用户空间和内核空间用户空间就是用户进程所在的内存区域，系统空间就是操作系统占据的内存区域。用户进程和系统进程的所有数据都在内存中。 在电脑开机之前，内存就是一块原始的物理内存。什么也没有。开机加电，系统启动后，就对物理内存进行了划分。当然，这是系统的规定，物理内存条上并没有划分好的地址和空间范围。这些划分都是操作系统在逻辑上的划分。不同版本的操作系统划分的结果都是不一样的。例如在32位操作系统中，一般将最高的1G字节划分为内核空间，供内核使用，而将较低的3G字节划分为用户空间，供各个进程使用。 其中: 内核空间中存放的是内核代码和数据，而进程的用户空间中存放的是用户程序的代码和数据。 进程在运行的时候，在内核空间和用户空间各有一个堆栈。 用户空间中，每个进程的用户空间是互相独立的，互不相干。 内核空间中，绝大部分是共享的，并不是完全共享，因为内核空间中，不同进程的内核栈之间是不共享的。 参考文档 https://cllc.fun/2019/03/02/linux-user-kernel-space //用户空间与内核空间简述 Linux探秘之用户态与内核态 //Linux云计算网络 转 cpu 寄存器 内核态 用户态","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"xvlan","slug":"linux-xvlan","date":"2020-04-05T08:00:42.000Z","updated":"2021-12-17T07:42:48.352Z","comments":true,"path":"2020/04/05/linux-xvlan/","link":"","permalink":"https://github.com/fafucoder/2020/04/05/linux-xvlan/","excerpt":"","text":"参考文档 https://cloud.tencent.com/developer/article/1459255 //xlan原理","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"Linux shell编程","slug":"linux-shell","date":"2020-04-03T01:29:31.000Z","updated":"2022-04-24T12:17:19.552Z","comments":true,"path":"2020/04/03/linux-shell/","link":"","permalink":"https://github.com/fafucoder/2020/04/03/linux-shell/","excerpt":"","text":"常见特殊符号 $# 是传给脚本的参数个数 $@ 是传给脚本的所有参数的列表 $* 是以一个单字符串显示所有向脚本传递的参数，与位置变量不同，参数可超过9个 $$ 是脚本运行的当前进程ID号 $? 是显示最后命令的退出状态，0表示没有错误，其他表示有错误 $0 是脚本本身的名字 $1 是传递给该shell脚本的第一个参数 $2 是传递给该shell脚本的第二个参数 $() 和 ` ` 是命令替换 ${} 是参数替换 $_ 在此之前执行的命令或者脚本的最后一个参数 常见括号区别 [ ]: 用于条件判断，判断对象包括文件类型和赋值比较 [ 4 -eq 3 ] [[ ]]: 跟[ ]基本一致，也是用于条件判断， 但是也有点区别，[[]] 支持正则表达式比较，且逻辑运算符不一致，”[[]]”为”&amp;&amp;”、”||”，”[]”为”-a”、”-o”。”[[]]”支持逻辑短路，而”[]”不支持， “[[]]”为一个keyword，同括号与表达式中间必须要有空格进行隔离，”[[]]”中使用比较符时不能转义，同时不会出现Word-Splitting 常见的判断类型文件判断 [ -a file ]：如果 file 存在，则为true。 [ -b file ]：如果 file 存在并且是一个块（设备）文件，则为true。 [ -c file ]：如果 file 存在并且是一个字符（设备）文件，则为true。 [ -d file ]：如果 file 存在并且是一个目录，则为true。 [ -e file ]：如果 file 存在，则为true。 [ -f file ]：如果 file 存在并且是一个普通文件，则为true。 [ -g file ]：如果 file 存在并且设置了组 ID，则为true。 [ -G file ]：如果 file 存在并且属于有效的组 ID，则为true。 [ -h file ]：如果 file 存在并且是符号链接，则为true。 [ -k file ]：如果 file 存在并且设置了它的“sticky bit”，则为true。 [ -L file ]：如果 file 存在并且是一个符号链接，则为true。 [ -N file ]：如果 file 存在并且自上次读取后已被修改，则为true。 [ -O file ]：如果 file 存在并且属于有效的用户 ID，则为true。 [ -p file ]：如果 file 存在并且是一个命名管道，则为true。 [ -r file ]：如果 file 存在并且可读（当前用户有可读权限），则为true。 [ -s file ]：如果 file 存在且其长度大于零，则为true。 [ -S file ]：如果 file 存在且是一个网络 socket，则为true。 [ -t fd ]：如果 fd 是一个文件描述符，并且重定向到终端，则为true。 这可以用来判断是否重定向了标准输入／输出／错误。 [ -u file ]：如果 file 存在并且设置了 setuid 位，则为true。 [ -w file ]：如果 file 存在并且可写（当前用户拥有可写权限），则为true。 [ -x file ]：如果 file 存在并且可执行（有效用户有执行／搜索权限），则为true。 [ file1 -nt file2 ]：如果 FILE1 比 FILE2 的更新时间最近，或者 FILE1 存在而 FILE2 不存在，则为true。 [ file1 -ot file2 ]：如果 FILE1 比 FILE2 的更新时间更旧，或者 FILE2 存在而 FILE1 不存在，则为true。 [ FILE1 -ef FILE2 ]：如果 FILE1 和 FILE2 引用相同的设备和 inode 编号，则为true。 字符串判断 [ string ]：如果string不为空（长度大于0），则判断为真。 [ -n string ]：如果字符串string的长度大于零，则判断为真。 [ -z string ]：如果字符串string的长度为零，则判断为真。 [ string1 = string2 ]：如果string1和string2相同，则判断为真。 [ string1 == string2 ] 等同于[ string1 = string2 ]。 [ string1 != string2 ]：如果string1和string2不相同，则判断为真。 [ string1 &#39;&gt;&#39; string2 ]：如果按照字典顺序string1排列在string2之后，则判断为真。 [ string1 &#39;&lt;&#39; string2 ]：如果按照字典顺序string1排列在string2之前，则判断为真。 整数判断 [ integer1 -eq integer2 ]：如果integer1等于integer2，则为true。 [ integer1 -ne integer2 ]：如果integer1不等于integer2，则为true。 [ integer1 -le integer2 ]：如果integer1小于或等于integer2，则为true。 [ integer1 -lt integer2 ]：如果integer1小于integer2，则为true。 [ integer1 -ge integer2 ]：如果integer1大于或等于integer2，则为true。 [ integer1 -gt integer2 ]：如果integer1大于integer2，则为true。 命令执行返回值在 Linux 下，不管你是启动一个桌面程序也好，还是在控制台下运行命令，所有的程序在结束时，都会返回一个数字值，这个值叫做返回值，或者称为错误号 ( Error Number )。 在控制台下，有一个特殊的环境变量 $?，保存着前一个程序的返回值 只要返回值是 0，就代表程序执行成功了～ &amp; &amp;&amp; | || 区别cmd1 操作符 cmd2 操作符 cmd3 把这一整体称为一个命令 &amp;：除了最后一个cmd，前面的cmd均已后台方式静默执行，执行结果显示在终端上，个别的cmd错误不影响整个命令的执行，全部的cmd同时执行 &amp;&amp;：从左到右顺序执行cmd，个别cmd错误不产生影响 |：各个cmd同时在前台被执行，但是除最后的cmd之外，其余的执行结果不会被显示在终端上 ||：从左到右顺序执行cmd，只有左侧的cmd执行出错，右边的cmd才会被执行，同时一旦有cmd被成功执行，整个命令就会结束，返回终端 参考网址 https://blog.csdn.net/w746805370/article/details/51044352 //linux shell编程中特殊字符 https://www.cnblogs.com/unknown404/p/10355705.html //Linux shell中&amp;，&amp;&amp;，|，||的用法 https://www.cnblogs.com/x_wukong/p/5148237.html //linux命令执行返回值（附错误对照表）","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"git-rebase使用","slug":"git-rebase","date":"2020-03-28T15:35:18.000Z","updated":"2021-12-17T07:42:48.324Z","comments":true,"path":"2020/03/28/git-rebase/","link":"","permalink":"https://github.com/fafucoder/2020/03/28/git-rebase/","excerpt":"","text":"git rebase注意事项 注意不要合并先前提交的东西，也就是已经提交远程分支的纪录。 git rebase的常用命令123456789101112131415161718# Rebase a639348..8c898e1 onto a639348 (2 commands)## Commands:# p, pick &#x3D; use commit# r, reword &#x3D; use commit, but edit the commit message# e, edit &#x3D; use commit, but stop for amending &#x2F;&#x2F;修改commit信息# s, squash &#x3D; use commit, but meld into previous commit &#x2F;&#x2F;丢弃commit信息, 合并到上个commit中# f, fixup &#x3D; like &quot;squash&quot;, but discard this commit&#39;s log message &#x2F;&#x2F;丢弃commit信息，日志不记录# x, exec &#x3D; run command (the rest of the line) using shell# d, drop &#x3D; remove commit## These lines can be re-ordered; they are executed from top to bottom.## If you remove a line here THAT COMMIT WILL BE LOST.## However, if you remove everything, the rebase will be aborted.## Note that empty commits are commented out 调整commit的位置12341. git rebase -i HEAD~42. 任意调整位置(vim ddp 调整位置)3. 如果有冲突,解决冲突4. git rebase --continue 合并commit(合并十个commit为一个)12341. git rebase -i HEAD~92. 把需要合并的标记为s&#x2F;squash(最顶上不能标记为s)3. 保存， 重新填写commit信息4. 修改commit信息(修改前两个的commit信息)12341. git rebase -i HEAD~22. 标记为e&#x2F;edit3. git commit --amend4. git rebase --continue 参考文档 http://jartto.wang/2018/12/11/git-rebase/ //git rebase使用实例","categories":[{"name":"git","slug":"git","permalink":"https://github.com/fafucoder/categories/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://github.com/fafucoder/tags/git/"}]},{"title":"vim常用命令","slug":"linux-vim","date":"2020-03-28T15:18:59.000Z","updated":"2021-12-17T07:42:48.352Z","comments":true,"path":"2020/03/28/linux-vim/","link":"","permalink":"https://github.com/fafucoder/2020/03/28/linux-vim/","excerpt":"","text":"上下行切换(光标在上行，其与下行交换) 1ddp 左右字符交换(光标在左，与右字符交换) 1xp 删除当前行到末尾 1dG 删除当前行 1dd 常见字符作用 123456789101112131415剪切、复制、删除Operator + Scope &#x3D; commandOperatord 剪切y 复制。p 粘帖，与 d 和 y 配和使用。可将最后d或y的资料放置於游标所在位置之行列下。c 修改，类似delete与insert的组和。删除一个字组、句子等之资料，并插入新建资料。Scopee 由游标所在位置至该字串的最后一个字元。w 由游标所在位置至下一个字串的第一个字元。b 由游标所在位置至前一个字串的第一个字元。$ 由游标所在位置至该行的最后一个字元。0 由游标所在位置至该行的第一个字元。 参考文档 https://cllc.fun/2019/03/04/vim/","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"iptables 学习记录","slug":"linux-iptables","date":"2020-03-24T06:46:05.000Z","updated":"2021-12-17T07:42:48.342Z","comments":true,"path":"2020/03/24/linux-iptables/","link":"","permalink":"https://github.com/fafucoder/2020/03/24/linux-iptables/","excerpt":"","text":"一、iptables相关概念iptables的底层实现是netfilter，整个流程图如下图所示。 当网卡上收到一个包送达协议栈时，最先经过的netfilter钩子是PREROUTING，如果确实有用户埋了这个钩子函数，那么内核将在这里对数据包进行目的地址转换（DNAT）。不管在PREROUTING有没有做过DNAT，内核都会通过查本地路由表决定这个数据包是发送给本地进程还是发送给其他机器。如果是发送给其他机器（或其他network namespace），就相当于把本地当作路由器，就会经过netfilter的FORWARD钩子，用户可以在此处设置包过滤钩子函数，例如iptables的reject函数。所有马上要发到协议栈外的包都会经过POSTROUTING钩子，用户可以在这里埋下源地址转换（SNAT）或源地址伪装（Masquerade，简称Masq）的钩子函数。如果经过上面的路由决策，内核决定把包发给本地进程，就会经过INPUT钩子。本地进程收到数据包后，回程报文会先经过OUTPUT钩子，然后经过一次路由决策（例如，决定从机器的哪块网卡出去，下一跳地址是多少等），最后出协议栈的网络包同样会经过POSTROUTING钩子。 二、iptables三板斧 table, rule, chainiptables的工作流程如下图所示， 五张链 INPUT: 处理输入本地进程的数据包 OUTPUT: 处理输入本地进程的输出数据包 FORWORD: 处理转发到其他机器/network namespace的数据包 PREROUTING: 目的地址转换DNAT POSTROUTING: 原地址转换SNAT 五张表 raw: 去除数据包连接追踪机制 fileter: 控制到达某条链上的数据包是继续放行、直接丢弃（drop）或拒绝（reject） nat: 修改数据包的源和目的地址 mangle: 修改数据包的IP头信息 security: …. 规则动作 ACCEPT: 将数据包放行，进行完此处理动作后，将不再比对其它规则，直接跳往下一个规则链 REJECT: 拦阻该数据包，并传送数据包通知对方 DROP: 丢弃包不予处理，进行完此处理动作后，将不再比对其它规则，直接中断过滤程序 REDIRECT: 将包重新导向到另一个端口（PNAT），进行完此处理动作后，将会继续比对其它规则。(iptables -t nat -A PREROUTING -p tcp –dport 80 -j REDIRECT –to-ports 8080) SNAT: 改写封包来源 IP 为某特定 IP 或 IP 范围，可以指定 port 对应的范围，进行完此处理动作后，将直接跳往下一个规则链。通过--to-source指定SNAT转换后的地址(iptables -t nat -A POSTROUTING -p tcp-o eth0 -j SNAT –to-source 194.236.50.155-194.236.50.160:1024-32000) DNAT: 改写封包目的地 IP 为某特定 IP 或 IP 范围，可以指定 port 对应的范围，进行完此处理动作后，将会直接跳往下一个规则链。通过--to-destination指定DNAT转换后的地址(iptables -t nat -A PREROUTING -p tcp -d 15.45.23.67 –dport 80 -j DNAT –to-destination 192.168.1.1-192.168.1.10:80-100) MASQUERADE： 改写封包来源 IP 为防火墙 NIC IP，可以指定 port 对应的范围，进行完此处理动作后，直接跳往下一个规则链。这个功能与 SNAT 略有不同，当进行IP伪装时，不需指定要伪装成哪个IP，IP 会从网卡直接读取，当使用拨接连线时，IP 通常是由ISP公司的 DHCP 服务器指派的，这个时候 MASQUERADE 特别有用 MARK: 将封包标上某个代号，以便提供作为后续过滤的条件判断依据，进行完此处理动作后，将会继续比对其它规则 RETURN: 结束在目前规则链中的过滤程序，返回主规则链继续过滤 QUEUE: 封包放入队列，交给其它程序处理 MIRROR: 镜射封包，也就是将来源IP与目的地IP对调后，将封包送回，进行完此处理动作后，将会中断过滤程序 LOG: 将封包相关讯息记录在/var/log中 注意点 表的优先级从高到低是：raw、mangle、nat、filter、security。 iptables支持用户自定义链， 不支持用户自定义表。 不是每个链上都能挂表，关系如下图 iptables的每条链下面的规则处理顺序是从上到下逐条遍历的，除非中途碰到DROP，REJECT，RETURN这些内置动作。如果iptables规则前面是自定义链，则意味着这条规则的动作是JUMP，即跳到这条自定义链遍历其下的所有规则，然后跳回来遍历原来那条链后面的规则 查询iptables时，默认是filter表 drop 跟reject区别 reject 动作会返回一个拒绝(终止)数据包(TCP FIN或UDP-ICMP-PORT-UNREACHABLE)，明确的拒绝对方的连接动作。 连接马上断开，Client会认为访问的主机不存在。 DROP动作只是简单的直接丢弃数据，并不反馈任何回应。需要Client等待超时。 数据经过的流程图 三、iptables常见命令命令格式iptables command chain chain_name options iptables 参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849Usage: iptables -[ACD] chain rule-specification [options] iptables -I chain [rulenum] rule-specification [options] iptables -R chain rulenum rule-specification [options] iptables -D chain rulenum [options] iptables -[LS] [chain [rulenum]] [options] iptables -[FZ] [chain] [options] iptables -[NX] chain iptables -E old-chain-name new-chain-name iptables -P chain target [options] iptables -h (print this help information)Commands: --append -A chain Append to chain --check -C chain Check for the existence of a rule --delete -D chain Delete matching rule from chain --delete -D chain rulenum Delete rule rulenum (1 &#x3D; first) from chain --insert -I chain [rulenum] Insert in chain as rulenum (default 1&#x3D;first) --replace -R chain rulenum Replace rule rulenum (1 &#x3D; first) in chain --list -L [chain [rulenum]] List the rules in a chain or all chains --list-rules -S [chain [rulenum]] Print the rules in a chain or all chains --flush -F [chain] Delete all rules in chain or all chains --zero -Z [chain [rulenum]] List Zero counters in chain or all chains --new -N chain Create a new user-defined chain --delete-chain -X [chain] Delete a user-defined chain --policy -P chain target Change policy on chain to target --rename-chain -E old-chain new-chain Change chain name, (moving any references)Options: --ipv4 -4 Nothing (line is ignored by ip6tables-restore) --ipv6 -6 Error (line is ignored by iptables-restore)[!] --protocol -p proto protocol: by number or name, eg. &#96;tcp&#39;[!] --source -s address[&#x2F;mask][...] source specification[!] --destination -d address[&#x2F;mask][...] destination specification[!] --in-interface -i input name[+] network interface name ([+] for wildcard) --jump -j target target for rule (may load target extension) --goto -g chain jump to chain with no return --match -m match extended match (may load extension) --numeric -n numeric output of addresses and ports[!] --out-interface -o output name[+] network interface name ([+] for wildcard) --table -t table table to manipulate (default: &#96;filter&#39;) --verbose -v verbose mode --wait -w [seconds] maximum wait to acquire xtables lock before give up --wait-interval -W [usecs] wait time to try to acquire xtables lock, default is 1 second --line-numbers print line numbers when listing --exact -x expand numbers (display exact values)[!] --fragment -f match second or further fragments only --modprobe&#x3D;&lt;command&gt; try to insert modules using this command --set-counters PKTS BYTES set the counter during insert&#x2F;append[!] --version -V print package version. 查询iptables –line-numbers -t 表名 -n(不解析ip) -v(详细信息) -L 链名 删除iptables 先查看路由规则，iptables –line-numbers -nvL INPUT 清除路由规则, iptables -t tablename -D 链名 line-number (例如3) iptables-saveiptables-save命令可以将内核中的当前iptables配置导出到标准输出中，可以通过IO重定向功能输出到文件，例如iptables-save &gt; hello.txt 通过使用iptables-save能更直观的看懂iptables路由规则 参数-t(–table): 指定iptables中的表-c(–counters): 当前的数据包计数器和字节计数器信息 四、Netfilter路由选择ip rule路由策略ip rule命令用于设置路由策略，使用它不仅能够根据目的地址而且能够根据报文大小，应用或IP源地址等属性来选择转发路径。 ip rule 命令12345678910111213141516Usage: ip rule [ list | add | del ] SELECTOR ACTION SELECTOR :&#x3D; [ from PREFIX 数据包源地址] [ to PREFIX 数据包目的地址] [ tos TOS 服务类型] [ dev STRING 物理接口] [ pref NUMBER ] [fwmark MARK iptables 标签] ACTION :&#x3D; [ table TABLE_ID 指定所使用的路由表] [ nat ADDRESS 网络地址转换] [ prohibit 丢弃该表| reject 拒绝该包| unreachable 丢弃该包] TABLE_ID :&#x3D; [ local | main | default | new | NUMBER ]# 例子通过路由表 inr.ruhep 路由来自源地址为192.203.80&#x2F;24的数据包ip rule add from 192.203.80&#x2F;24 table inr.ruhep prio 220 把源地址为193.233.7.83的数据报的源地址转换为192.203.80.144，并通过表1进行路由ip rule add from 193.233.7.83 nat 192.203.80.144 table 1 prio 320 ip rule 默认规则在 Linux 系统启动时，内核会为路由策略数据库配置三条缺省的规则： 0：匹配任何条件，查询路由表local(ID 255)，该表local是一个特殊的路由表，包含对于本地和广播地址的优先级控制路由。rule 0非常特殊，不能被删除或者覆盖。 32766：匹配任何条件，查询路由表main(ID 254)，该表是一个通常的表，包含所有的无策略路由。系统管理员可以删除或者使用另外的规则覆盖这条规则。 32767：匹配任何条件，查询路由表default(ID 253)，该表是一个空表，它是后续处理保留。对于前面的策略没有匹配到的数据包，系统使用这个策略进行处理，这个规则也可以删除。 ip route路由表所谓路由表，指的是路由器或者其他互联网网络设备上存储的表，该表中存有到达特定网络终端的路径，在某些情况下，还有一些与这些路径相关的度量。路由器的主要工作就是为经过路由器的每个数据包寻找一条最佳的传输路径，并将该数据有效地传送到目的站点。 123456789ip route list table table_number ip route list table table_name#例1：在一号表中添加默认路由为192.168.1.1ip route add default via 192.168.1.1 table 1 #例2：在一号表中添加一条到192.168.0.0网段的路由为192.168.1.2ip route add 192.168.0.0&#x2F;24 via 192.168.1.2 table 1 ip route 默认路由表linux 系统中，可以自定义从 1－252个路由表，其中，linux系统维护了4个路由表： 0#表： 系统保留表 253#表： defulte table 没特别指定的默认路由都放在改表 254#表： main table 没指明路由表的所有路由放在该表 255#表： locale table 保存本地接口地址，广播地址、NAT地址 由系统维护，用户不得更改 ip route 与 ip rule 关系在Netfilter中，通过路由选择决定把包发送给本地进程还是经过Forward链转发到其他接口上。 上图中routing就是使用ip rule，ip route设置的规则，其中ip route配置的路由表服务于ip rule配置的规则。 以一例子来说明：公司内网要求192.168.0.100 以内的使用 10.0.0.1 网关上网 （电信），其他IP使用 20.0.0.1 （网通）上网。 首先要在网关服务器上添加一个默认路由，当然这个指向是绝大多数的IP的出口网关：ip route add default gw 20.0.0.1 之后通过 ip route 添加一个路由表：ip route add table 3 via 10.0.0.1 dev ethX (ethx 是 10.0.0.1 所在的网卡, 3 是路由表的编号) 之后添加 ip rule 规则：ip rule add fwmark 3 table 3 （fwmark 3 是标记，table 3 是路由表3 上边。 意思就是凡事标记了 3 的数据使用 table3 路由表） 之后使用 iptables 给相应的数据打上标记：iptables -A PREROUTING -t mangle -i eth0 -s 192.168.0.1 - 192.168.0.100 -j MARK –set-mark 3 参考文档 http://www.zsythink.net/archives/1199 // iptables概念 http://www.zsythink.net/archives/1493 // iptables查询 https://blog.csdn.net/guochunyang/article/details/49867707 // iptables规则 https://blog.csdn.net/u011537073/article/details/82685586 // iptables基础知识 https://www.ichenfu.com/2018/09/09/packet-flow-in-netfilter/ // netfilter 数据流图 https://blog.csdn.net/qq_26602023/article/details/106413909 // linux网络知识：路由策略（ip rule，ip route）","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"journalctl日志查看命令","slug":"linux-journalctl","date":"2020-03-24T04:50:36.000Z","updated":"2021-12-17T07:42:48.343Z","comments":true,"path":"2020/03/24/linux-journalctl/","link":"","permalink":"https://github.com/fafucoder/2020/03/24/linux-journalctl/","excerpt":"","text":"Unit的概念systemd开启和监督整个系统是基于unit的概念。unit是由一个与配置文件名同名的名字和类型组成的(例如：mysql.service unit有一个具有相同名字的配置文件，它是守护进程avahi的一个封装单元) unit有以下几种类型： service：代表一个后台服务进程，比如 mysqld 这是最常用的一类。 socket：此类配置单元封装系统和互联网中的一个套接字。当下，systemd支持流式，数据报和连续包的 AF_INET，AF_INET6，AF_UNIX socket。 device：此类配置单元封装一个存在于Linux设备树中的设备。每个使用udev规则标记的设备都将会在systemd中作为一个设备配置单元出现。 mount：此类配置单元封装文件系统结构层次中的一个挂载点。systemd将对这个挂载点进行监控和管理。比如，可以在启动时自动将其挂载，可以在某些条件下自动卸载。systemd会将/etc/fstab中的条目都转换为挂载点，并在开机时处理。 automount：此类配置单元封装系统结构层次中的一个自挂载点。每个自挂载配置单元对应一个挂载配置单元，当该自动挂载点被访问时，systemd执行挂载点中定义的挂载行为。 swap：和挂载配置单元类似，交换配置单元用来管理交换分区。用户可以用交换配置单元来定义系统中的交换分区，可以让这些交换分区在启动时被激活。 target：此类配置单元为其他配置单元进行逻辑分组。它们本身实际上并不做什么，只是引用其他配置单元而已，这样便可以对配置单元做一个统一的控制，就可以实现大家都非常熟悉的运行级别的概念。比如，想让系统进入图形化模式，需要运行许多服务和配置命令，这些操作都由一个个的配置单元表示，将所有的这些配置单元组合为一个目标（target），就表示需要将这些配置单元全部执行一遍，以便进入目标所代表的系统运行状态（例如：multi-user.target相当于在传统使用sysv的系统中运行级别5）。 timer：定时器配置单元用来定时触发用户定义的操作。这类配置单元取代了atd，crond等传统的定时服务。 snapshot：与target配置单元相似，快照是一组配置单元，它保存了系统当前的运行状态。 journalctl概述Systemd 统一管理所有 Unit 的启动日志。带来的好处就是 ，可以只用journalctl一个命令，查看所有日志（内核日志和 应用日志）。 日志的配置文件/etc/systemd/journald.conf 不带任何选项时，journalctl 输出所有的日志记录(会发现日志信息贼多) 可以通过重定向只输出错误日志: journalctl &gt; 2&amp;&gt;1 journalctl 命令参数 -k(–dmesg): 仅显示内核日志 -f(–follow): 只显示最新的日志项， 并且不断显示新生成的日志项, 此选项隐含了 -n 选项。 -e(–pager-end): 在分页工具内 立即跳转到日志的尾部。 此选项隐含了 -n1000 以确保分页工具不必缓存太多的日志行。 -n(–lines=): 限制显示最新的日志行数。 –pager-end 与 –follow 隐含了此选项。 此选项的参数：若为正整数则表示最大行数； 若为 “all” 则表示不限制行数；若不设参数则表示默认值10行; -u(–unit=UNIT|PATTERN): 仅显示属于特定单元的日志, 也就是单元名称正好等于 UNIT 或者符合 PATTERN 模式的单元;可以多次使用此选项以添加多个并列的匹配条件(相当于用”OR”逻辑连接); 例如：journalctl -u mysql.service -S(–since=): 显示晚于指定时间(–since=)的日志, 参数的格式类似 “2012-10-30 18:17:16” 这样。 如果省略了”时:分:秒”部分，则相当于设为 “00:00:00”, 参数还可以进行如下设置： (1)设为 “yesterday”, “today”, “tomorrow” 以表示那一天的零点(00:00:00)。 (2)设为 “now” 以表示当前时间。 g(–grep=): 使用指定的正则表达式对MESSAGE=字段进行过滤，仅输出匹配的日志项。 -U(–until=): 显示早于指定时间(–until=)的日志。跟-S的一样 -b(–boot=[ID][±offset]): 显示特定于某次启动的日志， 这相当于添加了一个 “_BOOT_ID=” 匹配条件。举例来说， “-b 1”表示按时间顺序排列最早的那次启动， “-b 2”则表示在时间上第二早的那次启动； “-b -0”表示最后一次启动， “-b -1”表示在时间上第二近的那次启动， 以此类推。 _PID=[id]: 按进程、用户或者群组ID查询 p(–priority=): 根据日志等级(等级范围)过滤输出结果。 日志等级数字与其名称之间的对应关系如下： “emerg” (0), “alert” (1), “crit” (2), “err” (3), “warning” (4), “notice” (5), “info” (6), “debug” (7),若设为一个单独的数字或日志等级名称， 则表示仅显示小于或等于此等级的日志(也就是重要程度等于或高于此等级的日志) 参考文档 http://www.jinbuguo.com/systemd/journalctl.html","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"go mod整理文档","slug":"golang-mod","date":"2020-03-09T04:25:37.000Z","updated":"2022-01-27T08:51:49.035Z","comments":true,"path":"2020/03/09/golang-mod/","link":"","permalink":"https://github.com/fafucoder/2020/03/09/golang-mod/","excerpt":"","text":"GOPATH什么是GOPATH终端输入go env 123➜ ~ go envGOPATH&#x3D;&quot;&#x2F;Users&#x2F;dawn&#x2F;go&quot;... 进入到GOPATH目录下结构如下 1234567891011➜ ~ tree -d -L 2 $GOPATH&#x2F;Users&#x2F;dawn&#x2F;go├── bin├── pkg│ ├── mod│ └── sumdb└── src ├── github.com ├── go-types ├── gomod-test └── k8s.io GOPATH目录下一共包含了三个子目录，分别是： bin：存储所编译生成的二进制文件。 pkg：存储预编译的目标文件，以加快程序的后续编译速度。 src：存储所有.go文件或源代码。在编写 Go 应用程序，程序包和库时，一般会以$GOPATH/src/github.com/foo/bar的路径进行存放。 因此在使用 GOPATH 模式下，我们需要将应用代码存放在固定的$GOPATH/src目录下，并且如果执行go get来拉取外部依赖会自动下载并安装到$GOPATH目录下。 为何弃用GOPATHGOPATH 模式下没有版本控制的概念，具有致命的缺陷，至少会造成以下问题 在执行go get的时候，你无法传达任何的版本信息的期望，也就是说你也无法知道自己当前更新的是哪一个版本，也无法通过指定来拉取自己所期望的具体版本 在运行Go应用程序的时候，你无法保证其它人与你所期望依赖的第三方库是相同的版本，也就是说在项目依赖库的管理上，你无法保证所有人的依赖版本都一致 你没办法处理 v1、v2、v3 等等不同版本的引用问题，因为 GOPATH 模式下的导入路径都是一样的，都是github.com/foo/bar 因此Go语言官方从 Go1.11 起开始推进 Go modules（前身vgo），Go1.13 起不再推荐使用 GOPATH 的使用模式 Go ModulesGO MOD环境配置通过go env可以查看配置 12345678➜ ~ go env GO111MODULE&#x3D;&quot;off&quot;GOBIN&#x3D;&quot;&#x2F;Users&#x2F;dawn&#x2F;go&#x2F;bin&quot;GOPATH&#x3D;&quot;&#x2F;Users&#x2F;dawn&#x2F;go&quot;GOPRIVATE&#x3D;&quot;&quot;GOPROXY&#x3D;&quot;https:&#x2F;&#x2F;goproxy.cn,direct&quot;GOROOT&#x3D;&quot;&#x2F;usr&#x2F;local&#x2F;go&quot;GOSUMDB&#x3D;&quot;sum.golang.google.cn&quot; GO111MODULEGo语言提供了 GO111MODULE 这个环境变量来作为 Go modules 的开关，其允许设置以下参数： auto：只要项目包含了 go.mod 文件的话启用 Go modules，目前在 Go1.11 至 Go1.14 是默认值。on：启用 Go modules，推荐设置，将会是未来版本中的默认值。off：禁用 Go modules，不推荐设置。 GOPROXY这个环境变量主要是用于设置 Go 模块代理（Go module proxy），其作用是用于使 Go 在后续拉取模块版本时能够脱离传统的 VCS 方式，直接通过镜像站点来快速拉取。 GOPROXY 的默认值是：https://proxy.golang.org,direct，这有一个很严重的问题，就是 proxy.golang.org 在国内是无法访问的，因此这会直接卡住你的第一步，所以你必须在开启 Go modules 的时，同时设置国内的 Go 模块代理，执行如下命令：go env -w GOPROXY=https://goproxy.cn,direct GOPROXY的值是一个以英文逗号 “,” 分割的 Go 模块代理列表，允许设置多个模块代理，假设你不想使用，也可以将其设置为 “off” ，这将会禁止 Go 在后续操作中使用任何 Go 模块代理。 direct是什么而在刚刚设置的值中，我们可以发现值列表中有 “direct” 标识，它又有什么作用呢？ 实际上 “direct” 是一个特殊指示符，用于指示 Go 回源到模块版本的源地址去抓取（比如 GitHub 等），场景如下：当值列表中上一个 Go 模块代理返回 404 或 410 错误时，Go 自动尝试列表中的下一个，遇见 “direct” 时回源，也就是回到源地址去抓取，而遇见 EOF 时终止并抛出类似 “invalid version: unknown revision…” 的错误。 开启GO Modules12345➜ ~ go env -w GO111MODULE&#x3D;onwarning: go env -w GO111MODULE&#x3D;... does not override conflicting OS environment variable➜ ~ export GO111MODULE&#x3D;on ➜ ~ go env GO111MODULE&#x3D;&quot;on&quot; 如果对应的系统环境变量有值了（进行过设置），会出现如下警告信息：warning: go env -w GO111MODULE=… does not override conflicting OS environment variable。 可以通过直接设置系统环境变量（写入对应的.bash_profile文件亦可）来实现这个目的： ➜ ~ export GO111MODULE=on 关于环境变量优先级的问题go env -w的优先级是最低的(相当于默认配置)，然后系统环境变量次之(环境变量配置在本地，例如配置.zshrc/.bashrc里面的)，当前shell的优先级最高(当前shell export的最高，关闭当前shell,配置的就失效了) 因此如果想设置一次性环境变量，直接在当前终端export就可以了，关闭当前shell后，配置就失效了，所以可以不同的shell终端可以拥有不同的环境变量. 如果GO11MODULES=off,需要下载翻墙包，那要怎么办呢，可以通过如下命令GO11MOGULE=on go get golang.org/x/text@v0.3.2 GO MOD命令 命令 作用 说明 go mod init 生成 go.mod 文件 go mod初始化 go mod download 下载 go.mod 文件中指明的所有依赖 一般配合vendor使用 go mod tidy 整理现有的依赖 有点鸡肋 go mod graph 查看现有的依赖结构 有点鸡肋，依赖冲突可能有用，但是不好排查 go mod edit 编辑 go.mod 文件 有点鸡肋，一般直接在编辑器里修改 go mod vendor 导出项目所有的依赖到vendor目录 一般配合download使用 go mod verify 校验一个模块是否被篡改过 go mod why 查看为什么需要依赖某模块 Go Modules下的go get行为开启go modules后，如何导入数据包呢，答案是跟没开启一样，直接通过 go get导入数据包 go get 命令常用的拉取命令如下:| 命令 | 作用 | 说明 || —————- | —————————————————————- | ————– || go get | 拉取依赖，会进行指定性拉取（更新），并不会更新所依赖的其它模块。 | 获取包 || go get -u | 更新现有的依赖，会强制更新它所依赖的其它全部模块，不包括自身。 | 获取最新包 || go get -u -v | 更新现有的依赖, 并输出详细更新信息。 | 查看详情 || go get -u -t ./… | 更新所有直接依赖和间接依赖的模块版本，包括单元测试中用到的。 | 可以获取全部包 | go get 获取指定版本我想选择具体版本应当如何执行呢，如下:| 命令 | 作用 || ——————————- | —————————————————- || go get golang.org/x/text@latest | 拉取最新的版本，若存在tag，则优先使用。 || go get golang.org/x/text@master | 拉取 master 分支的最新 commit。 || go get golang.org/x/text@v0.3.2 | 拉取 tag 为 v0.3.2 的 commit。 || go get golang.org/x/text@342b2e | 拉取 hash 为 342b231 的 commit，最终会被转换为 v0.3. | go get 版本选择go get拉取的时候有两种选择: 所拉取的模块带有发布tags: 如果只有单个模块，那么就取主版本号最大的那个tag 如果有多个模块，则推算相应的模块路径，取主版本号最大的那个tag（子模块的tag的模块路径会有前缀要求） 所拉取的模块没有发布tags: 默认取主分支最新一次 commit 的 commithash。 go mod 依赖问题go mod 版本格式go mod的版本信息如下： 其版本格式为“主版本号.次版本号.修订号”，版本号的递增规则如下： 主版本号：当你做了不兼容的 API 修改。 次版本号：当你做了向下兼容的功能性新增。 修订号：当你做了向下兼容的问题修正。 假设先行版本号或特殊情况，可以将版本信息追加到“主版本号.次版本号.修订号”的后面，作为延伸，如下: go mod 依赖现在我们已经有一个模块，也有发布的 tag，但是一个模块往往依赖着许多其它许许多多的模块，并且不同的模块在依赖时很有可能会出现依赖同一个模块的不同版本，如下图（来自Russ Cox）： 在上述依赖中，模块 A 依赖了模块 B 和模块 C，而模块 B 依赖了模块 D，模块 C 依赖了模块 D 和 F，模块 D 又依赖了模块 E，而且同模块的不同版本还依赖了对应模块的不同版本。那么这个时候 Go modules 怎么选择版本，选择的是哪一个版本呢？ 我们根据 proposal 可得知，Go modules 会把每个模块的依赖版本清单都整理出来，最终得到一个构建清单，如下图（来自Russ Cox）： 我们看到 rough list 和 final list，两者的区别在于重复引用的模块 D（v1.3、v1.4），其最终清单选用了模块 D 的 v1.4 版本，主要原因： 语义化版本的控制：因为模块 D 的 v1.3 和 v1.4 版本变更，都属于次版本号的变更，而在语义化版本的约束下，v1.4 必须是要向下兼容 v1.3 版本，因此认为不存在破坏性变更，也就是兼容的。 模块导入路径的规范：主版本号不同，模块的导入路径不一样，因此若出现不兼容的情况，其主版本号会改变，模块的导入路径自然也就改变了，因此不会与第一点的基础相冲突。 go mod 引来引发的问题在导入模块的时候，社区规范的愿景很好，但是每个开发者不一定会严格遵循社区规范开发，最后的结果就是导入依赖的时候会出现不兼容的问题，那要如何解决呢？ 获取依赖图， go mod graph | grep “依赖”, 例如: go mod graph | grep “submariner” 使用replace替换为你你需要的版本，例如: github.com/submariner-io/submariner =&gt; github.com/submariner-io/submariner v0.2.0 参考文档 https://colobu.com/2019/09/23/review-go-module-again/ //鸟窝 https://eddycjy.com/posts/go/go-moduels/2020-02-28-go-modules/ //煎鱼(非常详细，推荐阅读) https://qcrao.com/2020/04/27/accident/ //码农桃花源(饶大还是幽默的) https://mp.weixin.qq.com/s/ENKV234bolS8UWd73JtRnQ","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"kubernetes中device-plugins原理解析","slug":"kubernetes-device-plugins","date":"2020-03-04T04:43:04.000Z","updated":"2021-12-17T07:42:48.334Z","comments":true,"path":"2020/03/04/kubernetes-device-plugins/","link":"","permalink":"https://github.com/fafucoder/2020/03/04/kubernetes-device-plugins/","excerpt":"","text":"参考文档 https://www.kubernetes.org.cn/4391.html //深入浅出device-plugins https://www.jianshu.com/p/1047e99ef0f5 //k8s源码分析之kubelet device plugin https://cloud.tencent.com/developer/article/1102413 //NVIDIA/k8s-device-plugin源码分析","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"Ginkgo-golang BDD代码测试框架","slug":"kubernetes-test","date":"2020-02-05T03:53:26.000Z","updated":"2021-12-17T07:42:48.340Z","comments":true,"path":"2020/02/05/kubernetes-test/","link":"","permalink":"https://github.com/fafucoder/2020/02/05/kubernetes-test/","excerpt":"","text":"Ginkgo简介inkgo是Go语言的一个行为驱动开发（BDD， Behavior-Driven Development）风格的测试框架，通常和库Gomega一起使用。Ginkgo在一系列的“Specs”中描述期望的程序行为。 安装使用1234go get -u github.com&#x2F;onsi&#x2F;ginkgo&#x2F;ginkgoginkgo bootstrap 测试模块 It: 是测试例的基本单位，即It包含的代码就算一个测试用例 Context和Describe: 是将一个或多个测试例归类 BeforeEach: 是每个测试例执行前执行该段代码 AfterEach: 是每个测试例执行后执行该段代码 JustBeforeEach: 是在BeforeEach执行之后，测试例执行之前执行 BeforeSuite: 是在该测试集执行前执行，即该文件夹内的测试例执行之前 AfterSuite: 是在该测试集执行后执行，即该文件夹内的测试例执行完后 By: 是打印信息，内容只能是字符串，只会在测试例失败后打印，一般用于调试和定位问题 Fail: 是标志该测试例运行结果为失败，并打印里面的信息 Specify: 和It功能完全一样，It属于其简写 测试方法断言12345678910111213Expect(ACTUAL).Should(Equal(EXPECTED))Expect(ACTUAL).To(Equal(EXPECTED))Expect(ACTUAL).ShouldNot(Equal(EXPECTED))Expect(ACTUAL).NotTo(Equal(EXPECTED))Expect(ACTUAL).ToNot(Equal(EXPECTED))&#x2F;&#x2F; 断言没有发生错误Expect(err).ShouldNot(HaveOccurred())Expect(DoSomethingSimple()).Should(Succeed())&#x2F;&#x2F;断言注解Expect(ACTUAL).To(Equal(EXPECTED), &quot;My annotation %d&quot;, foo)Expect(ACTUAL).To(Equal(EXPECTED), func() string &#123; return &quot;My annotation&quot; &#125;) 相等1234567&#x2F;&#x2F; 如果ACTUAL和EXPECTED都为nil，断言会失败Expect(ACTUAL).Should(Equal(EXPECTED)) Expect(ACTUAL).Should(BeEquivalentTo(EXPECTED)) &#x2F;&#x2F; 使用 &#x3D;&#x3D; 进行比较BeIdenticalTo(expected interface&#123;&#125;) 空值/零值12345&#x2F;&#x2F; 断言ACTUAL为NilExpect(ACTUAL).Should(BeNil()) &#x2F;&#x2F; 断言ACTUAL为它的类型的零值，或者是NilExpect(ACTUAL).Should(BeZero()) 布尔值12Expect(ACTUAL).Should(BeTrue())Expect(ACTUAL).Should(BeFalse()) 错误123456789Expect(ACTUAL).Should(HaveOccurred()) err :&#x3D; SomethingThatMightFail()&#x2F;&#x2F; 没有错误Expect(err).ShouldNot(HaveOccurred()) &#x2F;&#x2F; 如果ACTUAL为Nil则断言成功Expect(ACTUAL).Should(Succeed()) 字符串1234567891011&#x2F;&#x2F; 子串判断Expect(ACTUAL).Should(ContainSubstring(STRING, ARGS...)) &#x2F;&#x2F; 前缀判断Expect(ACTUAL).Should(HavePrefix(STRING, ARGS...)) &#x2F;&#x2F; 后缀判断Expect(ACTUAL).Should(HaveSuffix(STRING, ARGS...)) &#x2F;&#x2F; 正则式匹配Expect(ACTUAL).Should(MatchRegexp(STRING, ARGS...)) JSON/XML/YML123Expect(ACTUAL).Should(MatchJSON(EXPECTED))Expect(ACTUAL).Should(MatchXML(EXPECTED))Expect(ACTUAL).Should(MatchYAML(EXPECTED)) 集合（string, array, map, chan, slice）123456789101112131415161718192021222324&#x2F;&#x2F; 断言为空Expect(ACTUAL).Should(BeEmpty()) &#x2F;&#x2F; 断言长度Expect(ACTUAL).Should(HaveLen(INT)) &#x2F;&#x2F; 断言容量Expect(ACTUAL).Should(HaveCap(INT)) &#x2F;&#x2F; 断言包含元素Expect(ACTUAL).Should(ContainElement(ELEMENT)) &#x2F;&#x2F; 断言等于 其中之一Expect(ACTUAL).Should(BeElementOf(ELEMENT1, ELEMENT2, ELEMENT3, ...)) &#x2F;&#x2F; 断言元素相同，不考虑顺序Expect(ACTUAL).Should(ConsistOf(ELEMENT1, ELEMENT2, ELEMENT3, ...))Expect(ACTUAL).Should(ConsistOf([]SOME_TYPE&#123;ELEMENT1, ELEMENT2, ELEMENT3, ...&#125;)) &#x2F;&#x2F; 断言存在指定的键，仅用于mapExpect(ACTUAL).Should(HaveKey(KEY))&#x2F;&#x2F; 断言存在指定的键值对，仅用于mapExpect(ACTUAL).Should(HaveKeyWithValue(KEY, VALUE)) 数字/时间12345678910111213&#x2F;&#x2F; 断言数字意义（类型不感知）上的相等Expect(ACTUAL).Should(BeNumerically(&quot;&#x3D;&#x3D;&quot;, EXPECTED)) &#x2F;&#x2F; 断言相似，无差不超过THRESHOLD（默认1e-8）Expect(ACTUAL).Should(BeNumerically(&quot;~&quot;, EXPECTED, &lt;THRESHOLD&gt;)) Expect(ACTUAL).Should(BeNumerically(&quot;&gt;&quot;, EXPECTED))Expect(ACTUAL).Should(BeNumerically(&quot;&gt;&#x3D;&quot;, EXPECTED))Expect(ACTUAL).Should(BeNumerically(&quot;&lt;&quot;, EXPECTED))Expect(ACTUAL).Should(BeNumerically(&quot;&lt;&#x3D;&quot;, EXPECTED)) Expect(number).Should(BeBetween(0, 10)) 参考文档 https://blog.gmem.cc/ginkgo-study-note https://onsi.github.io/ginkgo/#running-tests https://blog.csdn.net/goodboynihaohao/article/details/79392500","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes api server接口交互","slug":"kubernetes-api-server","date":"2020-02-04T08:53:12.000Z","updated":"2021-12-17T07:42:48.333Z","comments":true,"path":"2020/02/04/kubernetes-api-server/","link":"","permalink":"https://github.com/fafucoder/2020/02/04/kubernetes-api-server/","excerpt":"","text":"API Server功能k8s API Server提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等HTTP Rest接口，是整个系统的数据总线和数据中心。 kubernetes API Server的功能： 提供了集群管理的REST API接口(包括认证授权、数据校验以及集群状态变更)； 提供其他模块之间的数据交互和通信的枢纽（其他模块通过API Server查询或修改数据，只有API Server才直接操作etcd）; 是资源配额控制的入口， 拥有完备的集群安全机制. API Server原理 api: k8s的内置的资源类型 apis: 用户自定义的crd资源类型 healthz: 健康检查 logs: 日志 API Server 接口访问本地端口 该端口用于接收HTTP请求； 该端口默认值为8080，可以通过API Server的启动参数“–insecure-port”的值来修改默认值； 默认的IP地址为“localhost”，可以通过启动参数“–insecure-bind-address”的值来修改该IP地址； 非认证或授权的HTTP请求通过该端口访问API Server。 安全端口 该端口默认值为6443，可通过启动参数“–secure-port”的值来修改默认值； 默认IP地址为非本地（Non-Localhost）网络端口，通过启动参数“–bind-address”设置该值； 该端口用于接收HTTPS请求； 用于基于Tocken文件或客户端证书及HTTP Base的认证； 用于基于策略的授权； 默认不启动HTTPS安全访问控制。 访问方式 通过 kube-proxy 访问集群： kubectl proxy --port=8080 使用token (参考里面的第二个地址) 使用编程方式调用(client-go调用) 使用kubectl (kubectl get –raw /api/v1/namespaces) 查看API获取所有的api kubectl api-versions curl localhost:8080/api pods api12api/v1/podsapi/v1/nodes/&#123;name&#125;/proxy/pods node api123api/v1/nodes/&#123;name&#125;/proxy/pods #列出指定节点内所有Pod的信息api/v1/nodes/&#123;name&#125;/proxy/stats #列出指定节点内物理资源的统计信息api/v1/nodes/&#123;name&#125;/proxy/spec #列出指定节点的概要信息 参考文档 https://www.huweihuang.com/kubernetes-notes/principle/kubernetes-core-principle-api-server.html //API Server简介 https://k8smeetup.github.io/docs/tasks/administer-cluster/access-cluster-api/ //通过 Kubernetes API 访问集群 https://juejin.im/post/5c934e5a5188252d7c216981 //Kubernetes源码分析之kube-apiserver https://feisky.gitbooks.io/kubernetes/components/apiserver.html //非常详细，推荐阅读 https://kubernetes.io/zh/docs/tasks/administer-cluster/access-cluster-services/","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"优秀的博客收藏","slug":"golang-blog","date":"2020-02-03T11:25:36.000Z","updated":"2021-12-17T07:42:48.325Z","comments":true,"path":"2020/02/03/golang-blog/","link":"","permalink":"https://github.com/fafucoder/2020/02/03/golang-blog/","excerpt":"","text":"博客主 https://changkun.us/about/ // (欧长坤) https://draveness.me/ //面向信仰编程 https://ggaaooppeenngg.github.io/ //高鹏 http://cxd2014.github.io/ //linux 源码阅读 https://ylgrgyq.github.io/ //ylgrgyq linux源码 https://www.luozhiyun.com/ //腾讯某大佬，写得文章那叫一个字牛皮 网址 https://changkun.de/golang/zh-cn/preface https://feisky.gitbooks.io/kubernetes/components/apiserver.html https://www.qikqiak.com/k8s-book","categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"}]},{"title":"kubernetes-kubevirt","slug":"kubernetes-kubevirt","date":"2020-02-03T08:17:56.000Z","updated":"2021-12-17T07:42:48.336Z","comments":true,"path":"2020/02/03/kubernetes-kubevirt/","link":"","permalink":"https://github.com/fafucoder/2020/02/03/kubernetes-kubevirt/","excerpt":"","text":"参考文档 https://cloud.tencent.com/developer/article/1452532 //在Kubernetes中利用 kubevirt 以容器方式运行虚拟机(好文推荐阅读) https://remimin.github.io/2018/09/14/kubevirt/ //跟上面的有点类似 https://blog.csdn.net/u010827484/article/details/86293104 //kubevirt安装使用指南","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes中kubelet源码解析","slug":"kubernetes-kubelet","date":"2020-02-03T02:31:42.000Z","updated":"2021-12-17T07:42:48.336Z","comments":true,"path":"2020/02/03/kubernetes-kubelet/","link":"","permalink":"https://github.com/fafucoder/2020/02/03/kubernetes-kubelet/","excerpt":"","text":"参考文档 https://cizixs.com/2017/06/06/kubelet-source-code-analysis-part-1/ //kubelet 源码分析：启动流程 https://cloud.tencent.com/developer/article/1557556","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes网络接口插件cni","slug":"kubernetes-cni","date":"2020-02-02T11:08:23.000Z","updated":"2022-05-09T14:45:47.632Z","comments":true,"path":"2020/02/02/kubernetes-cni/","link":"","permalink":"https://github.com/fafucoder/2020/02/02/kubernetes-cni/","excerpt":"","text":"CNI配置cni的默认配置目录在/etc/cni/cni.d/ cni的默认可执行目录在 /opt/cni/bin 可以在kubelet中修改默认的路径, 在/etc/systemd/system/kubelet.service.d/10-kubeadm.conf这个文件中的EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env里有关于cni的配置， 通过修改/kubeadm-flags.env可以修改cni的配置 CNI PluginsCNI的插件库主要有: cni interfaces cni plugins CNI InterfacesCNI项目 libcni： 实现 CNI runtime 的 SDK，比如 kubernetes 里面的 NetworkPlugin 部分就使用了 libcni 来调用 CNI plugins. 这里面也有一些 interface，容易混淆，这个 interface 是对于 runtime 而言的，并不是对 plugin 的约束，比如 AddNetworkList, runtime 调用这个方法后，会按顺序执行对应 plugin 的 add 命令。 libcni 里面的 config caching：解决的是如果 ADD 之后配置变化了，如何 DEL 的问题. skel: 实现 CNI plugin 的骨架代码 cnitool: 模拟 runtime 执行比如 libcni 的 AddNetworkList，触发执行 cni plugins CNI Plugin对runtime的假设 Container runtime 先为 container 创建 network 再调用 plugins. Container runtime 决定 container 属于哪个网络，继而决定执行哪个 CNI plugin. Network configuration 为 JSON 格式，包含一些必选可选参数. 创建时 container runtime 串行添加 container 到各个 network (ADD). 销毁时 container runtime 逆序将 container 从各个 network 移除 (DEL). 单个 container 的 ADD/DEL 操作串行，但是多个 container 之间可以并发. ADD 之后必有 DEL，多次 DEL 操作幂等. ADD 操作不会执行两次（对于同样的 KEY-network name, CNI_CONTAINERID, CNI_IFNAME） CNI的运行流程， 具体方法 基本操作: ADD, DEL, CHECK and VERSION Plugins 是二进制，当需要 network 操作时，runtime 执行二进制对应 命令 通过 stdin 向 plugin 输入 JSON 格式的配置文件，以及其他 container 相关的信息 比如：Container ID, Network namespace path, Network configuration, Extra arguments, Name of the interface inside the container 通过 stdout 返回结果, 返回包括Interfaces list, IP configuration assigned to each interface, DNS information 下面进行具体的代码分析： CNI接口实现主要有AddNetwork, CheckNetwork, DelNetwork 123456789101112131415161718// github.com/containernetworking/cni/libcni/api.gotype CNI interface &#123; AddNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) CheckNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error DelNetworkList(ctx context.Context, net *NetworkConfigList, rt *RuntimeConf) error GetNetworkListCachedResult(net *NetworkConfigList, rt *RuntimeConf) (types.Result, error) GetNetworkListCachedConfig(net *NetworkConfigList, rt *RuntimeConf) ([]byte, *RuntimeConf, error) AddNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) (types.Result, error) CheckNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error DelNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) error GetNetworkCachedResult(net *NetworkConfig, rt *RuntimeConf) (types.Result, error) GetNetworkCachedConfig(net *NetworkConfig, rt *RuntimeConf) ([]byte, *RuntimeConf, error) ValidateNetworkList(ctx context.Context, net *NetworkConfigList) ([]string, error) ValidateNetwork(ctx context.Context, net *NetworkConfig) ([]string, error)&#125; CNI结构体 1234567891011121314151617181920212223242526272829303132333435type NetworkConfig struct &#123; Network *types.NetConf //NetConf describes a network. Bytes []byte&#125;type CNIConfig struct &#123; Path []string exec invoke.Exec cacheDir string&#125;type NetConf struct &#123; CNIVersion string `json:\"cniVersion,omitempty\"` Name string `json:\"name,omitempty\"` Type string `json:\"type,omitempty\"` Capabilities map[string]bool `json:\"capabilities,omitempty\"` IPAM IPAM `json:\"ipam,omitempty\"` DNS DNS `json:\"dns\"` RawPrevResult map[string]interface&#123;&#125; `json:\"prevResult,omitempty\"` PrevResult Result `json:\"-\"`&#125;type RuntimeConf struct &#123; ContainerID string NetNS string IfName string Args [][2]string CapabilityArgs map[string]interface&#123;&#125; CacheDir string&#125; AddNetwork操作(CheckNetwork和Deletework同理) 12345678910111213141516171819202122232425262728//直接调用addwork,结果缓存func (c *CNIConfig) AddNetwork(ctx context.Context, net *NetworkConfig, rt *RuntimeConf) (types.Result, error) &#123; result, err := c.addNetwork(ctx, net.Network.Name, net.Network.CNIVersion, net, nil, rt) if err != nil &#123; return nil, err &#125; if err = c.cacheAdd(result, net.Bytes, net.Network.Name, rt); err != nil &#123; return nil, fmt.Errorf(\"failed to set network %q cached result: %v\", net.Network.Name, err) &#125; return result, nil&#125;func (c *CNIConfig) addNetwork(ctx context.Context, name, cniVersion string, net *NetworkConfig, prevResult types.Result, rt *RuntimeConf) (types.Result, error) &#123; // 一系列验证..... //生成networkconfig newConf, err := buildOneConfig(name, cniVersion, net, prevResult, rt) if err != nil &#123; return nil, err &#125; //执行pluginPath下的cni插件， netConf.Bytes写入stdin return invoke.ExecPluginWithResult(ctx, pluginPath, newConf.Bytes, c.args(\"ADD\", rt), c.exec)&#125; CNIConfig: 主要数据成员是plugin的路径，并实现了CNI接口 NetworkConfig和NetworkConfigList： 包括在/etc/cni/net.d下面的配置 RuntimeConf: 定义了runtimeConf配置 AddNetwork(): 先从pluginPath获得plugin的binary，然后injectRuntimeConfig()将网络配置注入到networkconfig中，并作为最后plugin执行的输入，然后还会将network的操作（ADD或者DEL）以及RuntimeConf，作为plugin执行时的变量 CNI Pluginscni plugins库中提供的插件类型有main, ipam, meta main是实现了某种特定网络功能的插件; meta本身并不会提供具体的网络功能，它会调用其他插件，或者单纯是为了测试；ipam是分配IP地址的插件, ipam并不提供某种网络功能，只是为了灵活性把它单独抽象出来，这样不同的网络插件可以根据需求选择ipam，或者实现自己的ipam。 main loopback：这个插件很简单，负责生成 lo 网卡，并配置上 127.0.0.1/8 地址 bridge：和 docker 默认的网络模型很像，把所有的容器连接到虚拟交换机上 macvlan：使用 macvlan 技术，从某个物理网卡虚拟出多个虚拟网卡，它们有独立的 ip 和 mac 地址 ipvlan：和 macvlan 类似，区别是虚拟网卡有着相同的 mac 地址 ptp：通过 veth pair 在容器和主机之间建立通道 meta flannel：结合 bridge 插件使用，根据 flannel 分配的网段信息，调用 bridge 插件，保证多主机情况下容器 tuning：调整现有接口的sysctl参数 portmap：一个基于iptables的portmapping插件。将端口从主机的地址空间映射到容器。 ipam host-local：基于本地文件的 ip 分配和管理，把分配的 IP 地址保存在文件中 dhcp：从已经运行的 DHCP 服务器中获取 ip 地址 kubelet如何调用CNI 在容器运行时(CRI)，会调用CNI插件为sanbox容器配置网络 参考文档 https://cloud.tencent.com/developer/article/1579809?s=original-sharing //Extend Kubernetes - CNI https://wiki.opskumu.com/kubernetes/wang-luo-fang-an/src-kubelet-cni //Kubelet CNI 源码解析 https://zhuanlan.zhihu.com/p/34085344 //kubernetes源码阅读 kubelet对cni的实现 https://www.jianshu.com/p/1919fb8a48ea //Kubelet 对CNI的实现 https://yucs.github.io/2017/12/06/2017-12-6-CNI/ //Kubernetes网络插件CNI调研整理 https://blog.csdn.net/waltonwang/article/details/72669826 //从源码看kubernetes与CNI Plugin的集成 https://www.ziji.work/kubernetes/kubernetes-architecture-from-scratch.html // 不错","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"docker容器运行时containerd","slug":"docker-containerd","date":"2020-01-31T07:51:26.000Z","updated":"2021-12-17T07:42:48.323Z","comments":true,"path":"2020/01/31/docker-containerd/","link":"","permalink":"https://github.com/fafucoder/2020/01/31/docker-containerd/","excerpt":"","text":"container概念一句话解释，就是一组受到资源限制，彼此间相互隔离的进程。实现起来也并不复杂，隔离所用到的技术都是由linux内核本身提供的（所以说目前绝大部分的容器都是必须要跑在linux里面的）。其中namespace用来做访问隔离（每个容器进程都有自己独立的进程空间，看不到其他进程），cgroups用来做资源限制（cpu、内存、存储、网络的使用限制）。总的来说容器就是一种基于操作系统能力的隔离技术，这和基于hypervisor的虚拟化技术（能完整模拟出虚拟硬件和客户机操作系统）复杂度不可同日而语。 可以看出，容器是没有自己的OS的，直接共享宿主机的内核，也没有hypervisor这一层进行资源隔离和限制，所有对于容器进程的限制都是基于操作系统本身的能力来进行的，由此容器获得了一个很大的优势：轻量化，由于没有hypervisor这一层，也没有自己的操作系统，自然占用资源很小，而打出来的镜像文件也要比虚拟机小的多。 containerd概念从Docker 1.11开始，Docker容器运行已经不是简单的通过Docker daemon来启动，而是集成了containerd、runC等多个组件， 其中，containerd独立负责容器运行时和生命周期（如创建、启动、停止、中止、信号处理、删除等），其他一些如镜像构建、卷管理、日志等由Docker Daemon的其他模块处理。 containerd是容器技术标准化之后的产物，为了能够兼容 OCI 标准，将容器运行时及其管理功能从 Docker Daemon 剥离。从理论上来说，即使不运行 dockerd，也能够直接通过 containerd 来管理容器。 containerd向上为Docker Daemon提供了gRPC接口，使得Docker Daemon屏蔽下面的结构变化。向下通过containerd-shim结合runC，使得引擎可以独立升级，避免之前Docker Daemon升级会导致所有容器不可用的问题。 containerd-shim称之为垫片，每启动一个容器都会起一个新的docker-shim的一个进程， 他直接通过指定的三个参数：容器id，boundle目录， 运行是二进制（默认为runc）来调用runc的api创建一个容器（比如创建容器：最后拼装的命令如下：runc create 。。。。。） runC是从Docker的libcontainer中迁移而来的，实现了容器启停、资源隔离等功能 总结分离后的docker运行原理如下所示： 参考手册 https://www.jianshu.com/p/52c0f12b0294 //docker，containerd，runc，docker-shim https://www.infoq.cn/article/2017/01/Docker-Containerd-OCI-1 //Docker 开源容器运行时组件 Containerd https://www.jianshu.com/p/517e757d6d17 //深入理解容器基础概念","categories":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/tags/docker/"}]},{"title":"kubernetes容器运行时containerd","slug":"kubernetes-cri","date":"2020-01-31T07:46:25.000Z","updated":"2021-12-17T07:42:48.334Z","comments":true,"path":"2020/01/31/kubernetes-cri/","link":"","permalink":"https://github.com/fafucoder/2020/01/31/kubernetes-cri/","excerpt":"","text":"k8s工作原理在 K8s 中，存在一个控制面板，也就是我们所说的 master node， 上面运行着 apiserver、controllerManager、kubeScheduler、kubedns 等组件。当我们想要创建一个应用（deployment、statefulset）时，主要流程如下： 具体的流程如下： 通过 kubectl 命令向 apiserver 提交， apiserver 将资源保存在 etcd 中 controller manager 通过控制循环，获取新创建的资源，并创建 pod 信息。注意这里只创建pod，并未调度和创建容器 kube-scheduler 也会循环获取新创建但未调度的pod，并在执行一系列调度算法后，将 pod 绑定到一个 node上，并更新 etcd 中的信息。具体方式是在 pod 的 spec 中加入 nodeName 字段。 Kubelet监视所有Pod对象的更改。当发现Pod已绑定到Node，并且绑定的Node本身时，Kubelet会接管所有后续任务，包括创建 pod 网络，container等。 kubelet通过 CRI 调用 container runtime 创建 pod 中的 container。 引入cri后 kubelet架构图： kubelet调用具体的cri流程如下(以docker为例)： kubelet 通过 CRI(Container Runtime Interface) 接口(gRPC) 调用 docker shim, 请求创建一个容器, 这一步中, Kubelet 可以视作一个简单的 CRI Client, 而 docker shim 就是接收请求的 Server docker shim 收到请求后, 通过适配的方式，适配成 Docker Daemon 的请求格式, 发到 Docker Daemon 上请求创建一个容器。在docker 1.12后版本中，docker daemon被拆分成dockerd和containerd，containerd负责操作容器 dockerd收到请求后， 调用containerd进程去创建一个容器 containerd 收到请求后, 并不会自己直接去操作容器, 而是创建一个叫做 containerd-shim 的进程, 让 containerd-shim 去操作容器 containerd-shim 在这一步需要调用 runC 这个命令行工具, 来启动容器，runC是OCI(Open Container Initiative, 开放容器标准) 的一个参考实现。主要用来设置 namespaces 和 cgroups, 挂载 root filesystem等操作 runC启动完容器后本身会直接退出, containerd-shim 则会成为容器进程的父进程, 负责收集容器进程的状态, 上报给 containerd, 并在容器中 pid 为 1 的进程退出后接管容器中的子进程进行清理, 确保不会出现僵尸进程（关闭进程描述符等） 为何要引入CRI为了让Kubernetes不和某种特定的容器运行时技术绑死，而是能无需重新编译源代码就能够支持多种容器运行时技术的替换，和我们面向对象设计中引入接口作为抽象层一样，在Kubernetes和容器运行时之间我们引入了一个抽象层，即容器运行时接口。 在CRI还没有问世的Kubernetes早期版本里，比如1.3版本里，添加了对另一个容器运行时技术rkt的支持，即rktnetes项目。 这个项目虽然让Kubernetes增加了除Docker之外的另一种容器运行时的支持，然而这种增强的实现方式是通过直接修改kubelet实现源代码进行的，需要贡献者非常熟悉kubelet内部原理，开发门槛较高。 为了实现一个真正支持可插拔替换的容器运行时的机制，Kubernetes引入了CRI的概念。 有了CRI后，kubelet不再直接和容器运行时交互，而是通过CRI这个中间层。 kubelet和CRI通过Unix 套接字或者gRPC框架进行通信。 OCI概念Open Container Initiative，也就是常说的OCI，是由多家公司共同成立的项目，并由linux基金会进行管理，致力于container runtime的标准的制定和runc的开发等工作。 所谓container runtime，主要负责的是容器的生命周期的管理。oci的runtime spec标准中对于容器的状态描述，以及对于容器的创建、删除、查看等操作进行了定义。 runc，是对于OCI标准的一个参考实现，是一个可以用于创建和运行容器的CLI(command-line interface)工具。runc直接与容器所依赖的cgroup/linux kernel等进行交互，负责为容器配置cgroup/namespace等启动容器所需的环境，创建启动容器的相关进程。 为了兼容oci标准，docker也做了架构调整。将容器运行时相关的程序从docker daemon剥离出来，形成了containerd。Containerd向docker提供运行容器的API，二者通过grpc进行交互。containerd最后会通过runc来实际运行容器。 为了进一步与oci进行兼容，kubernetes还孵化了cri-o，成为了架设在cri和oci之间的一座桥梁。通过这种方式，可以方便更多符合oci标准的容器运行时，接入kubernetes进行集成使用。 上面的图片代表容器运行时的四个阶段(从右往左): 最早是在kubelet这一层进行适配，通过启动docker-manager来访问docker（不同的容器产品有不同的manager，这部分代码是kubelet项目标准代码的一部分） K8 1.5之后引入了CRI接口，通过docker-shim（垫片）的方式接入docker。shim程序一般由容器厂商根据CRI规范自己开发，实现方式可以自己定义（即CRI规范定义了要做什么，怎么做可以基于自己的理解）。而docker-shim由于历史原因，还是k8项目组来做的，这部分代码也包含在kubelet代码里面，但架构上是分开的。 docker在分出containerd后，k8也顺应潮流，孵化了cri-containerd项目，用于和containerd对接，这样就不走docker daemon了。 目前孵化的cri-o项目，则连containerd都绕过了，直接使用runC去创建容器（你可以把cri-o看作是k8生态里面的containerd） containerd在Containerd1.0 及以前版本将 dockershim 和 docker daemon 替换为 cri-containerd + containerd，而在 1.1 版本直接将 cri-containerd 内置在 Containerd 中，简化为一个 CRI 插件。 Containerd 内置的 CRI 插件实现了 Kubelet CRI 接口中的 Image Service 和 Runtime Service，通过内部接口管理容器和镜像，并通过 CNI 插件给 Pod 配置网络。 CRI运行原理 CRI 基于 gRPC 定义了 RuntimeService 和 ImageService 等两个 gRPC 服务，分别用于容器运行时和镜像的管理 RuntimeService: 它提供的接口,主要就是和容器有关的操作.比如,创建和启动容器,删除容器,执行 exec 命令等 ImageService: 它提供的接口,主要是和容器镜像相关的操作,比如 拉取镜像,删除镜像等等 CRI工作流程 创建容器 kubelet会通过 grpc 调用 CRI 接口，首先去创建一个环境，也就是所谓的 PodSandbox（pause容器） 当 PodSandbox 可用后，继续调用 Image 或 Container 接口去拉取镜像和创建容器 运行容器CRI 机制能够发挥作用的核心,在于每一个容器项目现在都可以自己实现一个 CRI shim ,自行对 CRI 请求进行处理.这样, Kubernetes 就有了一个统一的容器抽象层,使得下层容器在运行的时候,可以自由地对接,从而进入 Kubernetes 当中去. CRI shim 还有一个重要的工作,就是如何实现 exec , logs 等接口.这些接口不同在于, gRPC 接口调用期间, kubelet 需要和容器项目维护一个长连接来传输数据.这种 API ,就称之为 Streaming API . CRI shim 中对 Streaming API 的实现,依赖于一套独立的 Streaming Server 机制如下： 当对一个容器执行 kubectl exec 命令时,这个请求首先会交给 API Server. API Server 就会调用 kubelet 的 Exec API .此时, kubelet 会调用 CRI 的 Exec 接口,而负责响应这个接口的,就是 CRI shim CRI shim 并不会直接去调用后端的容器项目(比如 Docker )来进行处理,而只会返回一个 URL 给 kubelet .这个 URL ,就是该 CRI shim 对应的 Streaming Server 的地址和端口. kubelet 在拿到这个 URL 之后,就会把它以 Redirect 的方式返回给 API Server . API Server 通过重定向来向 Streaming Server 发起真正的 /exec 请求,和它建立长连接. Stream Server 这一部分具体怎么实现,完全可以由 CRI shim 的维护者自行决定. 参考文档 https://juejin.im/post/5e1ec4456fb9a02ff254aaa7 //K8s、CRI与container https://www.cnblogs.com/xuxinkun/p/8036832.html //docker、oci、runc以及kubernetes梳理 https://www.jianshu.com/p/517e757d6d17 //深入理解容器基础概念 https://feisky.gitbooks.io/kubernetes/plugins/CRI.html //容器运行时文档(推荐阅读） https://www.cnblogs.com/zll-0405/p/10786541.html //[Kubernetes] CRI 的设计与工作原理 https://www.cnblogs.com/justinli/p/11578951.html //kubernetes CRI 前世今生 https://cloud.tencent.com/developer/article/1579900 //Extend Kubernetes - CRI（深度好文）","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes中list-watch机制","slug":"kubernetes-list-watch","date":"2020-01-31T06:45:38.000Z","updated":"2021-12-17T07:42:48.336Z","comments":true,"path":"2020/01/31/kubernetes-list-watch/","link":"","permalink":"https://github.com/fafucoder/2020/01/31/kubernetes-list-watch/","excerpt":"","text":"kubernetes的运行流程 当kubectl创建了ReplicaSet对象后，controller-manager都是通过list-watch这种方式得到了最新的ReplicaSet对象，并执行自己的逻辑来创建Pod对象 其他的几个组件，Scheduler/Kubelet也是一样，通过list-watch得知变化并进行处理。 list-watch机制kubernetes系统的采取Level Trigger而非Edge Trigger的设计理念，所以各组件只需要感知数据最新的状态，而不需要担心错过数据的变化过程。 list-watach机制需要满足以下需求： 实时性(即数据变化时，相关组件越快感知越好) 保证消息的顺序性(即消息要按发生先后顺序送达目的组件。很难想象在Pod创建消息前收到该Pod删除消息时组件应该怎么处理) 保证消息不丢失或者有可靠的重新获取机制(比如说kubelet和kube-apiserver间网络闪断，需要保证网络恢复后kubelet可以收到网络闪断期间产生的消息) list-watch由list和watch组成，list调用资源的list API罗列资源，基于HTTP短链接实现。 watch调用资源的watch API监听资源变更事件，基于HTTP 长链接实现。 当客户端调用 watch API 时，apiserver 在 response 的 HTTP Header 中设置 Transfer-Encoding 的值为 chunked，表示采用分块传输编码，客户端收到该信息后，便和服务端该链接，并等待下一个数据块，即资源的事件信息。 为了满足1,2要求， kubernetes中为每一个REST数据加了一个ResourceVersion字段，并且该字段的值由ETCD来保证全局单调递增(当ETCD中写入一个数据时，全局ResourceVersion就加1)。这样就保证了不同时刻的数据ResourceVersion不同，并且后产生数据的ResourceVersion较之前数据的ResourceVersion大。这样客户端发起watch请求时，只需要带上请求数据在本地缓存中的最新ResourceVersion，而服务端就根据ResourceVersion从小到大把 大于客户端ResourceVersion的数据按顺序推送给客户端即可。这样就保证了推送数据的顺序性。 为了满足3要求， watch请求开始之前，先发起一次list请求，获取集群中当前所有该类数据(同时得到最新的ResourceVersion)，之后基于最新的ResourceVersion发起watch请求, 当watch出错时(比如说网络闪断造成客户端和服务端数据不同步)，重新发起一次list请求获取所有数据，再重新基于最新ResourceVersion来watch. informer机制K8S 的 informer 模块封装 list-watch API，用户只需要指定资源，编写事件处理函数，AddFunc, UpdateFunc 和 DeleteFunc 等。如下图所示，informer 首先通过 list API 罗列资源，然后调用 watch API 监听资源的变更事件，并将结果放入到一个 FIFO 队列，队列的另一头有协程从中取出事件，并调用对应的注册函数处理事件。Informer 还维护了一个只读的 Map Store 缓存，主要为了提升查询的效率，降低 apiserver 的负载。 参考文档 https://zhuanlan.zhihu.com/p/59660536 //理解 K8S 的设计精髓之 List-Watch机制和Informer模块 https://juejin.im/entry/5ad95f55f265da0b767d042d //kubernetes的设计理念 https://yq.aliyun.com/articles/679797 //Kubernetes(k8s)代码解读-apiserver之list-watch篇","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes client-go原理解析","slug":"kubernetes-client-go","date":"2020-01-31T06:29:47.000Z","updated":"2021-12-17T07:42:48.333Z","comments":true,"path":"2020/01/31/kubernetes-client-go/","link":"","permalink":"https://github.com/fafucoder/2020/01/31/kubernetes-client-go/","excerpt":"","text":"参考文档 https://blog.csdn.net/weixin_42663840/article/details/81699303 //深入浅出client-go https://www.cnblogs.com/charlieroro/p/10330390.html //关于client-go的原理和关键代码 https://www.huweihuang.com/article/source-analysis/client-go-source-analysis //client-go源码详细解析 https://blog.ihypo.net/15763910382218.html //原理图 https://mp.weixin.qq.com/s/HSMza_UAkVRMtMLU9LkqFw kubebuilder https://blog.ihypo.net/15763910382218.html https://book.kubebuilder.io/quick-start.html https://github.com/kubernetes-sigs/controller-runtime client-go leader election http://blog.fatedier.com/2019/04/17/k8s-custom-controller-high-available/ https://www.jianshu.com/p/6e6f1d97d635 https://blog.csdn.net/RA681t58CJxsgCkJ31/article/details/104386126","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"kubernetes自定义CRD","slug":"kubernetes-crd","date":"2020-01-31T05:08:29.000Z","updated":"2021-12-17T07:42:48.334Z","comments":true,"path":"2020/01/31/kubernetes-crd/","link":"","permalink":"https://github.com/fafucoder/2020/01/31/kubernetes-crd/","excerpt":"","text":"CRD简介K8S中一切都是resource，比如Deployment，Service等等。 我们可以基于CRD（CustomResourceDefinitions）功能新增resource，比如我想自定义一种Deployment资源，提供不同的部署策略。 k8s中resource可以通过RESTFUL API进行CURD操作，对于CRD创建的resource也是一样的。 CRD仅仅是定义一种resource，我们还需要实现controller，类似于deployment controller等等，监听对应资源的CURD事件，做出对应的处理，比如部署POD。 CRD官方文档 编写自定义CRD编写自定义CRD包括两部分， 一个是定义crd, 一个是编写controller ####自定义CRD 1234567891011121314151617181920212223242526272829303132333435363738apiVersion: apiextensions.k8s.io/v1 #版本kind: CustomResourceDefinition #类型metadata: name: crontabs.stable.example.com #名称必须符合下面的格式：&lt;plural&gt;.&lt;group&gt;spec: group: stable.example.com # REST API使用的组名称：/apis/&lt;group&gt;/&lt;version&gt; version: v1 #版本 scope: Namespaced #Namespaced或Cluster names: plural: crontabs #复数名， URL中使用的复数名称: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt; singular: crontab #单数名 kind: CronTab # CamelCased格式的单数类型。在清单文件中使用 shortNames: # CLI中使用的资源简称 - ct additionalPrinterColumns: #终端显示额外显示的信息 - name: Spec type: string description: The cron spec defining the interval a CronJob is run JSONPath: .spec.cronSpec - name: Replicas type: integer description: The number of jobs launched by the CronJob JSONPath: .spec.replicas - name: Age type: date JSONPath: .metadata.creationTimestamp validation: #验证规则 openAPIV3Schema: properties: spec: properties: cronSpec: #--必须是字符串，并且必须是正则表达式所描述的形式 type: string pattern: '^(\\d+|\\*)(/\\d+)?(\\s+(\\d+|\\*)(/\\d+)?)&#123;4&#125;$' replicas: #----必须是整数，最小值必须为1，最大值必须为10 type: integer minimum: 1 maximum: 10 自定义crd还可以包含自定义验证规则,额外打印信息等 编写完自定义crd, 只需kubectl apply -f xxx.yaml就行 定义完crd就可以定义cr 定义CR1234567apiVersion: \"stable.example.com/v1\"kind: CronTabmetadata: name: my-new-cron-objectspec: cronSpec: \"* * * * */5\" image: my-awesome-cron-image 通过kubectl create -f crd.yaml可以创建一个CRD 通过kubectl get crd可以获取创建的所有CRD。 定义controller定义完crd部分， 就需要实现controller部分 可以使用client-go来作为Kubernetes的客户端， 常用的controller生成框架包括Kubebuilder 官方的自定义controller例子sample-controller client-go原理client-go的原理图如下： 具体流程就是list-watch机制， api-server会发送指定事件， controller通过watch事件， 处理事件。 各名词解析： Reflector: 定义在 cache 包的 Reflector 类中，它监听特定资源类型(Kind)的 Kubernetes API，在ListAndWatch方法中执行。监听的对象可以是 Kubernetes 的内置资源类型或者是自定义资源类型。当 reflector 通过 watch API 发现新的资源实例被创建，它将通过对应的 list API 获取到新创建的对象并在watchHandler方法中将其加入到Delta Fifo队列中。 Informer: 定义在 cache 包的 base controller 中，它从Delta Fifo队列中 pop 出对象，在processLoop方法中执行。base controller 的工作是将对象保存一遍后续获取，并调用 controller 将对象传给 controller。 Indexer: 提供对象的 indexing 方法，定义在 cache 包的 Indexer中。一个典型的 indexing 的应用场景是基于对象的 label 创建索引。Indexer 基于几个 indexing 方法维护索引，它使用线程安全的 data store 来存储对象和他们的key。在 cache 包的 Store 类中定义了一个名为MetaNamespaceKeyFunc的默认方法，可以为对象生成一个/形式的key。 编写controller代码这部分代码来源于maoqide 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150/**** main.go*/// 创建 clientsetkubeClient, err := kubernetes.NewForConfig(cfg) // k8s clientset, \"k8s.io/client-go/kubernetes\"exampleClient, err := clientset.NewForConfig(cfg) // sample clientset, \"k8s.io/sample-controller/pkg/generated/clientset/versioned\"// 创建 InformerkubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30) // k8s informer, \"k8s.io/client-go/informers\"exampleInformerFactory := informers.NewSharedInformerFactory(exampleClient, time.Second*30) // sample informer, \"k8s.io/sample-controller/pkg/generated/informers/externalversions\"// 创建 controller，传入 clientset 和 informercontroller := NewController(kubeClient, exampleClient, kubeInformerFactory.Apps().V1().Deployments(), exampleInformerFactory.Samplecontroller().V1alpha1().Foos())// 运行 Informer，Start 方法为非阻塞，会运行在单独的 goroutine 中kubeInformerFactory.Start(stopCh) exampleInformerFactory.Start(stopCh)// 运行 controllercontroller.Run(2, stopCh)/**** controller.go */NewController() *Controller &#123; // 将 CRD 资源类型定义加入到 Kubernetes 的 Scheme 中，以便 Events 可以记录 CRD 的事件 utilruntime.Must(samplescheme.AddToScheme(scheme.Scheme)) // 创建 Broadcaster eventBroadcaster := record.NewBroadcaster() // ... ... // 监听 CRD 类型'Foo'并注册 ResourceEventHandler 方法，当'Foo'的实例变化时进行处理 fooInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: controller.enqueueFoo, UpdateFunc: func(old, new interface&#123;&#125;) &#123; controller.enqueueFoo(new) &#125;, &#125;) // 监听 Deployment 变化并注册 ResourceEventHandler 方法， // 当它的 ownerReferences 为 Foo 类型实例时，将该 Foo 资源加入 work queue deploymentInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs&#123; AddFunc: controller.handleObject, UpdateFunc: func(old, new interface&#123;&#125;) &#123; newDepl := new.(*appsv1.Deployment) oldDepl := old.(*appsv1.Deployment) if newDepl.ResourceVersion == oldDepl.ResourceVersion &#123; return &#125; controller.handleObject(new) &#125;, DeleteFunc: controller.handleObject, &#125;)&#125;func (c *Controller) Run(threadiness int, stopCh &lt;-chan struct&#123;&#125;) error &#123; // 在启动 worker 前等待缓存同步 if ok := cache.WaitForCacheSync(stopCh, c.deploymentsSynced, c.foosSynced); !ok &#123; return fmt.Errorf(\"failed to wait for caches to sync\") &#125; // 运行两个 worker 来处理资源 for i := 0; i &lt; threadiness; i++ &#123; go wait.Until(c.runWorker, time.Second, stopCh) &#125;&#125;// 无限循环，不断的调用 processNextWorkItem 处理下一个对象func (c *Controller) runWorker() &#123; for c.processNextWorkItem() &#123; &#125;&#125;// 从workqueue中获取下一个对象并进行处理，通过调用 syncHandlerfunc (c *Controller) processNextWorkItem() bool &#123; obj, shutdown := c.workqueue.Get() if shutdown &#123; return false &#125; err := func(obj interface&#123;&#125;) error &#123; // 调用 workqueue.Done(obj) 方法告诉 workqueue 当前项已经处理完毕， // 如果我们不想让当前项重新入队，一定要调用 workqueue.Forget(obj)。 // 当我们没有调用Forget时，当前项会重新入队 workqueue 并在一段时间后重新被获取。 defer c.workqueue.Done(obj) var key string var ok bool // 我们期望的是 key 'namespace/name' 格式的 string if key, ok = obj.(string); !ok &#123; // 无效的项调用Forget方法，避免重新入队。 c.workqueue.Forget(obj) utilruntime.HandleError(fmt.Errorf(\"expected string in workqueue but got %#v\", obj)) return nil &#125; if err := c.syncHandler(key); err != nil &#123; // 放回workqueue避免偶发的异常 c.workqueue.AddRateLimited(key) return fmt.Errorf(\"error syncing '%s': %s, requeuing\", key, err.Error()) &#125; // 如果没有异常，Forget当前项，同步成功 c.workqueue.Forget(obj) klog.Infof(\"Successfully synced '%s'\", key) return nil &#125;(obj) if err != nil &#123; utilruntime.HandleError(err) return true &#125; return true&#125;// 对比真实的状态和期望的状态并尝试合并，然后更新Foo类型实例的状态信息func (c *Controller) syncHandler(key string) error &#123; // 通过 workqueue 中的 key 解析出 namespace 和 name namespace, name, err := cache.SplitMetaNamespaceKey(key) // 调用 lister 接口通过 namespace 和 name 获取 Foo 实例 foo, err := c.foosLister.Foos(namespace).Get(name) deploymentName := foo.Spec.DeploymentName // 获取 Foo 实例中定义的 deploymentname deployment, err := c.deploymentsLister.Deployments(foo.Namespace).Get(deploymentName) // 没有发现对应的 deployment，新建一个 if errors.IsNotFound(err) &#123; deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Create(newDeployment(foo)) &#125; // OwnerReferences 不是 Foo 实例，warning并返回错误 if !metav1.IsControlledBy(deployment, foo) &#123; msg := fmt.Sprintf(MessageResourceExists, deployment.Name) c.recorder.Event(foo, corev1.EventTypeWarning, ErrResourceExists, msg) return fmt.Errorf(msg) &#125; // deployment 中 的配置和 Foo 实例中 Spec 的配置不一致，即更新 deployment if foo.Spec.Replicas != nil &amp;&amp; *foo.Spec.Replicas != *deployment.Spec.Replicas &#123; deployment, err = c.kubeclientset.AppsV1().Deployments(foo.Namespace).Update(newDeployment(foo)) &#125; // 更新 Foo 实例状态 err = c.updateFooStatus(foo, deployment) c.recorder.Event(foo, corev1.EventTypeNormal, SuccessSynced, MessageResourceSynced)&#125; 参考文档 https://blog.csdn.net/boling_cavalry/article/details/88934063 //编写自定义CRD三部曲， 跟着编写就能实现一个自定义CRD http://maoqide.live/post/cloud/sample-controller/ //CRD的实现原理， 以及简要代码 https://jimmysong.io/kubernetes-handbook/concepts/crd.html //CRD的文档 https://www.jianshu.com/p/cc7eea6dd1fb //如何定义一个crd https://juejin.im/post/5daf8a8d6fb9a04e366a49a8 //编写crd","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"}]},{"title":"云计算基础-openvswitch技术","slug":"openvswitch","date":"2020-01-31T04:58:09.000Z","updated":"2021-12-17T07:42:48.353Z","comments":true,"path":"2020/01/31/openvswitch/","link":"","permalink":"https://github.com/fafucoder/2020/01/31/openvswitch/","excerpt":"","text":"概念在过去，数据中心的服务器是直接连在硬件交换机上，后来VMware实现了服务器虚拟化技术，使虚拟服务器(VMs)能够连接在虚拟交换机上，借助这个虚拟交换机，可以为服务器上运行的VMs或容器提供逻辑的虚拟的以太网接口，这些逻辑接口都连接到虚拟交换机上，有三种比较流行的虚拟交换机: VMware virtual switch, Cisco Nexus 1000V,和Open vSwitch。 OpenvSwitch(OVS)是运行在虚拟化平台上的虚拟交换机，其支持OpenFlow协议，也支持gre/vxlan/IPsec等隧道技术。在OVS之前，基于Linux的虚拟化平台比如KVM或Xen上，缺少一个功能丰富的虚拟交换机，因此OVS迅速崛起并开始在Xen/KVM中流行起来，并且应用于越来越多的开源项目，比如openstack neutron中的网络解决方案。 OpenvSwitch是一个高质量的、多层虚拟交换机，使用开源Apache2.0许可协议，由Nicira Networks开发，主要实现代码为可移植的C代码。它的目的是让大规模网络自动化可以通过编程扩展,同时仍然支持标准的管理接口和协议(例如NetFlow, sFlow, SPAN, RSPAN, CLI, LACP, 802.1ag)。 ovs支持以下特性: 支持NetFlow, IPFIX, sFlow, SPAN/RSPAN等流量监控协议 精细的ACL和QoS策略 可以使用OpenFlow和OVSDB协议进行集中控制 Port bonding，LACP，tunneling(vxlan/gre/Ipsec) 适用于Xen，KVM，VirtualBox等hypervisors 支持标准的802.1Q VLAN协议 基于VM interface的流量管理策略 支持组播功能 flow-caching engine(datapath模块) ovn架构OVN部署由以下几个组件组成： CMS(云管系统)。这是OVN的最终用户(通过其用户和管理员)。OVN最初的目标CMS是OpenStack。 安装在一个中央位置的OVN数据库，可以是物理节点，虚拟节点，甚至是一个集群。 一个或多个虚拟机管理程序（hypervisors）。hypervisors必须运行Open vSwitch, 任何支持的OVS的hypervisor平台都是可以接受的。 零个或多个网关。 网关通过在隧道和物理以太网端口之间双向转发数据包，将基于隧道的逻辑网络扩展到物理网络。这允许非虚拟机器参与逻辑网络。网关可能是物理机，虚拟机或基于ASIC同时支持vtep模式的硬件交换机。 hypervisor和网关一起被称为传输节点或chassis ovn架构如下所示： Openstack/CMS plugin 是 CMS 和 OVN 的接口，将CMS 的配置转化成 OVN 的格式写到 Northbound DB 。 Northbound DB 存储逻辑数据，与传统网络设备概念一致，比如 logical switch，logical router，ACL，logical port。 ovn-northd 类似于一个集中式控制器，把Northbound DB 里面的数据翻译后写到 Southbound DB 。 Southbound DB 保存的数据和 Northbound DB 语义完全不一样，主要包含 3 类数据，一是物理网络数据，比如 HV（hypervisor）的 IP 地址，HV 的 tunnel 封装格式；二是逻辑网络数据，比如报文如何在逻辑网络里面转发；三是物理网络和逻辑网络的绑定关系，比如逻辑端口关联到哪个 HV 上面。 ovn-controller 是 OVN 里面的 agent，类似于 neutron 里面的 ovs-agent，运行在每个 HV 上面，北向，ovn-controller 会把物理网络的信息写到 Southbound DB，南向，把 Southbound DB 保存的数据转化成 Openflow flow 配到本地的 OVS table 里面，来实现报文的转发。 ovs-vswitchd 和 ovsdb-server 是 OVS 的两个进程。 ovn中的信息流如下： OVN中的配置数据从北向南流动。CMS通过其OVN/CMS插件，通过北向数据库将逻辑网络配置传递给ovn-northd, ovn-northd将配置信息编译为较低级别的表单(流表），并通过南行数据库将其传递到所有的chassis。 OVN部署中每个chassis必须配置专用于OVN使用的OpenvSwitch网桥，称为集成网桥。如果需要，系统启动脚本可以在启动ovn-controller之前创建此网桥。如果ovn-controller启动时这个网桥不存在，它会自动创建。Chassis的信息保存在Southbound DB 里面，由 ovn-controller/ovn-controller-vtep 来维护。 当 ovn-controller 启动的时候，它去本地的数据库 Open_vSwitch 表里面读取external_ids:system_id，external_ids:ovn-remote，external_ids:ovn-encap-ip 和external_ids:ovn-encap-type的值，然后它把这些值写到 Southbound DB 里面的表 Chassis 和表 Encap 里面。(external_ids:system_id表示 Chassis 名字。external_ids:ovn-remote表示 Sounthbound DB 的 IP 地址。external_ids:ovn-encap-ip表示 tunnel endpoint IP 地址, external_ids:ovn-encap-type表示 tunnel 封装类型，可以是 VXLAN/Geneve/STT。) OVN 支持的 tunnel 类型有三种，分别是 Geneve，STT 和 VXLAN。HV 与 HV 之间的流量，只能用 Geneve 和 STT 两种，HV 和 VTEP 网关之间的流量除了用 Geneve 和 STT 外，还能用 VXLAN. OVN-North DBNB存放的是我们定义的逻辑交换机、逻辑路由器之类的数据，我们可以通过ovn提供的命令行（ovn-nbctl）完成添加、删除、修改、查询等操作；当然可以写代码通过OVSDB协议完成类似动作。OVN的NB是面向“上层应用”的或者叫“云管平台（Cloud Management System，CMS）”所以叫“北向接口”。 OVN-Sourth DBSB进程比较特殊它同时接受两边的“写入”，首先是运行在ovn-host上的ovn-controller启动之后会去主动连接到ovn-central节点上的SB进程，把自己的IP地址（Chassis），本机的OVS状态（Datapath_Binding）写入到SB数据库中（所以叫南向接口）。ovn-controller还“监听”（etcd、zookeeper类似的功能）SB数据库中流表的变化（Flow）去更新本地的OVS数据库，这叫“流表下发”。 SB中的流表是由运行在ovn-central节点上的ovn-northd进程修改的，ovn-northd会“监听”NB的改变，把逻辑交换机、路由器的定义转换成流表（Flow）写入到SB数据库。 ovs架构OVS整体架构中，用户空间主要组件有数据库服务ovsdb-server和守护进程ovs-vswitchd。内核空间中有datapath内核模块。最上面的Controller表示OpenFlow控制器，控制器与OVS是通过OpenFlow协议进行连接。 ovsdb-serverovsdb-server 是ovs轻量级的数据库服务，用于存储整个 OvS 的配置信息，包括接口，交换内容，VLAN，虚拟交换机的创建，网卡的添加等信息与操作记录。都被 ovsdb 保存到一个 conf.db 文件（JSON 格式）里面，通过 db.sock 提供服务。OvS 主进程 ovs-vswitchd 根据数据库中的配置信息工作。 1234567891011ovsdb-server &#x2F;etc&#x2F;openvswitch&#x2F;conf.db -vconsole:emer -vsyslog:err -vfile:info --remote&#x3D;punix:&#x2F;var&#x2F;run&#x2F;openvswitch&#x2F;db.sock --private-key&#x3D;db:Open_vSwitch,SSL,private_key --certificate&#x3D;db:Open_vSwitch,SSL,certificate --bootstrap-ca-cert&#x3D;db:Open_vSwitch,SSL,ca_cert --no-chdir --log-file&#x3D;&#x2F;var&#x2F;log&#x2F;openvswitch&#x2F;ovsdb-server.log --pidfile&#x3D;&#x2F;var&#x2F;run&#x2F;openvswitch&#x2F;ovsdb-server.pid --detach --monitor 其中： /etc/openvswitch/conf.db：是数据库文件存放位置，ovsdb-server 需要该文件才能启动，可以使用 ovsdb-tool create 命令创建并初始化此数据库文件。 –remote=punix:/var/run/openvswitch/db.sock：实现了一个 Unix Sockets 连接，OvS 主进程 ovs-vswitchd 或其它命令工具（e.g. ovsdb-client） 通过这个 Socket 连接来管理 ovsdb。 /var/log/openvswitch/ovsdb-server.log：ovsdb-server 的运行日志文件。 ovs-vswitchdovs-vswitchd 本质是一个守护进程，是 OvS 的核心部件。ovs-vswitchd 和 Datapath 一起实现 OvS 基于流表（Flow-based Switching）的数据交换。它通过 OpenFlow 协议可以与 OpenFlow 控制器通信，使用 ovsdb 协议与 ovsdb-server 数据库服务通信，使用 netlink 和 Datapath 内核模块通信。ovs-vswitchd 支持多个独立的 Datapath，ovs-vswitchd 需要加载 Datapath 内核模块才能正常运行。ovs-vswitchd 在启动时读取 ovsdb-server 中的配置信息，然后自动配置 Datapaths 和 OvS Switches 的 Flow Tables，所以用户不需要额外的通过执行ovs-dpctl指令工具去操作 Datapath。当 ovsdb 中的配置内容被修改，ovs-vswitched 也会自动更新其配置以保持数据同步。ovs-vswitchd 也可以从 OpenFlow 控制器获取流表项。 在OVS中，ovs-vswitchd从OpenFlow控制器获取流表规则，然后把从datapath中收到的数据包在流表中进行匹配，找到匹配的flows并把所需应用的actions返回给datapath，同时作为处理的一部分，ovs-vswitchd会在datapath中设置一条datapath flows用于后续相同类型的数据包可以直接在内核中执行动作，此datapath flows相当于OpenFlow flows的缓存。对于datapath来说，其并不知道用户空间OpenFlow的存在 工作原理 内核态的 Datapath 监听接口设备流入的数据包。 如果 Datapath 在内核态流表缓存没有找到相应的匹配流表项则将数据包传入（upcall）到用户态的 ovs-vswitchd 守护进程处理。 （可选）用户态的 ovs-vswitchd 拥有完整的流表项，通过 OpenFlow 协议与 OpenFlow 控制器或者 ovs-ofctl 命令行工具进行通信，主要是接收 OpenFlow 控制器南向接口的流表项下发。或者根据流表项设置，ovs-vswitchd 可能会将网络包以 Packet-In 消息发送给 OpenFlow 控制器处理。 ovs-vswitchd 接收到来自 OpenFlow 控制器或 ovs-ofctl 命令行工具的消息后会对内核态的 Flow Table 进行更新。或者根据局部性原理，用户态的 ovs-vswitchd 会将刚刚执行过的 Datapath 没有缓存的流表项注入到 Flow Table 中。 ovs-vswitchd 匹配完流表项之后将数据包重新注入（reinject）到 Datapath。 Datapath 再次访问 Flow Table 获取流表项进行匹配。 最后，网络包被 Datapath 根据流表项 Actions 转发或丢弃。 上述，Datapath 和 ovs-vswitchd 相互配合中包含了两种网络包的处理方式： Fast Path：Datapatch 加载到内核后，会在网卡上注册一个钩子函数，每当有网络包到达网卡时，这个函数就会被调用，将网络包开始层层拆包（MAC 层，IP 层，TCP 层等），然后与流表项匹配，如果找到匹配的流表项则根据既定策略来处理网络包（e.g. 修改 MAC，修改 IP，修改 TCP 端口，从哪个网卡发出去等等），再将网络包从网卡发出。这个处理过程全在内核完成，所以非常快，称之为 Fast Path。 Slow Path：内核态并没有被分配太多内存，所以内核态能够保存的流表项很少，往往有新的流表项到来后，老的流表项就被丢弃。如果在内核态找不到流表项，则需要到用户态去查询，网络包会通过 netlink（一种内核态与用户态交互的机制）发送给 ovs-vswitchd，ovs-vswitchd 有一个监听线程，当发现有从内核态发过来的网络包，就进入自己的处理流程，然后再次将网络包重新注入到 Datapath。显然，在用户态处理是相对较慢的，故称值为 Slow Path。 在用户态的 ovs-vswtichd 不需要吝啬内存，它包含了所有流表项，这些流表项可能是 OpenFlow 控制器通过 OpenFlow 协议下发的，也可能是 OvS 命令行工具 ovs-ofctl 设定的。ovs-vswtichd 会根据网络包的信息层层匹配，直到找到一款流表项进行处理。如果实在找不到，则一般会采用默认流表项，比如丢弃这个包。 当最终匹配到了一个流表项之后，则会根据 “局部性原理（局部数据在一段时间都会被频繁访问，是缓存设计的基础原理）” 再通过 netlink 协议，将这条策略下发到内核态，当这条策略下发给内核时，如果内核的内存空间不足，则会开始淘汰部分老策略。这样保证下一个相同类型的网络包能够直接从内核匹配到，以此加快执行效率。由于近因效应，接下来的网络包应该大概率能够匹配这条策略的。例如：传输一个文件，同类型的网络包会源源不断的到来。 常见命令OpenVswitch 有许多命令，分别有不同的作用，大致如下： ovs-vsctl 用于控制ovs db ovs-ofctl 用于管理OpenFlow switch中的流表flow ovs-dpctl 用于管理ovs的datapath ovs-appctl 用于管理ovs daemon 参考文档架构 https://zhuanlan.zhihu.com/p/105582483 https://www.cnblogs.com/liuhongru/p/11121731.html https://blog.csdn.net/ptmozhu/article/details/78644825 //值得一读 https://www.cnblogs.com/gaozhengwei/p/7099928.html //推荐阅读 ovs原理 https://opengers.github.io/openstack/openstack-base-use-openvswitch/ //open vswitch详解， 非常值得阅读 https://blog.csdn.net/Jmilk/article/details/86989975 //关于ovs的详细内容， 比上面的更加详细 https://tonydeng.github.io/sdn-handbook/ovs/internal.html //源码级别 https://www.cnblogs.com/popsuper1982/p/8948016.html //源码级别 ovs命令 https://blog.csdn.net/rocson001/article/details/73163041 //ovs-ofctl和ovs-vsctl http://www.rendoumi.com/open-vswitchde-ovs-vsctlming-ling-xiang-jie //ovs-vsctl vlan https://segmentfault.com/a/1190000019612525?utm_source=tag-newest //ovs docker https://www.sdnlab.com/sdn-guide/14747.html //ovs-vsctl 基础","categories":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"}]},{"title":"kubeadm搭建kubernetes环境(ubuntu)","slug":"kubernetes-install","date":"2020-01-10T03:39:01.000Z","updated":"2021-12-17T07:42:48.335Z","comments":true,"path":"2020/01/10/kubernetes-install/","link":"","permalink":"https://github.com/fafucoder/2020/01/10/kubernetes-install/","excerpt":"","text":"安装docker环境安装docker方法一(官方安装方式)： https://docs.docker.com/v17.12/install/ 方法二(使用国内镜像): curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add sudo add-apt-repository “deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu $(lsb_release -cs) stable” sudo apt-get update sudo apt install docker-ce docker国内镜像列表 中国科技大学: https://docker.mirrors.ustc.edu.cn ustc: https://docker.mirrors.ustc.edu.cn 网易: http://hub-mirror.c.163.com docker中国：https://registry.docker-cn.com 默认使用国内镜像sudo vim /etc/docker/daemon.json 123&#123; \"registry-mirrors\": [\"http://hub-mirror.c.163.com\"]&#125; sudo service docker reload 解决docker没权限问题(把当前用户组加到docker里面) sudo groupadd docker #添加docker用户组 sudo gpasswd -a $USER docker #将登陆用户加入到docker用户组中 newgrp docker #更新用户组 docker ps #测试docker命令是否可以使用sudo正常使用 禁用交换分区 sudo swapoff -a sudo sed -i ‘/swap/ s/^/#/‘ /etc/fstab 安装k8s环境使用阿里云源(官方源速度慢) sudo apt update &amp;&amp; sudo apt install -y apt-transport-https curl curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add - echo “deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main” | sudo tee /etc/apt/sources.list.d/kubernetes.list 安装kubeadm, kubelet, kubectl(默认安装的是最新版) sudo apt update sudo apt install -y kubelet kubeadm kubectl sudo apt-mark hold kubelet kubeadm kubectl 安装kubeadm, kubelet, kubectl(指定版本) sudo apt update sudo apt install -y kubelet=1.15.0-00 kubeadm=1.15.0-00 kubectl=1.15.0-00 sudo apt-mark hold kubelet=1.15.0-00 kubeadm=1.15.0-00 kubectl=1.15.0-00 验证是否安装成功 kubeadm version kubectl version 初始化集群master节点 sudo kubeadm init –pod-network-cidr=192.168.0.0/16 //pod网络可以使用的地址段 –kubernetes-version=kubeadm_version //kubernetes版本 –apiserver-advertise-address=public_address //API Server将要广播的监听地址。如指定为 0.0.0.0 将使用缺省的网卡地址， 可以指定公网ip –ignore-preflight-errors=all //忽略错误 –image-repository=registry.aliyuncs.com/google_containers //设置镜像源为阿里云源 如果已经init完，可以通过kubeadm token create --print-join-command获取token node节点sudo kubeadm join 172.20.245.50:6443 –token hsf26a.7la0qux76fnm0wko –discovery-token-ca-cert-hash sha256:8aae49541d380f16fae8a3fe4b9c436fbb5e7d2d4641f30790e561479f48cf13 –ignore-preflight-errors=all -v 10 解决无法init问题(如果你没有设置image-repository,默认使用k8s源)原因： 无法拉取包解决办法： fq(百度，github) 本地先拉去下来，然后替换（ https://blog.csdn.net/jinguangliu/article/details/82792617 ) 通过shell批量拉取删除（ https://ystyle.top/2018/07/02/pre-download-kubernetes-image ） node not ready排查master节点上： kubectl describe nodes node-name 查看node的详细信息node节点上： journalctl -xefu kubelet, 一般问题可能就是包没法下载docker排查： docker logs c36c56e4cfa3","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://github.com/fafucoder/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://github.com/fafucoder/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}]},{"title":"virtualbox 安装ubuntu问题记录","slug":"virtualbox-ubuntu","date":"2020-01-10T03:39:01.000Z","updated":"2021-12-17T07:42:48.353Z","comments":true,"path":"2020/01/10/virtualbox-ubuntu/","link":"","permalink":"https://github.com/fafucoder/2020/01/10/virtualbox-ubuntu/","excerpt":"","text":"安装ubuntu server源速度慢安装过程中，不要使用默认的源，在安装时选择国内源可解决此问题http://cn.archive.ubuntu.com/ubuntu/ 替换为 http://mirrors.163.com/ubuntu或http://mirrors.sohu.com/ubuntu virtualbox 复制中IP冲突/etc/machine-id，里面存放了机器ID，并尝试修改成不同值后，DHCP能得到不同IP地址了。 /etc/machine-id 是一个只读文件，权限是444（即,r–r–r–）。有两种修改方法： 先chmod修改其权限，让它可写；然后修改它；再将权限改回。 sudo将它删除，然后创建一个并写入一些字符；再改回权限。 machine-id中的值是一串字母数字组合，应该是固定长度。 胡乱修改一下，使它缺几位，多几位，非法字符等，则系统重启后，该值会自动重新分配。 参考资料 https://www.cnblogs.com/jyginger/p/12297548.html https://blog.csdn.net/u011850815/article/details/43668263","categories":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://github.com/fafucoder/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"环境搭建","slug":"环境搭建","permalink":"https://github.com/fafucoder/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"}]},{"title":"kube-ovn学习指南","slug":"kube-ovn","date":"2020-01-09T14:31:08.000Z","updated":"2021-12-17T07:42:48.328Z","comments":true,"path":"2020/01/09/kube-ovn/","link":"","permalink":"https://github.com/fafucoder/2020/01/09/kube-ovn/","excerpt":"","text":"kube-ovn的系统架构 Kube-OVN 自身的逻辑主要集中在图中蓝色的部分 kube-ovn-controller，kube-ovn-cni 和 kube-ovn-cniserver。 kube-ovn-controller 可以看成是一个 Kubernetes 资源的控制器，它会 watch Kubernetes 内所有和网络相关的资源变化，例如 Pod、Node、Namespace、Service、Endpoint 和 NetworkPolicy。每当资源发生变化 kube-ovn-controller 会计算预期的状态，并将网络对应的变化翻译成 OVN 北向数据库的资源对象。同时 kube-ovn-controller 会将配置具体网络的信息，例如分配的 IP、Mac、网关等信息再以 Annotation 的方式回写到 Kubernetes 的资源里，方便后续的处理。 kube-ovn-cni主要工作是适配 CNI 的协议和 Kubelet 对接，将标准的 cni add/del 命令发送到每台机器上的 kube-ovn-cniserver 进行后续处理。 kube-ovn-cniserver 会根据传入的参数反查 Kubernetes 中资源对应的 Annotation，操作每台机器上的 OVS 以及容器网络。 kubeovn-controller watch到数据后操作的是ovn, 而cni-server操作的是ovs, 这两个组件之间是通过annotation进行关联通信的。 以创建 Pod 的例子，Pod 下发到 apiserver 后 kube-ovn-controller 会 watch 的新生成了一个 Pod，然后调用 ovn-nb 的接口去创建一个虚拟交换机接口，成功后将 OVN 分配的 IP、Mac、GW 等信息反写到这个 Pod 的 Annotation 中。接下来 kubelet 创建 Pod 时会调用 kube-ovn-cni，kube-ovn-cni 将信息传递给 kube-ovn-cniserver。CNIServer 会反查 Pod 的 Annotation 获得具体的 IP 和 Mac 信息来配置本地的 OVS 和容器网卡，完成整个工作流。其他的 Service、NetworkPolicy 的流程也是和这个类似。 kube-ovn的网络模型 Kube-OVN 采用了一个 Namespace 一个子网的模型，子网是可以跨节点的这样比较符合用户的预期和管理。每个子网对应着 OVN 里的一个虚拟交换机，LB、DNS 和 ACL 等流量规则现在也是应用在虚拟交换机上，这样方便之后做更细粒度的权限控制，例如实现 VPC，多租户这样的功能。 所有的虚拟交换机目前会接在一个全局的虚拟路由器上(ovn-vsctl)，这样可以保证默认的容器网络互通，未来要做隔离也可以很方便的在路由层面进行控制。此外还有一个特殊的 Node 子网，会在每个宿主机上添加一块 OVS 的网卡，这个网络主要是把 Node 接入容器网络，使得主机和容器之间网络可以互通。需要注意的是这里的虚拟交换机和虚拟路由器都是逻辑上的，实现中是通过流表分布在所有的节点上的，因此并不存在单点的问题。 网关负责访问集群外部的网络，目前有两种实现，一种是分布式的，每台主机都可以作为运行在自己上面的 pod 的出网节点。另一种是集中式的，可以一个 Namespace 配置一个网关节点，作为当前 Namespace 里的 Pod 出网所使用的网关，这种方式所有出网流量用的都是特定的 IP nat 出去的，方便外部的审计和防火墙控制。当然网关节点也可以不做 nat 这样就可以把容器 IP 直接暴露给外网，达到内外网络的直连。 kube-ovn源代码kube-ovn-controller kube-ovn-cniserver 参考网址 http://weekly.dockerone.com/article/9076 //关于kube-ovn的网络架构和工作原理 https://github.com/alauda/kube-ovn/wiki //kube-ovn如何使用 https://neuvector.com/network-security/advanced-kubernetes-networking/ //其他网络插件流程 https://www.jianshu.com/p/a58514b34f54 //从cni到ovn网络","categories":[{"name":"kube-ovn","slug":"kube-ovn","permalink":"https://github.com/fafucoder/categories/kube-ovn/"}],"tags":[{"name":"kube-ovn","slug":"kube-ovn","permalink":"https://github.com/fafucoder/tags/kube-ovn/"}]}],"categories":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/categories/golang/"},{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/categories/linux/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/categories/kubernetes/"},{"name":"nodejs","slug":"nodejs","permalink":"https://github.com/fafucoder/categories/nodejs/"},{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/categories/docker/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://github.com/fafucoder/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"git","slug":"git","permalink":"https://github.com/fafucoder/categories/git/"},{"name":"环境搭建","slug":"环境搭建","permalink":"https://github.com/fafucoder/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"name":"kube-ovn","slug":"kube-ovn","permalink":"https://github.com/fafucoder/categories/kube-ovn/"}],"tags":[{"name":"golang","slug":"golang","permalink":"https://github.com/fafucoder/tags/golang/"},{"name":"linux","slug":"linux","permalink":"https://github.com/fafucoder/tags/linux/"},{"name":"kubernetes","slug":"kubernetes","permalink":"https://github.com/fafucoder/tags/kubernetes/"},{"name":"nodejs","slug":"nodejs","permalink":"https://github.com/fafucoder/tags/nodejs/"},{"name":"docker","slug":"docker","permalink":"https://github.com/fafucoder/tags/docker/"},{"name":"数据结构与算法","slug":"数据结构与算法","permalink":"https://github.com/fafucoder/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"},{"name":"git","slug":"git","permalink":"https://github.com/fafucoder/tags/git/"},{"name":"环境搭建","slug":"环境搭建","permalink":"https://github.com/fafucoder/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"name":"kube-ovn","slug":"kube-ovn","permalink":"https://github.com/fafucoder/tags/kube-ovn/"}]}